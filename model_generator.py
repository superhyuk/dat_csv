#!/usr/bin/env python
# train_ocsvm_gui.py

import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext
import psycopg2
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import joblib
import json
import os
import threading
from sklearn.svm import OneClassSVM
import optuna
from optuna import create_study
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg

# ë¼ì¦ˆë² ë¦¬íŒŒì´ì˜ CustomRobustScaler êµ¬í˜„
class CustomRobustScaler:
    def __init__(self):
        self.params = {}
    
    def fit(self, X):
        self.params = {}
        for i in range(X.shape[1]):
            col = X[:, i]
            q1, median, q3 = np.percentile(col, [25, 50, 75])
            iqr = q3 - q1
            self.params[i] = {'median': median, 'iqr': iqr}
    
    def transform(self, X):
        X_scaled = np.zeros_like(X, dtype=np.float64)
        for i in range(X.shape[1]):
            median = self.params[i]['median']
            iqr = self.params[i]['iqr']
            X_scaled[:, i] = (X[:, i] - median) / iqr
        return X_scaled
    
    def fit_transform(self, X):
        self.fit(X)
        return self.transform(X)
    
    def save(self, filepath):
        joblib.dump({"params": self.params}, filepath)
    
    def load(self, filepath):
        loaded = joblib.load(filepath)
        self.params = loaded["params"]
        
class OCSVMTrainerGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("OCSVM ëª¨ë¸ í•™ìŠµ ë„êµ¬")
        self.root.geometry("1400x900")
        
        # ë¡œê·¸ í…ìŠ¤íŠ¸ ìœ„ì ¯ì„ ë¨¼ì € ì´ˆê¸°í™”
        self.log_text = None
        
        # DB ì—°ê²°
        self.conn = None
        
        # ì„¤ì •ê°’
        self.sensor_config = {
            "mic": {
                "sampling_rate": 8000,
                "window_sec": 5,
                "features": ["mav", "rms", "peak", "amp_iqr"],
                "nu_range": [0.01, 0.15],
                "gamma_range": [0.0001, 0.005]
            },
            "acc": {
                "sampling_rate": 1666,
                "window_sec": 5,
                "features": ["x_peak", "x_crest_factor", "y_peak", "y_crest_factor", "z_peak", "z_crest_factor"],
                "nu_range": [0.01, 0.15],
                "gamma_range": [0.0001, 0.005]
            }
        }
        
        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì €ì¥
        self.training_periods = []
        self.test_periods = []
        
        # GUI ìƒì„±
        self.create_widgets()
        
        # DB ì—°ê²° (GUI ìƒì„± í›„)
        self.connect_db()
        
        # ë‚ ì§œ ë²”ìœ„ ë¡œë“œ
        self.load_date_range()
    
    def log(self, message):
        """ë¡œê·¸ ë©”ì‹œì§€ ì¶œë ¥"""
        if self.log_text:
            timestamp = datetime.now().strftime("%H:%M:%S")
            self.log_text.insert(tk.END, f"[{timestamp}] {message}\n")
            self.log_text.see(tk.END)
            self.root.update()
        else:
            print(f"[LOG] {message}")  # ë¡œê·¸ ìœ„ì ¯ì´ ì—†ì„ ë•Œ ì½˜ì†” ì¶œë ¥
        
        # ì½˜ì†”ì—ë„ í•­ìƒ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        print(f"[{timestamp}] {message}")
    
    def connect_db(self):
        """DB ì—°ê²° - íŠ¸ëœì­ì…˜ ì—ëŸ¬ ë°©ì§€"""
        try:
            self.conn = psycopg2.connect(
                host='localhost',
                port='5432',
                database='pdm_db',
                user='pdm_user',
                password='pdm_password'
            )
            # autocommit ì„¤ì •ìœ¼ë¡œ íŠ¸ëœì­ì…˜ ì—ëŸ¬ ë°©ì§€
            self.conn.autocommit = True
            self.log("âœ… DB ì—°ê²° ì„±ê³µ")
            
            # í…Œì´ë¸” ë° ë°ì´í„° í™•ì¸
            cur = self.conn.cursor()
            
            # í…Œì´ë¸” í™•ì¸
            cur.execute("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name IN ('normal_acc_data', 'normal_mic_data')
            """)
            tables = [row[0] for row in cur.fetchall()]
            self.log(f"í™•ì¸ëœ í…Œì´ë¸”: {tables}")
            
            # ê° í…Œì´ë¸”ì˜ ë°ì´í„° ìˆ˜ í™•ì¸
            for table in tables:
                cur.execute(f"SELECT COUNT(*) FROM {table}")
                count = cur.fetchone()[0]
                self.log(f"{table}: {count:,}ê°œ ë ˆì½”ë“œ")
            
            # ë¨¸ì‹ ë³„ ë°ì´í„° í™•ì¸
            for table in tables:
                cur.execute(f"""
                    SELECT machine_id, COUNT(*) as cnt, 
                           MIN(time) as min_time, MAX(time) as max_time
                    FROM {table}
                    GROUP BY machine_id
                """)
                for row in cur.fetchall():
                    self.log(f"{table} - {row[0]}: {row[1]:,}ê°œ, {row[2]} ~ {row[3]}")
            
        except Exception as e:
            self.log(f"DB ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            messagebox.showerror("DB ì—°ê²° ì‹¤íŒ¨", str(e))
    
    def create_widgets(self):
        # ë©”ì¸ ë…¸íŠ¸ë¶
        notebook = ttk.Notebook(self.root)
        notebook.pack(fill='both', expand=True, padx=5, pady=5)
        
        # í•™ìŠµ íƒ­
        train_tab = ttk.Frame(notebook)
        notebook.add(train_tab, text="ëª¨ë¸ í•™ìŠµ")
        self.create_train_tab(train_tab)
        
        # í…ŒìŠ¤íŠ¸ íƒ­
        test_tab = ttk.Frame(notebook)
        notebook.add(test_tab, text="ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        self.create_test_tab(test_tab)
        
        # ë¡œê·¸ íƒ­
        log_tab = ttk.Frame(notebook)
        notebook.add(log_tab, text="ë¡œê·¸")
        self.create_log_tab(log_tab)
    
    def create_train_tab(self, parent):
        # ì„¤ì • í”„ë ˆì„
        config_frame = ttk.LabelFrame(parent, text="í•™ìŠµ ì„¤ì •", padding="10")
        config_frame.pack(fill='x', padx=5, pady=5)
        
        # ë¨¸ì‹ /ì„¼ì„œ ì„ íƒ
        row = 0
        ttk.Label(config_frame, text="ë¨¸ì‹ :").grid(row=row, column=0, sticky=tk.W, padx=5)
        self.machine_var = tk.StringVar(value="CURINGOVEN_M1")
        machine_combo = ttk.Combobox(config_frame, textvariable=self.machine_var, 
                                    values=["CURINGOVEN_M1", "HOTCHAMBER_M2"], width=20)
        machine_combo.grid(row=row, column=1, padx=5)
        
        ttk.Label(config_frame, text="ì„¼ì„œ:").grid(row=row, column=2, sticky=tk.W, padx=5)
        self.sensor_var = tk.StringVar(value="acc")
        sensor_combo = ttk.Combobox(config_frame, textvariable=self.sensor_var,
                                   values=["acc", "mic"], width=10)
        sensor_combo.grid(row=row, column=3, padx=5)
        
        # ìµœì í™” ì„¤ì •
        row += 1
        ttk.Label(config_frame, text="Optuna Trials:").grid(row=row, column=0, sticky=tk.W, padx=5)
        self.trials_var = tk.IntVar(value=50)
        trials_spinbox = ttk.Spinbox(config_frame, from_=10, to=500, increment=10,
                                    textvariable=self.trials_var, width=10)
        trials_spinbox.grid(row=row, column=1, padx=5)
        
        # ì„±ëŠ¥ í‰ê°€ ìŠ¤í‚µ ì˜µì…˜
        row += 1
        self.skip_eval_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(config_frame, text="ì„±ëŠ¥ í‰ê°€ ìŠ¤í‚µ (ë¹ ë¥¸ í•™ìŠµ)", 
                       variable=self.skip_eval_var).grid(row=row, column=0, columnspan=2, sticky=tk.W, padx=5)
        
        # í•™ìŠµ ê¸°ê°„ í”„ë ˆì„
        period_frame = ttk.LabelFrame(parent, text="í•™ìŠµ ê¸°ê°„ ì„¤ì •", padding="10")
        period_frame.pack(fill='both', expand=True, padx=5, pady=5)
        
        # ê¸°ê°„ ì…ë ¥
        input_frame = ttk.Frame(period_frame)
        input_frame.pack(fill='x')
        
        ttk.Label(input_frame, text="ì‹œì‘ì¼:").pack(side='left', padx=5)
        self.train_start_date = ttk.Entry(input_frame, width=12)
        self.train_start_date.pack(side='left', padx=5)
        
        ttk.Label(input_frame, text="ì¢…ë£Œì¼:").pack(side='left', padx=5)
        self.train_end_date = ttk.Entry(input_frame, width=12)
        self.train_end_date.pack(side='left', padx=5)
        
        ttk.Button(input_frame, text="ê¸°ê°„ ì¶”ê°€", 
                  command=self.add_training_period).pack(side='left', padx=20)
        
        # ê¸°ê°„ ëª©ë¡
        list_frame = ttk.Frame(period_frame)
        list_frame.pack(fill='both', expand=True, pady=10)
        
        # íŠ¸ë¦¬ë·°
        columns = ('ì‹œì‘ì¼', 'ì¢…ë£Œì¼', 'ì¼ìˆ˜')
        self.train_tree = ttk.Treeview(list_frame, columns=columns, height=8)
        self.train_tree.heading('#0', text='No')
        self.train_tree.heading('ì‹œì‘ì¼', text='ì‹œì‘ì¼')
        self.train_tree.heading('ì¢…ë£Œì¼', text='ì¢…ë£Œì¼')
        self.train_tree.heading('ì¼ìˆ˜', text='ì¼ìˆ˜')
        
        self.train_tree.column('#0', width=50)
        self.train_tree.column('ì‹œì‘ì¼', width=150)
        self.train_tree.column('ì¢…ë£Œì¼', width=150)
        self.train_tree.column('ì¼ìˆ˜', width=80)
        
        self.train_tree.pack(side='left', fill='both', expand=True)
        
        # ìŠ¤í¬ë¡¤ë°”
        scrollbar = ttk.Scrollbar(list_frame, orient='vertical', command=self.train_tree.yview)
        scrollbar.pack(side='right', fill='y')
        self.train_tree.configure(yscrollcommand=scrollbar.set)
        
        # ë²„íŠ¼ í”„ë ˆì„
        button_frame = ttk.Frame(period_frame)
        button_frame.pack(fill='x')
        
        ttk.Button(button_frame, text="ì„ íƒ ì‚­ì œ", 
                  command=self.remove_training_period).pack(side='left', padx=5)
        ttk.Button(button_frame, text="ëª¨ë‘ ì‚­ì œ", 
                  command=self.clear_training_periods).pack(side='left', padx=5)
        
        # í•™ìŠµ ë²„íŠ¼
        train_button_frame = ttk.Frame(parent)
        train_button_frame.pack(fill='x', padx=5, pady=10)
        
        self.train_button = ttk.Button(train_button_frame, text="ëª¨ë¸ í•™ìŠµ ì‹œì‘", 
                                      command=self.start_training)
        self.train_button.pack(side='left', padx=5)
        
        self.progress_var = tk.StringVar(value="ëŒ€ê¸° ì¤‘...")
        ttk.Label(train_button_frame, textvariable=self.progress_var).pack(side='left', padx=20)
    
    def create_test_tab(self, parent):
        # ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒ í”„ë ˆì„
        model_frame = ttk.LabelFrame(parent, text="ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒ", padding="10")
        model_frame.pack(fill='x', padx=5, pady=5)
        
        # ì„¼ì„œ íƒ€ì… ì„ íƒ
        ttk.Label(model_frame, text="ì„¼ì„œ íƒ€ì…:").grid(row=0, column=0, sticky=tk.W, padx=5)
        self.test_sensor_var = tk.StringVar(value="acc")
        sensor_radio_frame = ttk.Frame(model_frame)
        sensor_radio_frame.grid(row=0, column=1, columnspan=2, sticky=tk.W, padx=5)
        
        ttk.Radiobutton(sensor_radio_frame, text="ê°€ì†ë„ ì„¼ì„œ", variable=self.test_sensor_var, 
                        value="acc", command=self.update_test_settings).pack(side='left', padx=5)
        ttk.Radiobutton(sensor_radio_frame, text="ë§ˆì´í¬ ì„¼ì„œ", variable=self.test_sensor_var, 
                        value="mic", command=self.update_test_settings).pack(side='left', padx=5)
        
        # ëª¨ë¸ íŒŒì¼ ì„ íƒ
        ttk.Label(model_frame, text="ëª¨ë¸ íŒŒì¼:").grid(row=1, column=0, sticky=tk.W, padx=5)
        self.model_file_var = tk.StringVar()
        ttk.Entry(model_frame, textvariable=self.model_file_var, width=50).grid(row=1, column=1, padx=5)
        ttk.Button(model_frame, text="ì°¾ì•„ë³´ê¸°", 
                  command=self.browse_model_file).grid(row=1, column=2, padx=5)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ ì„ íƒ
        ttk.Label(model_frame, text="ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼:").grid(row=2, column=0, sticky=tk.W, padx=5)
        self.scaler_file_var = tk.StringVar()
        ttk.Entry(model_frame, textvariable=self.scaler_file_var, width=50).grid(row=2, column=1, padx=5)
        ttk.Button(model_frame, text="ì°¾ì•„ë³´ê¸°", 
                  command=self.browse_scaler_file).grid(row=2, column=2, padx=5)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • í”„ë ˆì„
        db_frame = ttk.LabelFrame(parent, text="ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •", padding="10")
        db_frame.pack(fill='x', padx=5, pady=5)
        
        # ë¨¸ì‹  ì„ íƒ
        ttk.Label(db_frame, text="ë¨¸ì‹ :").grid(row=0, column=0, sticky=tk.W, padx=5)
        self.test_machine_var = tk.StringVar(value="CURINGOVEN_M1")
        ttk.Combobox(db_frame, textvariable=self.test_machine_var, 
                    values=["CURINGOVEN_M1", "HOTCHAMBER_M2"], 
                    width=20).grid(row=0, column=1, padx=5)
        
        # ìƒ˜í”Œë§ ì„¤ì • í‘œì‹œ
        ttk.Label(db_frame, text="ìƒ˜í”Œë§ ì„¤ì •:").grid(row=1, column=0, sticky=tk.W, padx=5)
        self.sampling_info_var = tk.StringVar(value="ê°€ì†ë„: 1666Hz, 5ì´ˆ ìœˆë„ìš°")
        ttk.Label(db_frame, textvariable=self.sampling_info_var).grid(row=1, column=1, sticky=tk.W, padx=5)
        
        # íŠ¹ì§• ì •ë³´ í‘œì‹œ
        ttk.Label(db_frame, text="ì¶”ì¶œ íŠ¹ì§•:").grid(row=2, column=0, sticky=tk.W, padx=5)
        self.features_info_var = tk.StringVar(value="x_peak, x_crest_factor, y_peak, y_crest_factor, z_peak, z_crest_factor")
        features_label = ttk.Label(db_frame, textvariable=self.features_info_var, wraplength=400)
        features_label.grid(row=2, column=1, sticky=tk.W, padx=5)
        
        # í…ŒìŠ¤íŠ¸ ê¸°ê°„ í”„ë ˆì„
        test_period_frame = ttk.LabelFrame(parent, text="í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì„¤ì •", padding="10")
        test_period_frame.pack(fill='both', expand=True, padx=5, pady=5)
        
        # ê¸°ê°„ ì…ë ¥
        test_input_frame = ttk.Frame(test_period_frame)
        test_input_frame.pack(fill='x')
        
        ttk.Label(test_input_frame, text="ë‚ ì§œ:").pack(side='left', padx=5)
        self.test_date = ttk.Entry(test_input_frame, width=12)
        self.test_date.pack(side='left', padx=5)
        
        ttk.Button(test_input_frame, text="ë‚ ì§œ ì¶”ê°€", 
                  command=self.add_test_date).pack(side='left', padx=20)
        
        # í…ŒìŠ¤íŠ¸ ë‚ ì§œ ëª©ë¡
        self.test_listbox = tk.Listbox(test_period_frame, height=8)
        self.test_listbox.pack(fill='both', expand=True, pady=10)
        
        # ë²„íŠ¼
        test_button_frame = ttk.Frame(test_period_frame)
        test_button_frame.pack(fill='x')
        
        ttk.Button(test_button_frame, text="ì„ íƒ ì‚­ì œ", 
                  command=self.remove_test_date).pack(side='left', padx=5)
        ttk.Button(test_button_frame, text="ëª¨ë‘ ì‚­ì œ", 
                  command=self.clear_test_dates).pack(side='left', padx=5)
        
        # í…ŒìŠ¤íŠ¸ ì‹œì‘ ë²„íŠ¼
        ttk.Button(parent, text="í…ŒìŠ¤íŠ¸ ì‹œì‘", 
                  command=self.start_testing).pack(pady=10)
        
        # ê²°ê³¼ í‘œì‹œ
        result_frame = ttk.LabelFrame(parent, text="í…ŒìŠ¤íŠ¸ ê²°ê³¼", padding="10")
        result_frame.pack(fill='both', expand=True, padx=5, pady=5)
        
        self.result_text = scrolledtext.ScrolledText(result_frame, height=10)
        self.result_text.pack(fill='both', expand=True)
    
    def create_log_tab(self, parent):
        # ë¡œê·¸ í…ìŠ¤íŠ¸ ìœ„ì ¯ ìƒì„±
        self.log_text = scrolledtext.ScrolledText(parent, height=30)
        self.log_text.pack(fill='both', expand=True, padx=5, pady=5)
    
    def update_test_settings(self):
        """ì„¼ì„œ íƒ€ì…ì— ë”°ë¼ ì„¤ì • ì •ë³´ ì—…ë°ì´íŠ¸"""
        sensor = self.test_sensor_var.get()
        if sensor == "acc":
            self.sampling_info_var.set("ê°€ì†ë„: 1666Hz, 5ì´ˆ ìœˆë„ìš°")
            self.features_info_var.set("x_peak, x_crest_factor, y_peak, y_crest_factor, z_peak, z_crest_factor")
        else:
            self.sampling_info_var.set("ë§ˆì´í¬: 8000Hz, 5ì´ˆ ìœˆë„ìš°")
            self.features_info_var.set("mav, rms, peak, amp_iqr")
    
    def browse_model_file(self):
        """ëª¨ë¸ íŒŒì¼ ì°¾ì•„ë³´ê¸°"""
        from tkinter import filedialog
        filename = filedialog.askopenfilename(
            title="ëª¨ë¸ íŒŒì¼ ì„ íƒ",
            initialdir="./models",
            filetypes=[("Pickle files", "*.pkl"), ("All files", "*.*")]
        )
        if filename:
            self.model_file_var.set(filename)
            
            # ëª¨ë¸ ì •ë³´ íŒŒì¼ë„ ê°™ì´ ì°¾ê¸°
            info_path = filename.replace('_model.pkl', '_model_info.json')
            if os.path.exists(info_path):
                with open(info_path, 'r') as f:
                    model_info = json.load(f)
                    # ì„¼ì„œ íƒ€ì… ìë™ ì„¤ì •
                    self.test_sensor_var.set(model_info.get('sensor', 'acc'))
                    self.update_test_settings()
    
    def browse_scaler_file(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ ì°¾ì•„ë³´ê¸°"""
        from tkinter import filedialog
        filename = filedialog.askopenfilename(
            title="ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ ì„ íƒ",
            initialdir="./models",
            filetypes=[("Pickle files", "*.pkl"), ("All files", "*.*")]
        )
        if filename:
            self.scaler_file_var.set(filename)
    
    def load_date_range(self):
        """DBì—ì„œ ë°ì´í„° ë‚ ì§œ ë²”ìœ„ í™•ì¸"""
        try:
            cur = self.conn.cursor()
            cur.execute("""
                SELECT MIN(DATE(time)), MAX(DATE(time))
                FROM normal_acc_data
            """)
            min_date, max_date = cur.fetchone()
            
            if min_date and max_date:
                self.train_start_date.insert(0, min_date.strftime("%Y-%m-%d"))
                self.train_end_date.insert(0, max_date.strftime("%Y-%m-%d"))
                self.test_date.insert(0, max_date.strftime("%Y-%m-%d"))
                
                self.log(f"ë°ì´í„° ë²”ìœ„: {min_date} ~ {max_date}")
        except Exception as e:
            self.log(f"ë‚ ì§œ ë²”ìœ„ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def check_data_exists(self):
        """ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        try:
            cur = self.conn.cursor()
            
            # ê° í…Œì´ë¸”ì˜ ë°ì´í„° ìˆ˜ í™•ì¸
            for table in ['acc_data', 'mic_data']:
                cur.execute(f"SELECT COUNT(*) FROM {table}")
                count = cur.fetchone()[0]
                self.log(f"{table}: {count}ê°œ ë ˆì½”ë“œ")
                
            # ë¨¸ì‹ ë³„ ë°ì´í„° ìˆ˜ í™•ì¸
            for table in ['acc_data', 'mic_data']:
                cur.execute(f"""
                    SELECT machine_id, COUNT(*), MIN(time), MAX(time)
                    FROM {table}
                    GROUP BY machine_id
                """)
                for row in cur.fetchall():
                    self.log(f"{table} - {row[0]}: {row[1]}ê°œ, {row[2]} ~ {row[3]}")
                    
        except Exception as e:
            self.log(f"ë°ì´í„° í™•ì¸ ì‹¤íŒ¨: {e}")
    
    def stratified_time_sampling(self, X_scaled, period_info, target_size):
        """ì‹œê°„ëŒ€ë³„ ê· ë“± ìƒ˜í”Œë§"""
        sample_indices = []
        
        # ê° ê¸°ê°„ì„ ì‹œê°„ëŒ€ë³„ë¡œ ë‚˜ëˆ„ê¸°
        for info in period_info:
            period_data_count = info['count']
            period_sample_size = int(target_size * (period_data_count / len(X_scaled)))
            
            if period_sample_size > 0:
                # í•˜ë£¨ë¥¼ 4ê°œ ì‹œê°„ëŒ€ë¡œ ë‚˜ëˆ„ê¸° (6ì‹œê°„ ë‹¨ìœ„)
                # ë˜ëŠ” í”¼í¬/ì˜¤í”„í”¼í¬ ì‹œê°„ëŒ€ë¡œ ë‚˜ëˆ„ê¸°
                period_start = info['start_idx']
                period_end = info['end_idx']
                
                # ê· ë“± ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
                indices = np.linspace(period_start, period_end-1, 
                                    period_sample_size, dtype=int)
                sample_indices.extend(indices)
        
        return np.array(sample_indices)
    
    def add_training_period(self):
        try:
            start = self.train_start_date.get()
            end = self.train_end_date.get()
            
            start_date = datetime.strptime(start, "%Y-%m-%d")
            end_date = datetime.strptime(end, "%Y-%m-%d")
            
            if start_date >= end_date:
                messagebox.showerror("ì˜¤ë¥˜", "ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤.")
                return
            
            days = (end_date - start_date).days + 1
            
            # íŠ¸ë¦¬ì— ì¶”ê°€
            item_id = self.train_tree.insert('', 'end', 
                                           text=str(len(self.training_periods) + 1),
                                           values=(start, end, days))
            
            self.training_periods.append((start, end))
            self.log(f"í•™ìŠµ ê¸°ê°„ ì¶”ê°€: {start} ~ {end} ({days}ì¼)")
            
        except ValueError:
            messagebox.showerror("ì˜¤ë¥˜", "ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. (YYYY-MM-DD)")
    
    def remove_training_period(self):
        selected = self.train_tree.selection()
        if selected:
            for item in selected:
                idx = self.train_tree.index(item)
                del self.training_periods[idx]
                self.train_tree.delete(item)
            
            # ë²ˆí˜¸ ì¬ì •ë ¬
            for i, item in enumerate(self.train_tree.get_children()):
                self.train_tree.item(item, text=str(i + 1))
    
    def clear_training_periods(self):
        self.training_periods.clear()
        for item in self.train_tree.get_children():
            self.train_tree.delete(item)
    
    def add_test_date(self):
        date = self.test_date.get()
        try:
            datetime.strptime(date, "%Y-%m-%d")
            if date not in self.test_listbox.get(0, tk.END):
                self.test_listbox.insert(tk.END, date)
                self.test_periods.append(date)
        except ValueError:
            messagebox.showerror("ì˜¤ë¥˜", "ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. (YYYY-MM-DD)")
    
    def remove_test_date(self):
        selected = self.test_listbox.curselection()
        for idx in reversed(selected):
            self.test_listbox.delete(idx)
            del self.test_periods[idx]
    
    def clear_test_dates(self):
        self.test_listbox.delete(0, tk.END)
        self.test_periods.clear()
    
    def extract_features_acc(self, x_data, y_data, z_data):
        """ACC íŠ¹ì§• ì¶”ì¶œ - ë‹¤ìš´ìƒ˜í”Œë§ëœ ë°ì´í„°ìš©"""
        try:
            # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ
            if len(x_data) < 10:
                raise ValueError(f"ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŒ: {len(x_data)}ê°œ")
            
            features = []
            
            # Xì¶•
            x_rms = np.sqrt(np.mean(x_data**2))
            x_peak = np.max(np.abs(x_data))
            x_crest = x_peak / x_rms if x_rms > 1e-10 else 0
            
            # Yì¶•
            y_rms = np.sqrt(np.mean(y_data**2))
            y_peak = np.max(np.abs(y_data))
            y_crest = y_peak / y_rms if y_rms > 1e-10 else 0
            
            # Zì¶•
            z_rms = np.sqrt(np.mean(z_data**2))
            z_peak = np.max(np.abs(z_data))
            z_crest = z_peak / z_rms if z_rms > 1e-10 else 0
            
            return np.array([x_peak, x_crest, y_peak, y_crest, z_peak, z_crest])
            
        except Exception as e:
            self.log(f"ACC íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            raise
    
    def extract_features_mic(self, mic_data):
        """MIC íŠ¹ì§• ì¶”ì¶œ - ë‹¤ìš´ìƒ˜í”Œë§ëœ ë°ì´í„°ìš©"""
        try:
            # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ
            if len(mic_data) < 10:
                raise ValueError(f"ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŒ: {len(mic_data)}ê°œ")
            
            mav = np.mean(np.abs(mic_data))
            rms = np.sqrt(np.mean(mic_data**2))
            peak = np.max(np.abs(mic_data))
            
            # IQR ê³„ì‚° ì‹œ ë°ì´í„°ê°€ ì ìœ¼ë¯€ë¡œ ì¡°ì‹¬
            q1, q3 = np.percentile(np.abs(mic_data), [25, 75])
            amp_iqr = q3 - q1
            
            return np.array([mav, rms, peak, amp_iqr])
            
        except Exception as e:
            self.log(f"MIC íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            raise
    
    def get_training_data(self, machine_id, sensor, start_date, end_date):
        """DBì—ì„œ í•™ìŠµ ë°ì´í„° ì¶”ì¶œ - ë‹¤ìš´ìƒ˜í”Œë§ëœ ë°ì´í„° ì²˜ë¦¬"""
        window_sec = self.sensor_config[sensor]['window_sec']
        
        # DBì—ëŠ” 1ì´ˆì— 10ê°œì”©ë§Œ ì €ì¥ë˜ì–´ ìˆìŒ
        db_sampling_rate = 10  # 1ì´ˆì— 10ê°œ
        window_samples = window_sec * db_sampling_rate  # 5ì´ˆ * 10 = 50ê°œ
        
        # ì „ì²´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
        if sensor == 'acc':
            query = """
            SELECT time, x, y, z
            FROM normal_acc_data
            WHERE machine_id = %s
            AND time >= %s AND time <= %s
            ORDER BY time
            """
        else:  # mic
            query = """
            SELECT time, mic_value
            FROM normal_mic_data
            WHERE machine_id = %s
            AND time >= %s AND time <= %s
            ORDER BY time
            """
        
        self.log(f"ë°ì´í„° ì¶”ì¶œ ì¤‘: {machine_id}, {sensor}, {start_date} ~ {end_date}")
        
        try:
            start_time = datetime.now()
            df = pd.read_sql(query, self.conn, 
                            params=(machine_id, start_date, end_date))
            
            if df.empty:
                self.log(f"ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
                return None
            
            self.log(f"ì „ì²´ ë°ì´í„°: {len(df)}ê°œ ìƒ˜í”Œ")
            self.log(f"ë°ì´í„° ë¡œë“œ ì‹œê°„: {(datetime.now() - start_time).total_seconds():.1f}ì´ˆ")
            
            # Pythonì—ì„œ 5ì´ˆ ìœˆë„ìš°ë¡œ ë¶„í• 
            features_list = []
            
            # ì‹œê°„ì„ datetimeìœ¼ë¡œ ë³€í™˜
            df['time'] = pd.to_datetime(df['time'])
            
            # 5ì´ˆ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
            df['window'] = df['time'].dt.floor(f'{window_sec}S')
            
            # ìœˆë„ìš°ë³„ë¡œ ì²˜ë¦¬
            window_count = 0
            for window, group in df.groupby('window'):
                # ìµœì†Œ 40ê°œ ì´ìƒ (80%) ë°ì´í„°ê°€ ìˆëŠ” ìœˆë„ìš°ë§Œ ì‚¬ìš©
                if len(group) >= window_samples * 0.8:  
                    try:
                        if sensor == 'acc':
                            features = self.extract_features_acc(
                                group['x'].values,
                                group['y'].values,
                                group['z'].values
                            )
                        else:
                            features = self.extract_features_mic(group['mic_value'].values)
                        
                        features_list.append(features)
                        window_count += 1
                        
                        # ì§„í–‰ ìƒí™© ë¡œê·¸ (1000ê°œë§ˆë‹¤)
                        if window_count % 1000 == 0:
                            elapsed = (datetime.now() - start_time).total_seconds()
                            rate = window_count / elapsed if elapsed > 0 else 0
                            self.log(f"  ì²˜ë¦¬ì¤‘: {window_count:,}ê°œ ìœˆë„ìš° ({rate:.0f} windows/sec)")
                        
                        # GUI ì—…ë°ì´íŠ¸ (100ê°œë§ˆë‹¤)
                        if window_count % 100 == 0:
                            self.progress_var.set(f"ë°ì´í„° ì¶”ì¶œ ì¤‘: {start_date} ~ {end_date} ({window_count}ê°œ)")
                            self.root.update_idletasks()
                            
                    except Exception as e:
                        if window_count % 1000 == 0:
                            self.log(f"  ìœˆë„ìš° ì²˜ë¦¬ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        continue
            
            total_time = (datetime.now() - start_time).total_seconds()
            self.log(f"ì¶”ì¶œ ì™„ë£Œ: {len(features_list)}ê°œ ìœˆë„ìš° (ì´ {total_time:.1f}ì´ˆ)")
            
            return np.array(features_list) if features_list else None
            
        except Exception as e:
            self.log(f"ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def train_model(self):
        """ëª¨ë¸ í•™ìŠµ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
        try:
            total_start_time = datetime.now()
            machine_id = self.machine_var.get()
            sensor = self.sensor_var.get()
            n_trials = self.trials_var.get()
            
            self.log(f"\n{'='*60}")
            self.log(f"{machine_id} / {sensor} ëª¨ë¸ í•™ìŠµ ì‹œì‘")
            self.log(f"{'='*60}")
            
            # í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
            all_features = []
            period_info = []  # ê° ê¸°ê°„ë³„ ì •ë³´ ì €ì¥
            
            self.log(f"í•™ìŠµ ê¸°ê°„: {len(self.training_periods)}ê°œ")
            
            for idx, (start, end) in enumerate(self.training_periods):
                self.log(f"\n[{idx+1}/{len(self.training_periods)}] ê¸°ê°„: {start} ~ {end}")
                self.progress_var.set(f"ë°ì´í„° ì¶”ì¶œ ì¤‘: {start} ~ {end}")
                features = self.get_training_data(machine_id, sensor, start, end)
                if features is not None and len(features) > 0:
                    all_features.append(features)
                    period_info.append({
                        'period': f"{start} ~ {end}",
                        'start_idx': len(np.vstack(all_features[:-1])) if len(all_features) > 1 else 0,
                        'end_idx': len(np.vstack(all_features)),
                        'count': len(features)
                    })
                    self.log(f"âœ… ì¶”ì¶œ ì„±ê³µ: {len(features)}ê°œ ìœˆë„ìš°")
            
            if not all_features:
                self.log("âŒ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                self.progress_var.set("í•™ìŠµ ë°ì´í„° ì—†ìŒ")
                return
            
            X_train = np.vstack(all_features)
            self.log(f"\nì „ì²´ í•™ìŠµ ë°ì´í„°: {X_train.shape}")
            
            # ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
            self.log("\nìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹œì‘...")
            self.progress_var.set("ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì¤‘...")
            scaler_start = datetime.now()
            scaler = CustomRobustScaler()
            X_scaled = scaler.fit_transform(X_train)
            scaler_time = (datetime.now() - scaler_start).total_seconds()
            self.log(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì™„ë£Œ ({scaler_time:.1f}ì´ˆ)")
            
            # OCSVM ìµœì í™”
            self.log(f"\ní•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ (Optuna {n_trials} trials)")
            optuna_start = datetime.now()
            self.progress_var.set(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘... (0/{n_trials})")
            
            opt_config = self.sensor_config[sensor]
            nu_range = opt_config['nu_range']
            gamma_range = opt_config['gamma_range']
            
            # ì‹œê°„ì  ë¶„í¬ë¥¼ ê³ ë ¤í•œ ê³„ì¸µì  ìƒ˜í”Œë§
            target_sample_size = min(20000, int(len(X_scaled) * 0.1))  # ìµœëŒ€ 20,000ê°œ
            
            if target_sample_size < len(X_scaled):
                self.log(f"\nğŸ“Š ê³„ì¸µì  ìƒ˜í”Œë§ ì‹œì‘: {len(X_scaled)}ê°œ â†’ {target_sample_size}ê°œ")
                
                sample_indices = []
                
                # ê° ê¸°ê°„ë³„ë¡œ ë¹„ë¡€ì ìœ¼ë¡œ ìƒ˜í”Œë§
                for info in period_info:
                    period_ratio = info['count'] / len(X_scaled)
                    period_sample_size = int(target_sample_size * period_ratio)
                    
                    if period_sample_size > 0:
                        # í•´ë‹¹ ê¸°ê°„ ë‚´ì—ì„œ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
                        period_indices = np.arange(info['start_idx'], info['end_idx'])
                        
                        if period_sample_size < len(period_indices):
                            # ì‹œê°„ ê°„ê²©ì„ ë‘ê³  ê· ë“±í•˜ê²Œ ì„ íƒ
                            step = len(period_indices) // period_sample_size
                            selected = period_indices[::step][:period_sample_size]
                        else:
                            selected = period_indices
                        
                        sample_indices.extend(selected)
                        self.log(f"  - {info['period']}: {len(selected)}ê°œ ìƒ˜í”Œ")
                
                sample_indices = np.array(sample_indices)
                X_sample = X_scaled[sample_indices]
                self.log(f"âœ… ì´ {len(X_sample)}ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì™„ë£Œ")
            else:
                X_sample = X_scaled
                self.log(f"ğŸ“Š ë°ì´í„°ê°€ ì¶©ë¶„íˆ ì‘ì•„ ì „ì²´ ì‚¬ìš©: {len(X_scaled)}ê°œ")
            
            trial_count = 0
            # study ë³€ìˆ˜ë¥¼ í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ë§Œë“¤ì–´ objective í•¨ìˆ˜ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ
            self.study = create_study(direction='maximize')
            
            def objective(trial):
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                current_trial = len(self.study.trials)
                self.progress_var.set(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘... ({current_trial}/{n_trials})")
                self.root.update_idletasks()
                
                nu = trial.suggest_float('nu', nu_range[0], nu_range[1], log=True)
                gamma = trial.suggest_float('gamma', gamma_range[0], gamma_range[1], log=True)
                
                model = OneClassSVM(kernel='rbf', nu=nu, gamma=gamma)
                model.fit(X_sample)
                
                scores = model.decision_function(X_sample)
                threshold = np.percentile(scores, 5)
                predictions = (scores > threshold).astype(int)
                score = np.mean(predictions)
                
                return score
            
            # Optuna ì½œë°± í•¨ìˆ˜
            def optuna_callback(study, trial):
                nonlocal trial_count
                trial_count += 1
                if trial_count % 10 == 0 or trial_count <= 5:
                    self.log(f"  Trial {trial_count}: nu={trial.params['nu']:.4f}, "
                            f"gamma={trial.params['gamma']:.6f}, score={trial.value:.4f}")
            
            # ìµœì í™” ì‹¤í–‰
            self.study.optimize(objective, n_trials=n_trials, callbacks=[optuna_callback])
            
            optuna_time = (datetime.now() - optuna_start).total_seconds()
            self.log(f"\nâœ… ìµœì í™” ì™„ë£Œ ({optuna_time:.1f}ì´ˆ)")
            self.log(f"ìµœì  íŒŒë¼ë¯¸í„°: nu={self.study.best_params['nu']:.4f}, "
                    f"gamma={self.study.best_params['gamma']:.6f}")
            self.log(f"ìµœì  ì ìˆ˜: {self.study.best_value:.4f}")
            
            # ìµœì  ëª¨ë¸ë¡œ ì „ì²´ ë°ì´í„° í•™ìŠµ
            self.progress_var.set("ìµœì¢… ëª¨ë¸ í•™ìŠµ ì¤‘...")
            self.log("\nìµœì¢… ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
            best_nu = self.study.best_params['nu']
            best_gamma = self.study.best_params['gamma']
            
            model = OneClassSVM(kernel='rbf', nu=best_nu, gamma=best_gamma)
            model.fit(X_scaled)
            self.log("âœ… ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            
            # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (ì„ íƒì )
            skip_evaluation = self.skip_eval_var.get()  # GUI ì²´í¬ë°•ìŠ¤ ê°’ ì‚¬ìš©
            
            if skip_evaluation:
                self.log("\nâš¡ ì„±ëŠ¥ í‰ê°€ ë‹¨ê³„ ìŠ¤í‚µ (ë¹ ë¥¸ í•™ìŠµ ëª¨ë“œ)")
                # ê°„ë‹¨í•œ ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€ëµì ì¸ ì„±ëŠ¥ë§Œ í™•ì¸
                sample_size = min(10000, len(X_scaled))
                sample_indices = np.random.choice(len(X_scaled), sample_size, replace=False)
                sample_scores = model.decision_function(X_scaled[sample_indices])
                decision_boundary = np.percentile(sample_scores, 5)
                
                model_info = {
                    'machine_id': machine_id,
                    'sensor': sensor,
                    'train_samples': len(X_train),
                    'training_periods': self.training_periods,
                    'features': self.sensor_config[sensor]['features'],
                    'best_params': self.study.best_params,
                    'decision_boundary': float(decision_boundary),
                    'evaluation_skipped': True,
                    'trained_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
            else:
                # ì „ì²´ ì„±ëŠ¥ í‰ê°€ (ë°°ì¹˜ ì²˜ë¦¬)
                self.log("\nëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
                eval_start = datetime.now()
                
                batch_size = 10000
                predictions = []
                scores = []
                
                self.log(f"ì „ì²´ {len(X_scaled):,}ê°œ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰ (ë°°ì¹˜ í¬ê¸°: {batch_size:,})")
                
                for i in range(0, len(X_scaled), batch_size):
                    batch_end = min(i + batch_size, len(X_scaled))
                    batch = X_scaled[i:batch_end]
                    
                    # ë°°ì¹˜ ì˜ˆì¸¡
                    batch_predictions = model.predict(batch)
                    batch_scores = model.decision_function(batch)
                    
                    predictions.extend(batch_predictions)
                    scores.extend(batch_scores)
                    
                    # ì§„í–‰ ìƒí™© ë¡œê·¸ (10ê°œ ë°°ì¹˜ë§ˆë‹¤)
                    if (i // batch_size + 1) % 10 == 0 or batch_end == len(X_scaled):
                        progress = batch_end / len(X_scaled) * 100
                        elapsed = (datetime.now() - eval_start).total_seconds()
                        rate = batch_end / elapsed if elapsed > 0 else 0
                        eta = (len(X_scaled) - batch_end) / rate if rate > 0 else 0
                        
                        self.log(f"  ì˜ˆì¸¡ ì§„í–‰: {batch_end:,}/{len(X_scaled):,} ({progress:.1f}%) "
                                f"- {rate:.0f} samples/sec, ETA: {eta:.0f}ì´ˆ")
                        self.progress_var.set(f"ì„±ëŠ¥ í‰ê°€ ì¤‘... {progress:.1f}%")
                        self.root.update_idletasks()
                
                # numpy ë°°ì—´ë¡œ ë³€í™˜
                predictions = np.array(predictions)
                scores = np.array(scores)
                
                eval_time = (datetime.now() - eval_start).total_seconds()
                self.log(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ ({eval_time:.1f}ì´ˆ)")
                
                # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
                self.log("\nì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì¤‘...")
                anomaly_ratio = np.sum(predictions == -1) / len(predictions) * 100
                decision_boundary = float(np.percentile(scores, 5))
                
                self.log(f"âœ… í•™ìŠµ ì™„ë£Œ!")
                self.log(f"  - ì´ìƒì¹˜ ë¹„ìœ¨: {anomaly_ratio:.2f}%")
                self.log(f"  - ê²°ì • ê²½ê³„: {decision_boundary:.4f}")
                self.log(f"  - ì ìˆ˜ ë²”ìœ„: [{np.min(scores):.4f}, {np.max(scores):.4f}]")
                self.log(f"  - ì ìˆ˜ í‰ê· Â±í‘œì¤€í¸ì°¨: {np.mean(scores):.4f} Â± {np.std(scores):.4f}")
                
                # ê¸°ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
                self.log("\nğŸ“Š ê¸°ê°„ë³„ ì„±ëŠ¥:")
                for info in period_info:
                    start_idx = info['start_idx']
                    end_idx = info['end_idx']
                    period_scores = scores[start_idx:end_idx]
                    period_predictions = predictions[start_idx:end_idx]
                    period_anomaly_ratio = np.sum(period_predictions == -1) / len(period_predictions) * 100
                    
                    self.log(f"  - {info['period']}: ì´ìƒ {period_anomaly_ratio:.1f}%, "
                            f"ì ìˆ˜ {np.mean(period_scores):.2f}Â±{np.std(period_scores):.2f}")
                
                # 2ì°¨ ë¡œì§ì„ ìœ„í•œ ìƒì„¸ í†µê³„ ë¶„ì„
                self.log("\nğŸ“Š 2ì°¨ ë¡œì§ ê²½ê³„ê°’ ì„¤ì •ì„ ìœ„í•œ ë¶„ì„:")
                
                # ì •ìƒ/ì´ìƒ ë°ì´í„° ë¶„ë¦¬
                normal_scores = scores[predictions == 1]
                anomaly_scores = scores[predictions == -1]
                
                # í¼ì„¼íƒ€ì¼ ê¸°ë°˜ ê²½ê³„ê°’ í›„ë³´
                percentiles = [0.1, 0.5, 1, 2, 3, 5, 10, 15, 20]
                percentile_values = {}
                
                self.log("\n  ì •ìƒ ë°ì´í„° ì ìˆ˜ ë¶„í¬:")
                self.log(f"    - ê°œìˆ˜: {len(normal_scores):,}ê°œ ({len(normal_scores)/len(scores)*100:.1f}%)")
                self.log(f"    - í‰ê· Â±í‘œì¤€í¸ì°¨: {np.mean(normal_scores):.2f} Â± {np.std(normal_scores):.2f}")
                self.log(f"    - ìµœì†Œ/ìµœëŒ€: {np.min(normal_scores):.2f} / {np.max(normal_scores):.2f}")
                
                self.log("\n  ì´ìƒ ë°ì´í„° ì ìˆ˜ ë¶„í¬:")
                self.log(f"    - ê°œìˆ˜: {len(anomaly_scores):,}ê°œ ({len(anomaly_scores)/len(scores)*100:.1f}%)")
                self.log(f"    - í‰ê· Â±í‘œì¤€í¸ì°¨: {np.mean(anomaly_scores):.2f} Â± {np.std(anomaly_scores):.2f}")
                self.log(f"    - ìµœì†Œ/ìµœëŒ€: {np.min(anomaly_scores):.2f} / {np.max(anomaly_scores):.2f}")
                
                self.log("\n  ì „ì²´ ì ìˆ˜ í¼ì„¼íƒ€ì¼:")
                for p in percentiles:
                    val = np.percentile(scores, p)
                    percentile_values[f"p{p}"] = float(val)
                    self.log(f"    - {p:5.1f}%: {val:8.2f}")
                
                # ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬
                self.log("\n  ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬:")
                score_ranges = [
                    (-np.inf, -100, "ê·¹ì‹¬í•œ ì´ìƒ"),
                    (-100, -50, "ì‹¬ê°í•œ ì´ìƒ"),
                    (-50, -20, "ì¤‘ê°„ ì´ìƒ"),
                    (-20, -10, "ê²½ë¯¸í•œ ì´ìƒ"),
                    (-10, 0, "ì˜ì‹¬ êµ¬ê°„"),
                    (0, 100, "ì •ìƒ ë²”ìœ„"),
                    (100, np.inf, "ë§¤ìš° ì •ìƒ")
                ]
                
                score_distribution = {}
                self.log("    [ì „ì²´ ë°ì´í„°]")
                for min_score, max_score, label in score_ranges:
                    count = np.sum((scores >= min_score) & (scores < max_score))
                    ratio = count / len(scores) * 100
                    self.log(f"    - {label:12s} [{min_score:6.0f} ~ {max_score:6.0f}]: "
                            f"{count:6,}ê°œ ({ratio:5.1f}%)")
                
                # ì •ìƒ ë°ì´í„°ì˜ ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬
                self.log("\n    [ì •ìƒìœ¼ë¡œ ë¶„ë¥˜ëœ ë°ì´í„°]")
                normal_distribution = {}
                for min_score, max_score, label in score_ranges:
                    count = np.sum((normal_scores >= min_score) & (normal_scores < max_score))
                    ratio = count / len(normal_scores) * 100 if len(normal_scores) > 0 else 0
                    normal_distribution[label] = {
                        'count': int(count),
                        'ratio': float(ratio),
                        'range': [float(min_score) if min_score != -np.inf else None,
                                 float(max_score) if max_score != np.inf else None]
                    }
                    if count > 0:
                        self.log(f"    - {label:12s} [{min_score:6.0f} ~ {max_score:6.0f}]: "
                                f"{count:6,}ê°œ ({ratio:5.1f}%)")
                
                # ì´ìƒ ë°ì´í„°ì˜ ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬
                self.log("\n    [ì´ìƒìœ¼ë¡œ ë¶„ë¥˜ëœ ë°ì´í„°]")
                anomaly_distribution = {}
                for min_score, max_score, label in score_ranges:
                    count = np.sum((anomaly_scores >= min_score) & (anomaly_scores < max_score))
                    ratio = count / len(anomaly_scores) * 100 if len(anomaly_scores) > 0 else 0
                    anomaly_distribution[label] = {
                        'count': int(count),
                        'ratio': float(ratio),
                        'range': [float(min_score) if min_score != -np.inf else None,
                                 float(max_score) if max_score != np.inf else None]
                    }
                    if count > 0:
                        self.log(f"    - {label:12s} [{min_score:6.0f} ~ {max_score:6.0f}]: "
                                f"{count:6,}ê°œ ({ratio:5.1f}%)")
                
                # ì „ì²´ í†µí•© ë¶„í¬
                for min_score, max_score, label in score_ranges:
                    total_count = np.sum((scores >= min_score) & (scores < max_score))
                    normal_count = np.sum((normal_scores >= min_score) & (normal_scores < max_score))
                    anomaly_count = np.sum((anomaly_scores >= min_score) & (anomaly_scores < max_score))
                    
                    score_distribution[label] = {
                        'total': {
                            'count': int(total_count),
                            'ratio': float(total_count / len(scores) * 100)
                        },
                        'normal': {
                            'count': int(normal_count),
                            'ratio': float(normal_count / len(normal_scores) * 100) if len(normal_scores) > 0 else 0,
                            'of_total': float(normal_count / total_count * 100) if total_count > 0 else 0
                        },
                        'anomaly': {
                            'count': int(anomaly_count),
                            'ratio': float(anomaly_count / len(anomaly_scores) * 100) if len(anomaly_scores) > 0 else 0,
                            'of_total': float(anomaly_count / total_count * 100) if total_count > 0 else 0
                        },
                        'range': [float(min_score) if min_score != -np.inf else None,
                                 float(max_score) if max_score != np.inf else None]
                    }
                
                # êµì°¨ ë¶„ì„
                self.log("\n  ğŸ“Š ì •ìƒ/ì´ìƒ êµì°¨ ë¶„ì„:")
                
                # ì •ìƒìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆì§€ë§Œ ì ìˆ˜ê°€ ë‚®ì€ ë°ì´í„°
                normal_but_low_score = np.sum(normal_scores < 0)
                if normal_but_low_score > 0:
                    self.log(f"    - ì •ìƒ ë¶„ë¥˜ì§€ë§Œ ì ìˆ˜ < 0: {normal_but_low_score:,}ê°œ "
                            f"({normal_but_low_score/len(normal_scores)*100:.1f}%)")
                    
                    # ìƒì„¸ ë¶„í¬
                    for threshold in [-10, -20, -50, -100]:
                        count = np.sum(normal_scores < threshold)
                        if count > 0:
                            self.log(f"      â€¢ ì ìˆ˜ < {threshold}: {count:,}ê°œ "
                                    f"({count/len(normal_scores)*100:.2f}%)")
                
                # ì´ìƒìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆì§€ë§Œ ì ìˆ˜ê°€ ë†’ì€ ë°ì´í„°
                if len(anomaly_scores) > 0:
                    anomaly_but_high_score = np.sum(anomaly_scores > 0)
                    if anomaly_but_high_score > 0:
                        self.log(f"    - ì´ìƒ ë¶„ë¥˜ì§€ë§Œ ì ìˆ˜ > 0: {anomaly_but_high_score:,}ê°œ "
                                f"({anomaly_but_high_score/len(anomaly_scores)*100:.1f}%)")
                
                # ê²½ê³„ ê·¼ì²˜ ë°ì´í„° ë¶„ì„
                boundary_range = 10  # ê²°ì • ê²½ê³„ Â±10
                near_boundary = np.sum(np.abs(scores - decision_boundary) < boundary_range)
                self.log(f"    - ê²°ì • ê²½ê³„({decision_boundary:.2f}) Â±{boundary_range} ë²”ìœ„: "
                        f"{near_boundary:,}ê°œ ({near_boundary/len(scores)*100:.1f}%)")
                
                # 2ì°¨ ë¡œì§ ê²½ê³„ê°’ ì¶”ì²œ
                self.log("\n  ğŸ’¡ 2ì°¨ ë¡œì§ ê²½ê³„ê°’ ì¶”ì²œ:")
                
                # ë°©ë²• 1: ì •ìƒ ë°ì´í„°ì˜ í•˜ìœ„ í¼ì„¼íƒ€ì¼
                normal_lower_bound = np.percentile(normal_scores, 1)  # ì •ìƒì˜ í•˜ìœ„ 1%
                self.log(f"    - ì •ìƒ ë°ì´í„° í•˜ìœ„ 1%: {normal_lower_bound:.2f}")
                
                # ë°©ë²• 2: ì „ì²´ ë°ì´í„°ì˜ íŠ¹ì • í¼ì„¼íƒ€ì¼
                overall_p3 = np.percentile(scores, 3)
                self.log(f"    - ì „ì²´ ë°ì´í„° í•˜ìœ„ 3%: {overall_p3:.2f}")
                
                # ë°©ë²• 3: í‰ê·  - n*í‘œì¤€í¸ì°¨
                mean_minus_2std = np.mean(scores) - 2 * np.std(scores)
                mean_minus_3std = np.mean(scores) - 3 * np.std(scores)
                self.log(f"    - í‰ê·  - 2Ïƒ: {mean_minus_2std:.2f}")
                self.log(f"    - í‰ê·  - 3Ïƒ: {mean_minus_3std:.2f}")
                
                # ë°©ë²• 4: ì´ìƒ ë°ì´í„°ì˜ ìƒìœ„ ê²½ê³„
                if len(anomaly_scores) > 0:
                    anomaly_upper = np.percentile(anomaly_scores, 90)  # ì´ìƒì˜ ìƒìœ„ 10%
                    self.log(f"    - ì´ìƒ ë°ì´í„° ìƒìœ„ 10%: {anomaly_upper:.2f}")
                
                # ëª¨ë¸ ì •ë³´
                model_info = {
                    'machine_id': machine_id,
                    'sensor': sensor,
                    'train_samples': len(X_train),
                    'training_periods': self.training_periods,
                    'features': self.sensor_config[sensor]['features'],
                    'best_params': self.study.best_params,
                    'decision_boundary': float(decision_boundary),
                    'anomaly_ratio': float(anomaly_ratio),
                    'score_statistics': {
                        'mean': float(np.mean(scores)),
                        'std': float(np.std(scores)),
                        'min': float(np.min(scores)),
                        'max': float(np.max(scores))
                    },
                    'normal_score_statistics': {
                        'count': int(len(normal_scores)),
                        'mean': float(np.mean(normal_scores)),
                        'std': float(np.std(normal_scores)),
                        'min': float(np.min(normal_scores)),
                        'max': float(np.max(normal_scores)),
                        'percentiles': {
                            'p1': float(np.percentile(normal_scores, 1)),
                            'p5': float(np.percentile(normal_scores, 5)),
                            'p10': float(np.percentile(normal_scores, 10))
                        }
                    },
                    'anomaly_score_statistics': {
                        'count': int(len(anomaly_scores)),
                        'mean': float(np.mean(anomaly_scores)) if len(anomaly_scores) > 0 else None,
                        'std': float(np.std(anomaly_scores)) if len(anomaly_scores) > 0 else None,
                        'min': float(np.min(anomaly_scores)) if len(anomaly_scores) > 0 else None,
                        'max': float(np.max(anomaly_scores)) if len(anomaly_scores) > 0 else None
                    },
                    'score_percentiles': percentile_values,
                    'score_distribution': score_distribution,
                    'secondary_thresholds': {
                        'normal_p1': float(normal_lower_bound),
                        'overall_p3': float(overall_p3),
                        'mean_minus_2std': float(mean_minus_2std),
                        'mean_minus_3std': float(mean_minus_3std),
                        'anomaly_p90': float(anomaly_upper) if len(anomaly_scores) > 0 else None
                    },
                    'evaluation_skipped': False,
                    'trained_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
            
            # ëª¨ë¸ ì €ì¥
            self.log("\nëª¨ë¸ ì €ì¥ ì¤‘...")
            timestamp = datetime.now().strftime('%y%m%d_%H%M%S')
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            model_dir = f"./models/{machine_id}/{sensor}/current_model"
            scale_dir = f"./models/{machine_id}/{sensor}/current_scale"
            os.makedirs(model_dir, exist_ok=True)
            os.makedirs(scale_dir, exist_ok=True)
            
            # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
            for d in [model_dir, scale_dir]:
                for f in os.listdir(d):
                    os.remove(os.path.join(d, f))
            
            # ìƒˆ íŒŒì¼ ì €ì¥
            model_path = os.path.join(model_dir, f"{timestamp}_model.pkl")
            scaler_path = os.path.join(scale_dir, f"{timestamp}_scaler.pkl")
            info_path = os.path.join(model_dir, f"{timestamp}_model_info.json")
            
            joblib.dump(model, model_path)
            scaler.save(scaler_path)
            
            with open(info_path, 'w') as f:
                json.dump(model_info, f, indent=2)
            
            self.log(f"\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ:")
            self.log(f"  - ëª¨ë¸: {model_path}")
            self.log(f"  - ìŠ¤ì¼€ì¼ëŸ¬: {scaler_path}")
            self.log(f"  - ì •ë³´: {info_path}")
            
            # í˜„ì¬ ëª¨ë¸ ì •ë³´ ì €ì¥
            self.current_model_info = model_info
            
            # ì „ì²´ ì†Œìš” ì‹œê°„
            total_time = (datetime.now() - total_start_time).total_seconds()
            self.log(f"\nì „ì²´ í•™ìŠµ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ ({total_time/60:.1f}ë¶„)")
            
            self.progress_var.set("í•™ìŠµ ì™„ë£Œ!")
            messagebox.showinfo("ì™„ë£Œ", f"ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\në¨¸ì‹ : {machine_id}\nì„¼ì„œ: {sensor}")
            
        except Exception as e:
            self.log(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
            self.progress_var.set("í•™ìŠµ ì‹¤íŒ¨")
            messagebox.showerror("ì˜¤ë¥˜", f"í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            self.train_button.config(state='normal')
    
    def start_training(self):
        if not self.training_periods:
            messagebox.showerror("ì˜¤ë¥˜", "í•™ìŠµ ê¸°ê°„ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
            return
        
        if not self.conn:
            messagebox.showerror("ì˜¤ë¥˜", "DB ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        self.train_button.config(state='disabled')
        thread = threading.Thread(target=self.train_model)
        thread.daemon = True
        thread.start()
    
    def test_model(self):
        """ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        try:
            # ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ í™•ì¸
            model_path = self.model_file_var.get()
            scaler_path = self.scaler_file_var.get()
            
            if not model_path or not scaler_path:
                messagebox.showerror("ì˜¤ë¥˜", "ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
                return
            
            if not os.path.exists(model_path) or not os.path.exists(scaler_path):
                messagebox.showerror("ì˜¤ë¥˜", "ì„ íƒí•œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return
            
            # ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            self.log("ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì¤‘...")
            model = joblib.load(model_path)
            
            scaler = CustomRobustScaler()
            scaler.load(scaler_path)
            
            # ëª¨ë¸ ì •ë³´ ë¡œë“œ
            info_path = model_path.replace('_model.pkl', '_model_info.json')
            if os.path.exists(info_path):
                with open(info_path, 'r') as f:
                    model_info = json.load(f)
            else:
                # ê¸°ë³¸ ì •ë³´ ì‚¬ìš©
                model_info = {
                    'decision_boundary': -5.0,
                    'sensor': self.test_sensor_var.get()
                }
            
            # í…ŒìŠ¤íŠ¸ ì„¤ì •
            machine_id = self.test_machine_var.get()
            sensor = self.test_sensor_var.get()
            
            # ì„¼ì„œë³„ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ì„ íƒ
            table_name = f"normal_{sensor}_data"
            
            self.result_text.delete(1.0, tk.END)
            self.result_text.insert(tk.END, f"ëª¨ë¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼\n")
            self.result_text.insert(tk.END, f"{'='*60}\n")
            self.result_text.insert(tk.END, f"ë¨¸ì‹ : {machine_id}, ì„¼ì„œ: {sensor}\n")
            self.result_text.insert(tk.END, f"í…Œì´ë¸”: {table_name}\n")
            self.result_text.insert(tk.END, f"ìƒ˜í”Œë§: {self.sampling_info_var.get()}\n")
            self.result_text.insert(tk.END, f"ê²°ì • ê²½ê³„: {model_info.get('decision_boundary', 'N/A')}\n\n")
            
            for test_date in self.test_periods:
                self.result_text.insert(tk.END, f"\n[{test_date}]\n")
                
                # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ (ë¼ì¦ˆë² ë¦¬íŒŒì´ì™€ ë™ì¼í•œ ë°©ì‹)
                test_data = self.get_training_data(
                    machine_id, sensor,
                    f"{test_date} 00:00:00",
                    f"{test_date} 23:59:59"
                )
                
                if test_data is None or len(test_data) == 0:
                    self.result_text.insert(tk.END, "  - ë°ì´í„° ì—†ìŒ\n")
                    continue
                
                # ì˜ˆì¸¡
                X_test_scaled = scaler.transform(test_data)
                predictions = model.predict(X_test_scaled)
                scores = model.decision_function(X_test_scaled)
                
                # ì‹œê°„ëŒ€ë³„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
                self.log(f"\n{test_date} ì‹œê°„ëŒ€ë³„ ë¶„ì„ ì¤‘...")
                
                # ì›ë³¸ ë°ì´í„°ì— ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
                hourly_stats = self.analyze_hourly_anomalies(
                    machine_id, sensor, test_date, 
                    predictions, scores
                )
                
                # ê²°ê³¼ ë¶„ì„
                anomaly_count = np.sum(predictions == -1)
                anomaly_ratio = anomaly_count / len(predictions) * 100
                
                self.result_text.insert(tk.END, f"  - ìƒ˜í”Œ ìˆ˜: {len(test_data)}\n")
                self.result_text.insert(tk.END, f"  - ì´ìƒ íƒì§€: {anomaly_count}ê°œ ({anomaly_ratio:.1f}%)\n")
                self.result_text.insert(tk.END, f"  - ì ìˆ˜: í‰ê· ={np.mean(scores):.3f}, ")
                self.result_text.insert(tk.END, f"ìµœì†Œ={np.min(scores):.3f}, ìµœëŒ€={np.max(scores):.3f}\n")
                
                # ì ìˆ˜ ë¶„í¬
                self.result_text.insert(tk.END, f"  - ì ìˆ˜ < 0: {np.sum(scores < 0)}ê°œ\n")
                self.result_text.insert(tk.END, f"  - ì ìˆ˜ < -5: {np.sum(scores < -5)}ê°œ\n")
                self.result_text.insert(tk.END, f"  - ì ìˆ˜ < -10: {np.sum(scores < -10)}ê°œ\n")
                
                # ê²°ì • ê²½ê³„ ê¸°ì¤€ ì´ìƒ íƒì§€
                if 'decision_boundary' in model_info:
                    boundary = model_info['decision_boundary']
                    below_boundary = np.sum(scores < boundary)
                    self.result_text.insert(tk.END, f"  - ì ìˆ˜ < {boundary:.3f} (ê²°ì •ê²½ê³„): {below_boundary}ê°œ\n")
                
                # ì‹œê°„ëŒ€ë³„ ì´ìƒ íƒì§€ ê²°ê³¼
                if hourly_stats:
                    self.result_text.insert(tk.END, "\n  ì‹œê°„ëŒ€ë³„ ì´ìƒ íƒì§€:\n")
                    for hour, stats in hourly_stats.items():
                        if stats['anomaly_count'] > 0:
                            self.result_text.insert(
                                tk.END, 
                                f"    {hour:02d}ì‹œ: {stats['anomaly_count']}ê°œ/"
                                f"{stats['total_count']}ê°œ ({stats['anomaly_ratio']:.1f}%), "
                                f"ì ìˆ˜: {stats['mean_score']:.2f}\n"
                            )
                    
                    # ì´ìƒì´ ê°€ì¥ ë§ì´ ë°œìƒí•œ ì‹œê°„ëŒ€
                    peak_hour = max(hourly_stats.items(), 
                                  key=lambda x: x[1]['anomaly_count'])[0]
                    self.result_text.insert(tk.END, 
                                          f"  - ì´ìƒ ìµœë‹¤ ë°œìƒ ì‹œê°„: {peak_hour}ì‹œ\n")
                
            self.result_text.insert(tk.END, f"\n{'='*60}\n")
            self.result_text.insert(tk.END, "í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n")
            
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def analyze_hourly_anomalies(self, machine_id, sensor, test_date, predictions, scores):
        """ì‹œê°„ëŒ€ë³„ ì´ìƒ íƒì§€ ë¶„ì„"""
        try:
            # í•´ë‹¹ ë‚ ì§œì˜ ì›ë³¸ ë°ì´í„° ë‹¤ì‹œ ë¡œë“œ (ì‹œê°„ ì •ë³´ í¬í•¨)
            if sensor == 'acc':
                query = """
                SELECT time, x, y, z
                FROM normal_acc_data
                WHERE machine_id = %s
                AND DATE(time) = %s
                ORDER BY time
                """
            else:
                query = """
                SELECT time, mic_value
                FROM normal_mic_data
                WHERE machine_id = %s
                AND DATE(time) = %s
                ORDER BY time
                """
            
            df = pd.read_sql(query, self.conn, params=(machine_id, test_date))
            df['time'] = pd.to_datetime(df['time'])
            
            # 5ì´ˆ ìœˆë„ìš°ë¡œ ê·¸ë£¹í™” (í•™ìŠµê³¼ ë™ì¼)
            window_sec = self.sensor_config[sensor]['window_sec']
            df['window'] = df['time'].dt.floor(f'{window_sec}S')
            df['hour'] = df['time'].dt.hour
            
            # ê° ìœˆë„ìš°ì˜ ì‹œê°„ëŒ€ í• ë‹¹
            window_hours = df.groupby('window')['hour'].first().values
            
            # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ëŠ” ìœˆë„ìš°ë§Œ í•„í„°ë§
            window_samples = window_sec * 10  # DBëŠ” 10Hz
            valid_windows = df.groupby('window').size() >= window_samples * 0.8
            valid_indices = valid_windows[valid_windows].index
            
            # ìœ íš¨í•œ ìœˆë„ìš°ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
            window_idx = 0
            hourly_predictions = {h: [] for h in range(24)}
            hourly_scores = {h: [] for h in range(24)}
            
            for window in valid_indices:
                if window_idx < len(predictions):
                    hour = df[df['window'] == window]['hour'].iloc[0]
                    hourly_predictions[hour].append(predictions[window_idx])
                    hourly_scores[hour].append(scores[window_idx])
                    window_idx += 1
            
            # ì‹œê°„ëŒ€ë³„ í†µê³„ ê³„ì‚°
            hourly_stats = {}
            for hour in range(24):
                if hourly_predictions[hour]:
                    anomaly_count = sum(p == -1 for p in hourly_predictions[hour])
                    total_count = len(hourly_predictions[hour])
                    
                    hourly_stats[hour] = {
                        'anomaly_count': anomaly_count,
                        'total_count': total_count,
                        'anomaly_ratio': anomaly_count / total_count * 100,
                        'mean_score': np.mean(hourly_scores[hour]),
                        'min_score': np.min(hourly_scores[hour]),
                        'max_score': np.max(hourly_scores[hour])
                    }
            
            return hourly_stats
            
        except Exception as e:
            self.log(f"ì‹œê°„ëŒ€ë³„ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return None
    
    def start_testing(self):
        if not self.test_periods:
            messagebox.showerror("ì˜¤ë¥˜", "í…ŒìŠ¤íŠ¸ ë‚ ì§œë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
            return
        
        thread = threading.Thread(target=self.test_model)
        thread.daemon = True
        thread.start()

def main():
    root = tk.Tk()
    app = OCSVMTrainerGUI(root)
    root.mainloop()

if __name__ == "__main__":
    main()