#!/usr/bin/env python
# train_ocsvm_gui.py

import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext
import psycopg2
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import joblib
import json
import os
import threading
from sklearn.svm import OneClassSVM
from sklearn.model_selection import KFold
import optuna
from optuna import create_study
from tkcalendar import DateEntry
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import matplotlib.dates as mdates

# ë¼ì¦ˆë² ë¦¬íŒŒì´ì˜ CustomRobustScaler êµ¬í˜„
class CustomRobustScaler:
    def __init__(self):
        self.params = {}
    
    def fit(self, X):
        self.params = {}
        for i in range(X.shape[1]):
            col = X[:, i]
            q1, median, q3 = np.percentile(col, [25, 50, 75])
            iqr = q3 - q1
            self.params[i] = {'median': median, 'iqr': iqr}
    
    def transform(self, X):
        X_scaled = np.zeros_like(X, dtype=np.float64)
        for i in range(X.shape[1]):
            median = self.params[i]['median']
            iqr = self.params[i]['iqr']
            X_scaled[:, i] = (X[:, i] - median) / iqr
        return X_scaled
    
    def fit_transform(self, X):
        self.fit(X)
        return self.transform(X)
    
    def save(self, filepath):
        joblib.dump({"params": self.params}, filepath)
    
    def load(self, filepath):
        loaded = joblib.load(filepath)
        self.params = loaded["params"]
        
class OCSVMTrainerGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("OCSVM ëª¨ë¸ í•™ìŠµ ë„êµ¬")
        self.root.geometry("1400x900")
        
        # ë¡œê·¸ í…ìŠ¤íŠ¸ ìœ„ì ¯ì„ ë¨¼ì € ì´ˆê¸°í™”
        self.log_text = None
        
        # DB ì—°ê²°
        self.conn = None
        
        # ì„¤ì •ê°’
        self.sensor_config = {
            "mic": {
                "sampling_rate": 8000,
                "window_sec": 5,
                "features": ["mav", "rms", "peak", "amp_iqr"],
                "nu_range": [0.05, 0.3],  # ì •ìƒ ë°ì´í„°ë§Œ ìˆìœ¼ë¯€ë¡œ ë” ë„“ê²Œ
                "gamma_range": [0.01, 1.0]  # gamma ë²”ìœ„ë¥¼ ë†’ì—¬ì„œ RBF ì»¤ë„ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ë„ë¡
            },
            "acc": {
                "sampling_rate": 1666,
                "window_sec": 5,
                "features": ["x_peak", "x_crest_factor", "y_peak", "y_crest_factor", "z_peak", "z_crest_factor"],
                "nu_range": [0.05, 0.3],  # ì •ìƒ ë°ì´í„°ë§Œ ìˆìœ¼ë¯€ë¡œ ë” ë„“ê²Œ
                "gamma_range": [0.01, 1.0]  # gamma ë²”ìœ„ë¥¼ ë†’ì—¬ì„œ RBF ì»¤ë„ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ë„ë¡
            }
        }
        
        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì €ì¥
        self.training_periods = []
        self.test_periods = []
        
        # GUI ìƒì„±
        self.create_widgets()
        
        # DB ì—°ê²° (GUI ìƒì„± í›„)
        self.connect_db()
        
        # ë‚ ì§œ ë²”ìœ„ ë¡œë“œ
        self.load_date_range()
    
    def log(self, message):
        """ë¡œê·¸ ë©”ì‹œì§€ ì¶œë ¥"""
        if self.log_text:
            timestamp = datetime.now().strftime("%H:%M:%S")
            self.log_text.insert(tk.END, f"[{timestamp}] {message}\n")
            self.log_text.see(tk.END)
            self.root.update()
        else:
            print(f"[LOG] {message}")  # ë¡œê·¸ ìœ„ì ¯ì´ ì—†ì„ ë•Œ ì½˜ì†” ì¶œë ¥
        
        # ì½˜ì†”ì—ë„ í•­ìƒ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        print(f"[{timestamp}] {message}")
    
    def connect_db(self):
        """DB ì—°ê²° - íŠ¸ëœì­ì…˜ ì—ëŸ¬ ë°©ì§€"""
        try:
            self.conn = psycopg2.connect(
                host='localhost',
                port='5432',
                database='pdm_db',
                user='pdm_user',
                password='pdm_password'
            )
            # autocommit ì„¤ì •ìœ¼ë¡œ íŠ¸ëœì­ì…˜ ì—ëŸ¬ ë°©ì§€
            self.conn.autocommit = True
            self.log("âœ… DB ì—°ê²° ì„±ê³µ")
            
            # í…Œì´ë¸” ë° ë°ì´í„° í™•ì¸
            cur = self.conn.cursor()
            
            # í…Œì´ë¸” í™•ì¸
            cur.execute("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name IN ('normal_acc_data', 'normal_mic_data')
            """)
            tables = [row[0] for row in cur.fetchall()]
            self.log(f"í™•ì¸ëœ í…Œì´ë¸”: {tables}")
            
            # ê° í…Œì´ë¸”ì˜ ë°ì´í„° ìˆ˜ í™•ì¸
            for table in tables:
                cur.execute(f"SELECT COUNT(*) FROM {table}")
                count = cur.fetchone()[0]
                self.log(f"{table}: {count:,}ê°œ ë ˆì½”ë“œ")
            
            # ë¨¸ì‹ ë³„ ë°ì´í„° í™•ì¸
            for table in tables:
                cur.execute(f"""
                    SELECT machine_id, COUNT(*) as cnt, 
                           MIN(time) as min_time, MAX(time) as max_time
                    FROM {table}
                    GROUP BY machine_id
                """)
                for row in cur.fetchall():
                    self.log(f"{table} - {row[0]}: {row[1]:,}ê°œ, {row[2]} ~ {row[3]}")
            
        except Exception as e:
            self.log(f"DB ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            messagebox.showerror("DB ì—°ê²° ì‹¤íŒ¨", str(e))
    
    def create_widgets(self):
        # ë©”ì¸ ë…¸íŠ¸ë¶
        notebook = ttk.Notebook(self.root)
        notebook.pack(fill='both', expand=True, padx=5, pady=5)
        
        # í•™ìŠµ íƒ­
        train_tab = ttk.Frame(notebook)
        notebook.add(train_tab, text="ëª¨ë¸ í•™ìŠµ")
        self.create_train_tab(train_tab)
        
        # í…ŒìŠ¤íŠ¸ íƒ­
        test_tab = ttk.Frame(notebook)
        notebook.add(test_tab, text="ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        self.create_test_tab(test_tab)
        
        # ë¡œê·¸ íƒ­
        log_tab = ttk.Frame(notebook)
        notebook.add(log_tab, text="ë¡œê·¸")
        self.create_log_tab(log_tab)
    
    def create_train_tab(self, parent):
        # ì„¤ì • í”„ë ˆì„
        config_frame = ttk.LabelFrame(parent, text="í•™ìŠµ ì„¤ì •", padding="10")
        config_frame.pack(fill='x', padx=5, pady=5)
        
        # ë¨¸ì‹ /ì„¼ì„œ ì„ íƒ
        row = 0
        ttk.Label(config_frame, text="ë¨¸ì‹ :").grid(row=row, column=0, sticky=tk.W, padx=5)
        self.machine_var = tk.StringVar(value="CURINGOVEN_M1")
        machine_combo = ttk.Combobox(config_frame, textvariable=self.machine_var, 
                                    values=["CURINGOVEN_M1", "HOTCHAMBER_M2"], width=20)
        machine_combo.grid(row=row, column=1, padx=5)
        
        ttk.Label(config_frame, text="ì„¼ì„œ:").grid(row=row, column=2, sticky=tk.W, padx=5)
        self.sensor_var = tk.StringVar(value="acc")
        sensor_combo = ttk.Combobox(config_frame, textvariable=self.sensor_var,
                                   values=["acc", "mic"], width=10)
        sensor_combo.grid(row=row, column=3, padx=5)
        
        # ìµœì í™” ì„¤ì •
        row += 1
        ttk.Label(config_frame, text="Optuna Trials:").grid(row=row, column=0, sticky=tk.W, padx=5)
        self.trials_var = tk.IntVar(value=100)  # ë¼ì¦ˆë² ë¦¬íŒŒì´ì²˜ëŸ¼ ë” ë§ì€ trials
        trials_spinbox = ttk.Spinbox(config_frame, from_=10, to=500, increment=10,
                                    textvariable=self.trials_var, width=10)
        trials_spinbox.grid(row=row, column=1, padx=5)
        
        # ì„±ëŠ¥ í‰ê°€ ìŠ¤í‚µ ì˜µì…˜
        row += 1
        self.skip_eval_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(config_frame, text="ì„±ëŠ¥ í‰ê°€ ìŠ¤í‚µ (ë¹ ë¥¸ í•™ìŠµ)", 
                       variable=self.skip_eval_var).grid(row=row, column=0, columnspan=2, sticky=tk.W, padx=5)
        
        # í•™ìŠµ ê¸°ê°„ í”„ë ˆì„
        period_frame = ttk.LabelFrame(parent, text="í•™ìŠµ ê¸°ê°„ ì„¤ì •", padding="10")
        period_frame.pack(fill='both', expand=True, padx=5, pady=5)
        
        # ê¸°ê°„ ì…ë ¥
        input_frame = ttk.Frame(period_frame)
        input_frame.pack(fill='x')
        
        ttk.Label(input_frame, text="ì‹œì‘ì¼:").pack(side='left', padx=5)
        self.train_start_date = DateEntry(input_frame, width=12, background='darkblue',
                                         foreground='white', borderwidth=2,
                                         date_pattern='yyyy-mm-dd')
        self.train_start_date.pack(side='left', padx=5)
        
        ttk.Label(input_frame, text="ì¢…ë£Œì¼:").pack(side='left', padx=5)
        self.train_end_date = DateEntry(input_frame, width=12, background='darkblue',
                                       foreground='white', borderwidth=2,
                                       date_pattern='yyyy-mm-dd')
        self.train_end_date.pack(side='left', padx=5)
        
        ttk.Button(input_frame, text="ê¸°ê°„ ì¶”ê°€", 
                  command=self.add_training_period).pack(side='left', padx=20)
        
        # ê¸°ê°„ ëª©ë¡
        list_frame = ttk.Frame(period_frame)
        list_frame.pack(fill='both', expand=True, pady=10)
        
        # íŠ¸ë¦¬ë·°
        columns = ('ì‹œì‘ì¼', 'ì¢…ë£Œì¼', 'ì¼ìˆ˜')
        self.train_tree = ttk.Treeview(list_frame, columns=columns, height=8)
        self.train_tree.heading('#0', text='No')
        self.train_tree.heading('ì‹œì‘ì¼', text='ì‹œì‘ì¼')
        self.train_tree.heading('ì¢…ë£Œì¼', text='ì¢…ë£Œì¼')
        self.train_tree.heading('ì¼ìˆ˜', text='ì¼ìˆ˜')
        
        self.train_tree.column('#0', width=50)
        self.train_tree.column('ì‹œì‘ì¼', width=150)
        self.train_tree.column('ì¢…ë£Œì¼', width=150)
        self.train_tree.column('ì¼ìˆ˜', width=80)
        
        self.train_tree.pack(side='left', fill='both', expand=True)
        
        # ìŠ¤í¬ë¡¤ë°”
        scrollbar = ttk.Scrollbar(list_frame, orient='vertical', command=self.train_tree.yview)
        scrollbar.pack(side='right', fill='y')
        self.train_tree.configure(yscrollcommand=scrollbar.set)
        
        # ë²„íŠ¼ í”„ë ˆì„
        button_frame = ttk.Frame(period_frame)
        button_frame.pack(fill='x')
        
        ttk.Button(button_frame, text="ì„ íƒ ì‚­ì œ", 
                  command=self.remove_training_period).pack(side='left', padx=5)
        ttk.Button(button_frame, text="ëª¨ë‘ ì‚­ì œ", 
                  command=self.clear_training_periods).pack(side='left', padx=5)
        
        # í•™ìŠµ ë²„íŠ¼
        train_button_frame = ttk.Frame(parent)
        train_button_frame.pack(fill='x', padx=5, pady=10)
        
        self.train_button = ttk.Button(train_button_frame, text="ëª¨ë¸ í•™ìŠµ ì‹œì‘", 
                                      command=self.start_training)
        self.train_button.pack(side='left', padx=5)
        
        self.progress_var = tk.StringVar(value="ëŒ€ê¸° ì¤‘...")
        ttk.Label(train_button_frame, textvariable=self.progress_var).pack(side='left', padx=20)
    
    def create_test_tab(self, parent):
        # ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒ í”„ë ˆì„
        model_frame = ttk.LabelFrame(parent, text="ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒ", padding="10")
        model_frame.pack(fill='x', padx=5, pady=5)
        
        # ì„¼ì„œ íƒ€ì… ì„ íƒ
        ttk.Label(model_frame, text="ì„¼ì„œ íƒ€ì…:").grid(row=0, column=0, sticky=tk.W, padx=5)
        self.test_sensor_var = tk.StringVar(value="acc")
        sensor_radio_frame = ttk.Frame(model_frame)
        sensor_radio_frame.grid(row=0, column=1, columnspan=2, sticky=tk.W, padx=5)
        
        ttk.Radiobutton(sensor_radio_frame, text="ê°€ì†ë„ ì„¼ì„œ", variable=self.test_sensor_var, 
                        value="acc", command=self.update_test_settings).pack(side='left', padx=5)
        ttk.Radiobutton(sensor_radio_frame, text="ë§ˆì´í¬ ì„¼ì„œ", variable=self.test_sensor_var, 
                        value="mic", command=self.update_test_settings).pack(side='left', padx=5)
        
        # ëª¨ë¸ íŒŒì¼ ì„ íƒ
        ttk.Label(model_frame, text="ëª¨ë¸ íŒŒì¼:").grid(row=1, column=0, sticky=tk.W, padx=5)
        self.model_file_var = tk.StringVar()
        ttk.Entry(model_frame, textvariable=self.model_file_var, width=50).grid(row=1, column=1, padx=5)
        ttk.Button(model_frame, text="ì°¾ì•„ë³´ê¸°", 
                  command=self.browse_model_file).grid(row=1, column=2, padx=5)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ ì„ íƒ
        ttk.Label(model_frame, text="ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼:").grid(row=2, column=0, sticky=tk.W, padx=5)
        self.scaler_file_var = tk.StringVar()
        ttk.Entry(model_frame, textvariable=self.scaler_file_var, width=50).grid(row=2, column=1, padx=5)
        ttk.Button(model_frame, text="ì°¾ì•„ë³´ê¸°", 
                  command=self.browse_scaler_file).grid(row=2, column=2, padx=5)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • í”„ë ˆì„
        db_frame = ttk.LabelFrame(parent, text="ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •", padding="10")
        db_frame.pack(fill='x', padx=5, pady=5)
        
        # ë¨¸ì‹  ì„ íƒ
        ttk.Label(db_frame, text="ë¨¸ì‹ :").grid(row=0, column=0, sticky=tk.W, padx=5)
        self.test_machine_var = tk.StringVar(value="CURINGOVEN_M1")
        ttk.Combobox(db_frame, textvariable=self.test_machine_var, 
                    values=["CURINGOVEN_M1", "HOTCHAMBER_M2"], 
                    width=20).grid(row=0, column=1, padx=5)
        
        # ìƒ˜í”Œë§ ì„¤ì • í‘œì‹œ
        ttk.Label(db_frame, text="ìƒ˜í”Œë§ ì„¤ì •:").grid(row=1, column=0, sticky=tk.W, padx=5)
        self.sampling_info_var = tk.StringVar(value="ê°€ì†ë„: 1666Hz, 5ì´ˆ ìœˆë„ìš°")
        ttk.Label(db_frame, textvariable=self.sampling_info_var).grid(row=1, column=1, sticky=tk.W, padx=5)
        
        # íŠ¹ì§• ì •ë³´ í‘œì‹œ
        ttk.Label(db_frame, text="ì¶”ì¶œ íŠ¹ì§•:").grid(row=2, column=0, sticky=tk.W, padx=5)
        self.features_info_var = tk.StringVar(value="x_peak, x_crest_factor, y_peak, y_crest_factor, z_peak, z_crest_factor")
        features_label = ttk.Label(db_frame, textvariable=self.features_info_var, wraplength=400)
        features_label.grid(row=2, column=1, sticky=tk.W, padx=5)
        
        # í…ŒìŠ¤íŠ¸ ê¸°ê°„ í”„ë ˆì„
        test_period_frame = ttk.LabelFrame(parent, text="í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì„¤ì •", padding="10")
        test_period_frame.pack(fill='both', expand=True, padx=5, pady=5)
        
        # ê¸°ê°„ ì…ë ¥
        test_input_frame = ttk.Frame(test_period_frame)
        test_input_frame.pack(fill='x')
        
        ttk.Label(test_input_frame, text="ì‹œì‘ì¼:").pack(side='left', padx=5)
        self.test_start_date = DateEntry(test_input_frame, width=12, background='darkblue',
                                        foreground='white', borderwidth=2,
                                        date_pattern='yyyy-mm-dd')
        self.test_start_date.pack(side='left', padx=5)
        
        ttk.Label(test_input_frame, text="ì¢…ë£Œì¼:").pack(side='left', padx=5)
        self.test_end_date = DateEntry(test_input_frame, width=12, background='darkblue',
                                      foreground='white', borderwidth=2,
                                      date_pattern='yyyy-mm-dd')
        self.test_end_date.pack(side='left', padx=5)
        
        ttk.Label(test_input_frame, text="(ìµœëŒ€ 30ì¼)").pack(side='left', padx=10)
        
        # í…ŒìŠ¤íŠ¸ ë²„íŠ¼
        ttk.Button(parent, text="í…ŒìŠ¤íŠ¸ ì‹œì‘", 
                  command=self.start_testing).pack(pady=10)
        
        # í”Œë¡¯ ì˜ì—­
        plot_frame = ttk.LabelFrame(parent, text="ì‹œê³„ì—´ í”Œë¡¯", padding="10")
        plot_frame.pack(fill='both', expand=True, padx=5, pady=5)
        
        # matplotlib Figure
        self.fig, (self.ax1, self.ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)
        self.canvas = FigureCanvasTkAgg(self.fig, master=plot_frame)
        self.canvas.get_tk_widget().pack(fill='both', expand=True)
        
        test_button_frame = ttk.Frame(test_period_frame)
        test_button_frame.pack(fill='x')
        
        # ê²°ê³¼ í‘œì‹œ
        result_frame = ttk.LabelFrame(parent, text="í…ŒìŠ¤íŠ¸ ê²°ê³¼", padding="10")
        result_frame.pack(fill='x', padx=5, pady=5)
        
        self.result_text = scrolledtext.ScrolledText(result_frame, height=10)
        self.result_text.pack(fill='both', expand=True)
    
    def create_log_tab(self, parent):
        # ë¡œê·¸ í…ìŠ¤íŠ¸ ìœ„ì ¯ ìƒì„±
        self.log_text = scrolledtext.ScrolledText(parent, height=30)
        self.log_text.pack(fill='both', expand=True, padx=5, pady=5)
    
    def update_test_settings(self):
        """ì„¼ì„œ íƒ€ì…ì— ë”°ë¼ ì„¤ì • ì •ë³´ ì—…ë°ì´íŠ¸"""
        sensor = self.test_sensor_var.get()
        if sensor == "acc":
            self.sampling_info_var.set("ê°€ì†ë„: 1666Hz, 5ì´ˆ ìœˆë„ìš°")
            self.features_info_var.set("x_peak, x_crest_factor, y_peak, y_crest_factor, z_peak, z_crest_factor")
        else:
            self.sampling_info_var.set("ë§ˆì´í¬: 8000Hz, 5ì´ˆ ìœˆë„ìš°")
            self.features_info_var.set("mav, rms, peak, amp_iqr")
    
    def browse_model_file(self):
        """ëª¨ë¸ íŒŒì¼ ì°¾ì•„ë³´ê¸°"""
        from tkinter import filedialog
        filename = filedialog.askopenfilename(
            title="ëª¨ë¸ íŒŒì¼ ì„ íƒ",
            initialdir="./models",
            filetypes=[("Pickle files", "*.pkl"), ("All files", "*.*")]
        )
        if filename:
            self.model_file_var.set(filename)
            
            # ëª¨ë¸ ì •ë³´ íŒŒì¼ë„ ê°™ì´ ì°¾ê¸°
            info_path = filename.replace('_model.pkl', '_model_info.json')
            if os.path.exists(info_path):
                with open(info_path, 'r') as f:
                    model_info = json.load(f)
                    # ì„¼ì„œ íƒ€ì… ìë™ ì„¤ì •
                    self.test_sensor_var.set(model_info.get('sensor', 'acc'))
                    self.update_test_settings()
    
    def browse_scaler_file(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ ì°¾ì•„ë³´ê¸°"""
        from tkinter import filedialog
        filename = filedialog.askopenfilename(
            title="ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ ì„ íƒ",
            initialdir="./models",
            filetypes=[("Pickle files", "*.pkl"), ("All files", "*.*")]
        )
        if filename:
            self.scaler_file_var.set(filename)
    
    def load_date_range(self):
        """DBì—ì„œ ë°ì´í„° ë‚ ì§œ ë²”ìœ„ í™•ì¸"""
        try:
            cur = self.conn.cursor()
            cur.execute("""
                SELECT MIN(DATE(time)), MAX(DATE(time))
                FROM normal_acc_data
            """)
            min_date, max_date = cur.fetchone()
            
            if min_date and max_date:
                self.train_start_date.set_date(min_date)
                self.train_end_date.set_date(max_date)
                self.test_start_date.set_date(max_date - timedelta(days=7))
                self.test_end_date.set_date(max_date)
                
                self.log(f"ë°ì´í„° ë²”ìœ„: {min_date} ~ {max_date}")
        except Exception as e:
            self.log(f"ë‚ ì§œ ë²”ìœ„ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def check_data_exists(self):
        """ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        try:
            cur = self.conn.cursor()
            
            # ê° í…Œì´ë¸”ì˜ ë°ì´í„° ìˆ˜ í™•ì¸
            for table in ['acc_data', 'mic_data']:
                cur.execute(f"SELECT COUNT(*) FROM {table}")
                count = cur.fetchone()[0]
                self.log(f"{table}: {count}ê°œ ë ˆì½”ë“œ")
                
            # ë¨¸ì‹ ë³„ ë°ì´í„° ìˆ˜ í™•ì¸
            for table in ['acc_data', 'mic_data']:
                cur.execute(f"""
                    SELECT machine_id, COUNT(*), MIN(time), MAX(time)
                    FROM {table}
                    GROUP BY machine_id
                """)
                for row in cur.fetchall():
                    self.log(f"{table} - {row[0]}: {row[1]}ê°œ, {row[2]} ~ {row[3]}")
                    
        except Exception as e:
            self.log(f"ë°ì´í„° í™•ì¸ ì‹¤íŒ¨: {e}")
    
    def stratified_time_sampling(self, X_scaled, period_info, target_size):
        """ì‹œê°„ëŒ€ë³„ ê· ë“± ìƒ˜í”Œë§"""
        sample_indices = []
        
        # ê° ê¸°ê°„ì„ ì‹œê°„ëŒ€ë³„ë¡œ ë‚˜ëˆ„ê¸°
        for info in period_info:
            period_data_count = info['count']
            period_sample_size = int(target_size * (period_data_count / len(X_scaled)))
            
            if period_sample_size > 0:
                # í•˜ë£¨ë¥¼ 4ê°œ ì‹œê°„ëŒ€ë¡œ ë‚˜ëˆ„ê¸° (6ì‹œê°„ ë‹¨ìœ„)
                # ë˜ëŠ” í”¼í¬/ì˜¤í”„í”¼í¬ ì‹œê°„ëŒ€ë¡œ ë‚˜ëˆ„ê¸°
                period_start = info['start_idx']
                period_end = info['end_idx']
                
                # ê· ë“± ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
                indices = np.linspace(period_start, period_end-1, 
                                    period_sample_size, dtype=int)
                sample_indices.extend(indices)
        
        return np.array(sample_indices)
    
    def add_training_period(self):
        try:
            start_date = self.train_start_date.get_date()
            end_date = self.train_end_date.get_date()
            start = start_date.strftime("%Y-%m-%d")
            end = end_date.strftime("%Y-%m-%d")
            
            if start_date >= end_date:
                messagebox.showerror("ì˜¤ë¥˜", "ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤.")
                return
            
            days = (end_date - start_date).days + 1
            
            # íŠ¸ë¦¬ì— ì¶”ê°€
            item_id = self.train_tree.insert('', 'end', 
                                           text=str(len(self.training_periods) + 1),
                                           values=(start, end, days))
            
            self.training_periods.append((start, end))
            self.log(f"í•™ìŠµ ê¸°ê°„ ì¶”ê°€: {start} ~ {end} ({days}ì¼)")
            
        except ValueError:
            messagebox.showerror("ì˜¤ë¥˜", "ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. (YYYY-MM-DD)")
    
    def remove_training_period(self):
        selected = self.train_tree.selection()
        if selected:
            for item in selected:
                idx = self.train_tree.index(item)
                del self.training_periods[idx]
                self.train_tree.delete(item)
            
            # ë²ˆí˜¸ ì¬ì •ë ¬
            for i, item in enumerate(self.train_tree.get_children()):
                self.train_tree.item(item, text=str(i + 1))
    
    def clear_training_periods(self):
        self.training_periods.clear()
        for item in self.train_tree.get_children():
            self.train_tree.delete(item)

    
    def extract_features_acc(self, x_data, y_data, z_data):
        """ACC íŠ¹ì§• ì¶”ì¶œ - ë‹¤ìš´ìƒ˜í”Œë§ëœ ë°ì´í„°ìš©"""
        try:
            # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ
            if len(x_data) < 10:
                raise ValueError(f"ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŒ: {len(x_data)}ê°œ")
            
            features = []
            
            # Xì¶•
            x_rms = np.sqrt(np.mean(x_data**2))
            x_peak = np.max(np.abs(x_data))
            x_crest = x_peak / x_rms if x_rms > 1e-10 else 0
            
            # Yì¶•
            y_rms = np.sqrt(np.mean(y_data**2))
            y_peak = np.max(np.abs(y_data))
            y_crest = y_peak / y_rms if y_rms > 1e-10 else 0
            
            # Zì¶•
            z_rms = np.sqrt(np.mean(z_data**2))
            z_peak = np.max(np.abs(z_data))
            z_crest = z_peak / z_rms if z_rms > 1e-10 else 0
            
            return np.array([x_peak, x_crest, y_peak, y_crest, z_peak, z_crest])
            
        except Exception as e:
            self.log(f"ACC íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            raise
    
    def extract_features_mic(self, mic_data):
        """MIC íŠ¹ì§• ì¶”ì¶œ - ë‹¤ìš´ìƒ˜í”Œë§ëœ ë°ì´í„°ìš©"""
        try:
            # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ
            if len(mic_data) < 10:
                raise ValueError(f"ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŒ: {len(mic_data)}ê°œ")
            
            mav = np.mean(np.abs(mic_data))
            rms = np.sqrt(np.mean(mic_data**2))
            peak = np.max(np.abs(mic_data))
            
            # IQR ê³„ì‚° ì‹œ ë°ì´í„°ê°€ ì ìœ¼ë¯€ë¡œ ì¡°ì‹¬
            q1, q3 = np.percentile(np.abs(mic_data), [25, 75])
            amp_iqr = q3 - q1
            
            return np.array([mav, rms, peak, amp_iqr])
            
        except Exception as e:
            self.log(f"MIC íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            raise
    
    def get_training_data(self, machine_id, sensor, start_date, end_date):
        """DBì—ì„œ í•™ìŠµ ë°ì´í„° ì¶”ì¶œ - ë‹¤ìš´ìƒ˜í”Œë§ëœ ë°ì´í„° ì²˜ë¦¬"""
        window_sec = self.sensor_config[sensor]['window_sec']
        
        # DBì—ëŠ” 1ì´ˆì— 10ê°œì”©ë§Œ ì €ì¥ë˜ì–´ ìˆìŒ
        db_sampling_rate = 10  # 1ì´ˆì— 10ê°œ
        window_samples = window_sec * db_sampling_rate  # 5ì´ˆ * 10 = 50ê°œ
        
        # ì „ì²´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
        if sensor == 'acc':
            query = """
            SELECT time, x, y, z
            FROM normal_acc_data
            WHERE machine_id = %s
            AND time >= %s AND time <= %s
            ORDER BY time
            """
        else:  # mic
            query = """
            SELECT time, mic_value
            FROM normal_mic_data
            WHERE machine_id = %s
            AND time >= %s AND time <= %s
            ORDER BY time
            """
        
        self.log(f"ë°ì´í„° ì¶”ì¶œ ì¤‘: {machine_id}, {sensor}, {start_date} ~ {end_date}")
        
        try:
            start_time = datetime.now()
            df = pd.read_sql(query, self.conn, 
                            params=(machine_id, start_date, end_date))
            
            if df.empty:
                self.log(f"ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
                return None
            
            self.log(f"ì „ì²´ ë°ì´í„°: {len(df)}ê°œ ìƒ˜í”Œ")
            self.log(f"ë°ì´í„° ë¡œë“œ ì‹œê°„: {(datetime.now() - start_time).total_seconds():.1f}ì´ˆ")
            
            # Pythonì—ì„œ 5ì´ˆ ìœˆë„ìš°ë¡œ ë¶„í• 
            features_list = []
            
            # ì‹œê°„ì„ datetimeìœ¼ë¡œ ë³€í™˜
            df['time'] = pd.to_datetime(df['time'])
            
            # 5ì´ˆ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
            df['window'] = df['time'].dt.floor(f'{window_sec}S')
            
            # ìœˆë„ìš°ë³„ë¡œ ì²˜ë¦¬
            window_count = 0
            for window, group in df.groupby('window'):
                # ìµœì†Œ 40ê°œ ì´ìƒ (80%) ë°ì´í„°ê°€ ìˆëŠ” ìœˆë„ìš°ë§Œ ì‚¬ìš©
                if len(group) >= window_samples * 0.8:  
                    try:
                        if sensor == 'acc':
                            features = self.extract_features_acc(
                                group['x'].values,
                                group['y'].values,
                                group['z'].values
                            )
                        else:
                            features = self.extract_features_mic(group['mic_value'].values)
                        
                        features_list.append(features)
                        window_count += 1
                        
                        # ì§„í–‰ ìƒí™© ë¡œê·¸ (1000ê°œë§ˆë‹¤)
                        if window_count % 1000 == 0:
                            elapsed = (datetime.now() - start_time).total_seconds()
                            rate = window_count / elapsed if elapsed > 0 else 0
                            self.log(f"  ì²˜ë¦¬ì¤‘: {window_count:,}ê°œ ìœˆë„ìš° ({rate:.0f} windows/sec)")
                        
                        # GUI ì—…ë°ì´íŠ¸ (100ê°œë§ˆë‹¤)
                        if window_count % 100 == 0:
                            self.progress_var.set(f"ë°ì´í„° ì¶”ì¶œ ì¤‘: {start_date} ~ {end_date} ({window_count}ê°œ)")
                            self.root.update_idletasks()
                            
                    except Exception as e:
                        if window_count % 1000 == 0:
                            self.log(f"  ìœˆë„ìš° ì²˜ë¦¬ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        continue
            
            total_time = (datetime.now() - start_time).total_seconds()
            self.log(f"ì¶”ì¶œ ì™„ë£Œ: {len(features_list)}ê°œ ìœˆë„ìš° (ì´ {total_time:.1f}ì´ˆ)")
            
            return np.array(features_list) if features_list else None
            
        except Exception as e:
            self.log(f"ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def train_model(self):
        """ëª¨ë¸ í•™ìŠµ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
        try:
            total_start_time = datetime.now()
            machine_id = self.machine_var.get()
            sensor = self.sensor_var.get()
            n_trials = self.trials_var.get()
            
            self.log(f"\n{'='*60}")
            self.log(f"{machine_id} / {sensor} ëª¨ë¸ í•™ìŠµ ì‹œì‘")
            self.log(f"{'='*60}")
            
            # í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
            all_features = []
            period_info = []  # ê° ê¸°ê°„ë³„ ì •ë³´ ì €ì¥
            
            self.log(f"í•™ìŠµ ê¸°ê°„: {len(self.training_periods)}ê°œ")
            
            for idx, (start, end) in enumerate(self.training_periods):
                self.log(f"\n[{idx+1}/{len(self.training_periods)}] ê¸°ê°„: {start} ~ {end}")
                self.progress_var.set(f"ë°ì´í„° ì¶”ì¶œ ì¤‘: {start} ~ {end}")
                features = self.get_training_data(machine_id, sensor, start, end)
                if features is not None and len(features) > 0:
                    all_features.append(features)
                    period_info.append({
                        'period': f"{start} ~ {end}",
                        'start_idx': len(np.vstack(all_features[:-1])) if len(all_features) > 1 else 0,
                        'end_idx': len(np.vstack(all_features)),
                        'count': len(features)
                    })
                    self.log(f"âœ… ì¶”ì¶œ ì„±ê³µ: {len(features)}ê°œ ìœˆë„ìš°")
            
            if not all_features:
                self.log("âŒ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                self.progress_var.set("í•™ìŠµ ë°ì´í„° ì—†ìŒ")
                return
            
            X_train = np.vstack(all_features)
            self.log(f"\nì „ì²´ í•™ìŠµ ë°ì´í„°: {X_train.shape}")
            
            # ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ (fitë§Œ ìˆ˜í–‰)
            self.log("\nìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹œì‘...")
            self.progress_var.set("ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì¤‘...")
            scaler_start = datetime.now()
            scaler = CustomRobustScaler()
            scaler.fit(X_train)  # ì „ì²´ ë°ì´í„°ë¡œ ë²”ìœ„ë§Œ í•™ìŠµ
            scaler_time = (datetime.now() - scaler_start).total_seconds()
            self.log(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì™„ë£Œ ({scaler_time:.1f}ì´ˆ)")
            
            # ì „ì²´ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (í•œ ë²ˆì—)
            self.log("\nì „ì²´ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì‹œì‘...")
            self.progress_var.set("ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì¤‘...")
            scaling_start = datetime.now()
            X_scaled = scaler.transform(X_train)  # ì „ì²´ë¥¼ í•œ ë²ˆì— ë³€í™˜
            scaling_time = (datetime.now() - scaling_start).total_seconds()
            self.log(f"âœ… ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ ({scaling_time:.1f}ì´ˆ)")
            
            # ğŸ” ë””ë²„ê¹…: ì›ë³¸ ë°ì´í„°ì™€ ìŠ¤ì¼€ì¼ëœ ë°ì´í„° ë¹„êµ
            self.log("\nğŸ” [ë””ë²„ê¹…] ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ê²€ì¦:")
            for i, feature_name in enumerate(self.sensor_config[sensor]['features']):
                self.log(f"\n  [{feature_name}]")
                self.log(f"    ì›ë³¸ - min: {X_train[:, i].min():.2f}, max: {X_train[:, i].max():.2f}, "
                        f"mean: {X_train[:, i].mean():.2f}, std: {X_train[:, i].std():.2f}")
                self.log(f"    ìŠ¤ì¼€ì¼ - min: {X_scaled[:, i].min():.2f}, max: {X_scaled[:, i].max():.2f}, "
                        f"mean: {X_scaled[:, i].mean():.2f}, std: {X_scaled[:, i].std():.2f}")
                
                # ìŠ¤ì¼€ì¼ëŸ¬ íŒŒë¼ë¯¸í„° í™•ì¸
                self.log(f"    ìŠ¤ì¼€ì¼ëŸ¬ - median: {scaler.params[i]['median']:.2f}, "
                        f"IQR: {scaler.params[i]['iqr']:.2f}")
            
            # ğŸ” ì „ì²´ ìŠ¤ì¼€ì¼ëœ ë°ì´í„° í†µê³„
            self.log(f"\nğŸ” [ë””ë²„ê¹…] ì „ì²´ ìŠ¤ì¼€ì¼ëœ ë°ì´í„°:")
            self.log(f"  - ë²”ìœ„: [{X_scaled.min():.4f}, {X_scaled.max():.4f}]")
            self.log(f"  - í‰ê· : {X_scaled.mean():.4f}")
            self.log(f"  - í‘œì¤€í¸ì°¨: {X_scaled.std():.4f}")
            self.log(f"  - ì¤‘ì•™ê°’: {np.median(X_scaled):.4f}")
            self.log(f"  - 95% ë²”ìœ„: [{np.percentile(X_scaled, 2.5):.4f}, "
                    f"{np.percentile(X_scaled, 97.5):.4f}]")
            
            # OCSVM ìµœì í™”
            self.log(f"\ní•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ (Optuna {n_trials} trials)")
            optuna_start = datetime.now()
            self.progress_var.set(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘... (0/{n_trials})")
            
            opt_config = self.sensor_config[sensor]
            nu_range = opt_config['nu_range']
            gamma_range = opt_config['gamma_range']
            
            # ì‹œê°„ì  ë¶„í¬ë¥¼ ê³ ë ¤í•œ ê³„ì¸µì  ìƒ˜í”Œë§
            target_sample_size = min(20000, int(len(X_scaled) * 0.1))  # ìµœëŒ€ 20,000ê°œ
            
            if target_sample_size < len(X_scaled):
                self.log(f"\nğŸ“Š ê³„ì¸µì  ìƒ˜í”Œë§ ì‹œì‘: {len(X_scaled)}ê°œ â†’ {target_sample_size}ê°œ")
                
                sample_indices = []
                
                # ê° ê¸°ê°„ë³„ë¡œ ë¹„ë¡€ì ìœ¼ë¡œ ìƒ˜í”Œë§
                for info in period_info:
                    period_ratio = info['count'] / len(X_scaled)
                    period_sample_size = int(target_sample_size * period_ratio)
                    
                    if period_sample_size > 0:
                        # í•´ë‹¹ ê¸°ê°„ ë‚´ì—ì„œ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
                        period_indices = np.arange(info['start_idx'], info['end_idx'])
                        
                        if period_sample_size < len(period_indices):
                            # ì‹œê°„ ê°„ê²©ì„ ë‘ê³  ê· ë“±í•˜ê²Œ ì„ íƒ
                            step = len(period_indices) // period_sample_size
                            selected = period_indices[::step][:period_sample_size]
                        else:
                            selected = period_indices
                        
                        sample_indices.extend(selected)
                        self.log(f"  - {info['period']}: {len(selected)}ê°œ ìƒ˜í”Œ")
                
                sample_indices = np.array(sample_indices)
                
                # ìƒ˜í”Œë§ëœ ë°ì´í„° ìŠ¤ì¼€ì¼ë§
                X_sample = scaler.transform(X_train[sample_indices])
                self.log(f"âœ… ì´ {len(X_sample)}ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì™„ë£Œ")
            else:
                X_sample = X_scaled
                self.log(f"ğŸ“Š ë°ì´í„°ê°€ ì¶©ë¶„íˆ ì‘ì•„ ì „ì²´ ì‚¬ìš©: {len(X_scaled)}ê°œ")
            
            trial_count = 0
            # study ë³€ìˆ˜ë¥¼ í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ë§Œë“¤ì–´ objective í•¨ìˆ˜ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ
            self.study = create_study(direction='maximize')
            
            # K-fold ì„¤ì • (ë¼ì¦ˆë² ë¦¬íŒŒì´ëŠ” 3ì„ ì‚¬ìš©)
            n_splits = 3
            
            def objective(trial):
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                current_trial = len(self.study.trials)
                self.progress_var.set(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘... ({current_trial}/{n_trials})")
                self.root.update_idletasks()
                
                nu = trial.suggest_float('nu', nu_range[0], nu_range[1], log=True)
                gamma = trial.suggest_float('gamma', gamma_range[0], gamma_range[1], log=True)
                
                # K-Fold ì‚¬ìš© (ë¼ì¦ˆë² ë¦¬íŒŒì´ì™€ ë™ì¼)
                if n_splits <= 1:
                    model = OneClassSVM(kernel='rbf', nu=nu, gamma=gamma, cache_size=200)
                    model.fit(X_sample)
                    preds = model.predict(X_sample)
                    return np.mean(preds == -1)  # ì´ìƒì¹˜ ë¹„ìœ¨ ìµœì†Œí™”
                
                # K-Foldê°€ ìˆëŠ” ê²½ìš°
                from sklearn.model_selection import KFold
                kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
                scores = []
                for train_idx, test_idx in kf.split(X_sample):
                    X_train, X_test = X_sample[train_idx], X_sample[test_idx]
                    model = OneClassSVM(kernel='rbf', nu=nu, gamma=gamma, cache_size=200)
                    model.fit(X_train)
                    preds = model.predict(X_test)
                    scores.append(np.mean(preds == -1))
                
                return np.mean(scores)
            
            # Optuna ì½œë°± í•¨ìˆ˜
            def optuna_callback(study, trial):
                nonlocal trial_count
                trial_count += 1
                if trial_count % 10 == 0 or trial_count <= 5:
                    self.log(f"  Trial {trial_count}: nu={trial.params['nu']:.4f}, "
                            f"gamma={trial.params['gamma']:.6f}, score={trial.value:.4f}")
            
            # ìµœì í™” ì‹¤í–‰ (direction='minimize'ë¡œ ë³€ê²½)
            self.study = create_study(direction='minimize')  # ì´ìƒì¹˜ ë¹„ìœ¨ ìµœì†Œí™”
            self.study.optimize(objective, n_trials=n_trials, callbacks=[optuna_callback])
            
            optuna_time = (datetime.now() - optuna_start).total_seconds()
            self.log(f"\nâœ… ìµœì í™” ì™„ë£Œ ({optuna_time:.1f}ì´ˆ)")
            self.log(f"ìµœì  íŒŒë¼ë¯¸í„°: nu={self.study.best_params['nu']:.4f}, "
                    f"gamma={self.study.best_params['gamma']:.6f}")
            self.log(f"ìµœì  ì ìˆ˜: {self.study.best_value:.4f}")
            
            # ìµœì  ëª¨ë¸ë¡œ ì „ì²´ ë°ì´í„° í•™ìŠµ
            self.progress_var.set("ìµœì¢… ëª¨ë¸ í•™ìŠµ ì¤‘...")
            self.log("\nğŸ” ìµœì¢… ëª¨ë¸ í•™ìŠµ ë°ì´í„° í™•ì¸...")
            best_nu = self.study.best_params['nu']
            best_gamma = self.study.best_params['gamma']
            
            # í•™ìŠµ ì§ì „ ë°ì´í„° í™•ì¸
            self.log(f"\nğŸ” [ì¤‘ìš”] ìµœì¢… í•™ìŠµ ë°ì´í„° ê²€ì¦:")
            self.log(f"  - X_scaled shape: {X_scaled.shape}")
            self.log(f"  - X_scaled dtype: {X_scaled.dtype}")
            self.log(f"  - X_scaled ì „ì²´ ë²”ìœ„: [{X_scaled.min():.4f}, {X_scaled.max():.4f}]")
            self.log(f"  - X_scaled ì „ì²´ í‰ê· : {X_scaled.mean():.4f}")
            self.log(f"  - X_scaled ì „ì²´ í‘œì¤€í¸ì°¨: {X_scaled.std():.4f}")
            
            # ì²« 5ê°œ ìƒ˜í”Œ ìƒì„¸ í™•ì¸
            self.log(f"\nğŸ” ì²« 5ê°œ ìƒ˜í”Œ ìƒì„¸ í™•ì¸:")
            for i in range(min(5, len(X_scaled))):
                self.log(f"  ìƒ˜í”Œ {i}: {X_scaled[i]}")
            
            # ê° íŠ¹ì§•ë³„ ë²”ìœ„ í™•ì¸
            self.log(f"\nğŸ” ê° íŠ¹ì§•ë³„ ìŠ¤ì¼€ì¼ëœ ë²”ìœ„:")
            for i, feature_name in enumerate(self.sensor_config[sensor]['features']):
                self.log(f"  [{feature_name}] ë²”ìœ„: [{X_scaled[:, i].min():.4f}, {X_scaled[:, i].max():.4f}], "
                        f"í‰ê· : {X_scaled[:, i].mean():.4f}, í‘œì¤€í¸ì°¨: {X_scaled[:, i].std():.4f}")
            
            model = OneClassSVM(kernel='rbf', nu=best_nu, gamma=best_gamma, cache_size=200)
            self.log(f"\nìµœì¢… ëª¨ë¸ í•™ìŠµ ì‹œì‘ (nu={best_nu:.4f}, gamma={best_gamma:.6f})...")
            
            # ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬: ê¸°ê°„ë³„ ë¹„ë¡€ ìƒ˜í”Œë§
            max_train_samples = 10000
            if len(X_scaled) > max_train_samples:
                self.log(f"\nğŸ“Š ê¸°ê°„ë³„ ë¹„ë¡€ ìƒ˜í”Œë§: {len(X_scaled):,}ê°œ â†’ {max_train_samples:,}ê°œ")
                
                sampled_indices = []
                for info in period_info:
                    period_start = info['start_idx']
                    period_end = info['end_idx']
                    period_count = period_end - period_start
                    
                    # ê° ê¸°ê°„ì—ì„œ ë¹„ë¡€ì ìœ¼ë¡œ ìƒ˜í”Œ
                    period_sample_size = int(max_train_samples * (period_count / len(X_scaled)))
                    if period_sample_size > 0:
                        # ê· ë“± ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
                        indices = np.linspace(period_start, period_end-1, period_sample_size, dtype=int)
                        sampled_indices.extend(indices)
                        self.log(f"  - {info['period']}: {period_sample_size}ê°œ ìƒ˜í”Œ")
                
                sampled_indices = np.array(sampled_indices)
                X_train_final = X_scaled[sampled_indices]
            else:
                X_train_final = X_scaled
                
            model.fit(X_train_final)
            self.log("âœ… ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            
            # ğŸ” ë””ë²„ê¹…: ëª¨ë¸ ì •ë³´
            self.log(f"\nğŸ” [ë””ë²„ê¹…] ëª¨ë¸ ì •ë³´:")
            self.log(f"  - Support vectors: {model.support_vectors_.shape[0]}ê°œ")
            self.log(f"  - Dual coefficients ë²”ìœ„: [{model.dual_coef_.min():.4f}, "
                    f"{model.dual_coef_.max():.4f}]")
            if hasattr(model, 'offset_'):
                self.log(f"  - Offset: {model.offset_[0]:.4f}")
                
            # Offset ê²½ê³ 
            if hasattr(model, 'offset_') and abs(model.offset_[0]) > 10:
                self.log(f"\nâš ï¸ ê²½ê³ : Offsetì´ ë¹„ì •ìƒì ìœ¼ë¡œ í½ë‹ˆë‹¤! ({model.offset_[0]:.4f})")
                self.log(f"  â†’ ìŠ¤ì¼€ì¼ë§ì´ ì œëŒ€ë¡œ ì•ˆ ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
                self.log(f"  â†’ ë˜ëŠ” gammaê°€ ë„ˆë¬´ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (ì„ íƒì )
            skip_evaluation = self.skip_eval_var.get()  # GUI ì²´í¬ë°•ìŠ¤ ê°’ ì‚¬ìš©
            
            # ğŸ” í•­ìƒ ì‘ì€ ìƒ˜í”Œë¡œ score ë¶„í¬ í™•ì¸
            sample_size = min(1000, len(X_scaled))
            sample_indices = np.random.choice(len(X_scaled), sample_size, replace=False)
            debug_scores = model.decision_function(X_scaled[sample_indices])
            self.log(f"\nğŸ” [ë””ë²„ê¹…] ìƒ˜í”Œ {sample_size}ê°œì˜ score ë¶„í¬:")
            self.log(f"  - ë²”ìœ„: [{debug_scores.min():.2f}, {debug_scores.max():.2f}]")
            
            if skip_evaluation:
                self.log("\nâš¡ ì„±ëŠ¥ í‰ê°€ ë‹¨ê³„ ìŠ¤í‚µ (ë¹ ë¥¸ í•™ìŠµ ëª¨ë“œ)")
                # ê°„ë‹¨í•œ ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€ëµì ì¸ ì„±ëŠ¥ë§Œ í™•ì¸
                sample_size = min(10000, len(X_scaled))
                sample_indices = np.random.choice(len(X_scaled), sample_size, replace=False)
                sample_scores = model.decision_function(X_scaled[sample_indices])
                # IQR ë°©ì‹ ì‚¬ìš©
                q1, q3 = np.percentile(sample_scores, [25, 75])
                iqr = q3 - q1
                decision_boundary = q1 - 3 * iqr
                
                # ğŸ” ë””ë²„ê¹…: boundary ê³„ì‚° ê³¼ì •
                self.log(f"\nğŸ” [ë””ë²„ê¹…] Decision Boundary ê³„ì‚°:")
                self.log(f"  - Score ë¶„í¬: min={sample_scores.min():.2f}, max={sample_scores.max():.2f}")
                self.log(f"  - Q1: {q1:.2f}")
                self.log(f"  - Q3: {q3:.2f}")
                self.log(f"  - IQR: {iqr:.2f}")
                self.log(f"  - Boundary = Q1 - 3*IQR = {q1:.2f} - 3*{iqr:.2f} = {decision_boundary:.2f}")
                
                model_info = {
                    'machine_id': machine_id,
                    'sensor': sensor,
                    'train_samples': len(X_train),
                    'training_periods': self.training_periods,
                    'features': self.sensor_config[sensor]['features'],
                    'best_params': self.study.best_params,
                    'decision_boundary': float(decision_boundary),
                    'boundary_method': 'IQR',
                    'boundary_stats': {
                        'q1': float(q1),
                        'q3': float(q3),
                        'iqr': float(iqr)
                    },
                    'evaluation_skipped': True,
                    'trained_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
            else:
                # ì „ì²´ ì„±ëŠ¥ í‰ê°€ (ë°°ì¹˜ ì²˜ë¦¬)
                self.log("\nëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
                eval_start = datetime.now()
                
                batch_size = 10000
                predictions = []
                scores = []
                
                self.log(f"ì „ì²´ {len(X_scaled):,}ê°œ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰ (ë°°ì¹˜ í¬ê¸°: {batch_size:,})")
                
                for i in range(0, len(X_scaled), batch_size):
                    batch_end = min(i + batch_size, len(X_scaled))
                    batch = X_scaled[i:batch_end]
                    
                    # ë°°ì¹˜ ì˜ˆì¸¡
                    batch_predictions = model.predict(batch)
                    batch_scores = model.decision_function(batch)
                    
                    predictions.extend(batch_predictions)
                    scores.extend(batch_scores)
                    
                    # ì§„í–‰ ìƒí™© ë¡œê·¸ (10ê°œ ë°°ì¹˜ë§ˆë‹¤)
                    if (i // batch_size + 1) % 10 == 0 or batch_end == len(X_scaled):
                        progress = batch_end / len(X_scaled) * 100
                        elapsed = (datetime.now() - eval_start).total_seconds()
                        rate = batch_end / elapsed if elapsed > 0 else 0
                        eta = (len(X_scaled) - batch_end) / rate if rate > 0 else 0
                        
                        self.log(f"  ì˜ˆì¸¡ ì§„í–‰: {batch_end:,}/{len(X_scaled):,} ({progress:.1f}%) "
                                f"- {rate:.0f} samples/sec, ETA: {eta:.0f}ì´ˆ")
                        self.progress_var.set(f"ì„±ëŠ¥ í‰ê°€ ì¤‘... {progress:.1f}%")
                        self.root.update_idletasks()
                
                # numpy ë°°ì—´ë¡œ ë³€í™˜
                predictions = np.array(predictions)
                scores = np.array(scores)
                
                eval_time = (datetime.now() - eval_start).total_seconds()
                self.log(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ ({eval_time:.1f}ì´ˆ)")
                
                # ê²°ì • ê²½ê³„ ê³„ì‚°
                self.log("\nê²°ì • ê²½ê³„ ê³„ì‚° ì¤‘...")
                # IQR ë°©ì‹ìœ¼ë¡œ ë³€ê²½ (ë¼ì¦ˆë² ë¦¬íŒŒì´ì™€ ë™ì¼)
                q1, q3 = np.percentile(scores, [25, 75])
                iqr = q3 - q1
                decision_boundary = q1 - 3 * iqr
                
                # ğŸ” ë””ë²„ê¹…: ì „ì²´ í‰ê°€ì—ì„œë„ ê²½ê³„ê°’ í™•ì¸
                self.log(f"\nğŸ” [ë””ë²„ê¹…] ì „ì²´ í‰ê°€ Decision Boundary:")
                self.log(f"  - ì „ì²´ Score ë¶„í¬:")
                self.log(f"    â€¢ ë²”ìœ„: [{np.min(scores):.2f}, {np.max(scores):.2f}]")
                self.log(f"    â€¢ í‰ê· : {np.mean(scores):.2f}")
                self.log(f"    â€¢ í‘œì¤€í¸ì°¨: {np.std(scores):.2f}")
                self.log(f"  - Percentiles:")
                for p in [1, 5, 10, 25, 50, 75, 90, 95, 99]:
                    self.log(f"    â€¢ P{p}: {np.percentile(scores, p):.2f}")
                self.log(f"  - IQR ê³„ì‚°: Q1={q1:.2f}, Q3={q3:.2f}, IQR={iqr:.2f}")
                self.log(f"  - Boundary = {q1:.2f} - 3*{iqr:.2f} = {decision_boundary:.2f}")
                
                self.log(f"ê²°ì • ê²½ê³„: {decision_boundary:.6f} (Q1={q1:.6f}, Q3={q3:.6f}, IQR={iqr:.6f})")
                
                # ì´ìƒì¹˜ ë¹„ìœ¨ ê³„ì‚°
                predictions = model.predict(X_scaled)
                anomaly_ratio = np.sum(predictions == -1) / len(predictions) * 100
                
                self.log(f"âœ… í•™ìŠµ ì™„ë£Œ!")
                self.log(f"  - ì´ìƒì¹˜ ë¹„ìœ¨: {anomaly_ratio:.2f}%")
                self.log(f"  - ê²°ì • ê²½ê³„: {decision_boundary:.4f}")
                self.log(f"  - ì ìˆ˜ ë²”ìœ„: [{np.min(scores):.4f}, {np.max(scores):.4f}]")
                self.log(f"  - ì ìˆ˜ í‰ê· Â±í‘œì¤€í¸ì°¨: {np.mean(scores):.4f} Â± {np.std(scores):.4f}")
                
                # ê¸°ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
                self.log("\nğŸ“Š ê¸°ê°„ë³„ ì„±ëŠ¥:")
                for info in period_info:
                    start_idx = info['start_idx']
                    end_idx = info['end_idx']
                    period_scores = scores[start_idx:end_idx]
                    period_predictions = predictions[start_idx:end_idx]
                    period_anomaly_ratio = np.sum(period_predictions == -1) / len(period_predictions) * 100
                    
                    self.log(f"  - {info['period']}: ì´ìƒ {period_anomaly_ratio:.1f}%, "
                            f"ì ìˆ˜ {np.mean(period_scores):.2f}Â±{np.std(period_scores):.2f}")
                
                # 2ì°¨ ë¡œì§ì„ ìœ„í•œ ìƒì„¸ í†µê³„ ë¶„ì„
                self.log("\nğŸ“Š 2ì°¨ ë¡œì§ ê²½ê³„ê°’ ì„¤ì •ì„ ìœ„í•œ ë¶„ì„:")
                
                # ì •ìƒ/ì´ìƒ ë°ì´í„° ë¶„ë¦¬
                normal_scores = scores[predictions == 1]
                anomaly_scores = scores[predictions == -1]
                
                self.log(f"  ì •ìƒ ë°ì´í„° ì ìˆ˜ ë¶„í¬:")
                self.log(f"    - ê°œìˆ˜: {len(normal_scores):,}ê°œ ({len(normal_scores)/len(scores)*100:.1f}%)")
                self.log(f"    - í‰ê· Â±í‘œì¤€í¸ì°¨: {np.mean(normal_scores):.2f} Â± {np.std(normal_scores):.2f}")
                self.log(f"    - ìµœì†Œ/ìµœëŒ€: {np.min(normal_scores):.2f} / {np.max(normal_scores):.2f}")
                
                self.log(f"  ì´ìƒ ë°ì´í„° ì ìˆ˜ ë¶„í¬:")
                self.log(f"    - ê°œìˆ˜: {len(anomaly_scores):,}ê°œ ({len(anomaly_scores)/len(scores)*100:.1f}%)")
                self.log(f"    - í‰ê· Â±í‘œì¤€í¸ì°¨: {np.mean(anomaly_scores):.2f} Â± {np.std(anomaly_scores):.2f}")
                self.log(f"    - ìµœì†Œ/ìµœëŒ€: {np.min(anomaly_scores):.2f} / {np.max(anomaly_scores):.2f}")
                
                # í¼ì„¼íƒ€ì¼ ê¸°ë°˜ ê²½ê³„ê°’ í›„ë³´
                percentiles = [0.1, 0.5, 1, 2, 3, 5, 10, 15, 20]
                percentile_values = {}
                
                self.log("\n  ì „ì²´ ì ìˆ˜ í¼ì„¼íƒ€ì¼:")
                for p in percentiles:
                    val = np.percentile(scores, p)
                    percentile_values[f"p{p}"] = float(val)
                    self.log(f"    - {p:5.1f}%: {val:8.2f}")
                
                # ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬
                self.log("\n  ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬:")
                score_ranges = [
                    (-np.inf, -100, "ê·¹ì‹¬í•œ ì´ìƒ"),
                    (-100, -50, "ì‹¬ê°í•œ ì´ìƒ"),
                    (-50, -20, "ì¤‘ê°„ ì´ìƒ"),
                    (-20, -10, "ê²½ë¯¸í•œ ì´ìƒ"),
                    (-10, 0, "ì˜ì‹¬ êµ¬ê°„"),
                    (0, 100, "ì •ìƒ ë²”ìœ„"),
                    (100, np.inf, "ë§¤ìš° ì •ìƒ")
                ]
                
                score_distribution = {}
                self.log("    [ì „ì²´ ë°ì´í„°]")
                for min_score, max_score, label in score_ranges:
                    count = np.sum((scores >= min_score) & (scores < max_score))
                    ratio = count / len(scores) * 100
                    self.log(f"    - {label:12s} [{min_score:6.0f} ~ {max_score:6.0f}]: "
                            f"{count:6,}ê°œ ({ratio:5.1f}%)")
                
                # ì •ìƒ ë°ì´í„°ì˜ ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬
                self.log("\n    [ì •ìƒìœ¼ë¡œ ë¶„ë¥˜ëœ ë°ì´í„°]")
                normal_distribution = {}
                for min_score, max_score, label in score_ranges:
                    count = np.sum((normal_scores >= min_score) & (normal_scores < max_score))
                    ratio = count / len(normal_scores) * 100 if len(normal_scores) > 0 else 0
                    normal_distribution[label] = {
                        'count': int(count),
                        'ratio': float(ratio),
                        'range': [float(min_score) if min_score != -np.inf else None,
                                 float(max_score) if max_score != np.inf else None]
                    }
                    if count > 0:
                        self.log(f"    - {label:12s} [{min_score:6.0f} ~ {max_score:6.0f}]: "
                                f"{count:6,}ê°œ ({ratio:5.1f}%)")
                
                # ì´ìƒ ë°ì´í„°ì˜ ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬
                self.log("\n    [ì´ìƒìœ¼ë¡œ ë¶„ë¥˜ëœ ë°ì´í„°]")
                anomaly_distribution = {}
                for min_score, max_score, label in score_ranges:
                    count = np.sum((anomaly_scores >= min_score) & (anomaly_scores < max_score))
                    ratio = count / len(anomaly_scores) * 100 if len(anomaly_scores) > 0 else 0
                    anomaly_distribution[label] = {
                        'count': int(count),
                        'ratio': float(ratio),
                        'range': [float(min_score) if min_score != -np.inf else None,
                                 float(max_score) if max_score != np.inf else None]
                    }
                    if count > 0:
                        self.log(f"    - {label:12s} [{min_score:6.0f} ~ {max_score:6.0f}]: "
                                f"{count:6,}ê°œ ({ratio:5.1f}%)")
                
                # ì „ì²´ í†µí•© ë¶„í¬
                for min_score, max_score, label in score_ranges:
                    total_count = np.sum((scores >= min_score) & (scores < max_score))
                    normal_count = np.sum((normal_scores >= min_score) & (normal_scores < max_score))
                    anomaly_count = np.sum((anomaly_scores >= min_score) & (anomaly_scores < max_score))
                    
                    score_distribution[label] = {
                        'total': {
                            'count': int(total_count),
                            'ratio': float(total_count / len(scores) * 100)
                        },
                        'normal': {
                            'count': int(normal_count),
                            'ratio': float(normal_count / len(normal_scores) * 100) if len(normal_scores) > 0 else 0,
                            'of_total': float(normal_count / total_count * 100) if total_count > 0 else 0
                        },
                        'anomaly': {
                            'count': int(anomaly_count),
                            'ratio': float(anomaly_count / len(anomaly_scores) * 100) if len(anomaly_scores) > 0 else 0,
                            'of_total': float(anomaly_count / total_count * 100) if total_count > 0 else 0
                        },
                        'range': [float(min_score) if min_score != -np.inf else None,
                                 float(max_score) if max_score != np.inf else None]
                    }
                
                # êµì°¨ ë¶„ì„
                self.log("\n  ğŸ“Š ì •ìƒ/ì´ìƒ êµì°¨ ë¶„ì„:")
                
                # ì •ìƒìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆì§€ë§Œ ì ìˆ˜ê°€ ë‚®ì€ ë°ì´í„°
                normal_but_low_score = np.sum(normal_scores < 0)
                if normal_but_low_score > 0:
                    self.log(f"    - ì •ìƒ ë¶„ë¥˜ì§€ë§Œ ì ìˆ˜ < 0: {normal_but_low_score:,}ê°œ "
                            f"({normal_but_low_score/len(normal_scores)*100:.1f}%)")
                    
                    # ìƒì„¸ ë¶„í¬
                    for threshold in [-10, -20, -50, -100]:
                        count = np.sum(normal_scores < threshold)
                        if count > 0:
                            self.log(f"      â€¢ ì ìˆ˜ < {threshold}: {count:,}ê°œ "
                                    f"({count/len(normal_scores)*100:.2f}%)")
                
                # ì´ìƒìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆì§€ë§Œ ì ìˆ˜ê°€ ë†’ì€ ë°ì´í„°
                if len(anomaly_scores) > 0:
                    anomaly_but_high_score = np.sum(anomaly_scores > 0)
                    if anomaly_but_high_score > 0:
                        self.log(f"    - ì´ìƒ ë¶„ë¥˜ì§€ë§Œ ì ìˆ˜ > 0: {anomaly_but_high_score:,}ê°œ "
                                f"({anomaly_but_high_score/len(anomaly_scores)*100:.1f}%)")
                
                # ê²½ê³„ ê·¼ì²˜ ë°ì´í„° ë¶„ì„
                boundary_range = 10  # ê²°ì • ê²½ê³„ Â±10
                near_boundary = np.sum(np.abs(scores - decision_boundary) < boundary_range)
                self.log(f"    - ê²°ì • ê²½ê³„({decision_boundary:.2f}) Â±{boundary_range} ë²”ìœ„: "
                        f"{near_boundary:,}ê°œ ({near_boundary/len(scores)*100:.1f}%)")
                
                # 2ì°¨ ë¡œì§ ê²½ê³„ê°’ ì¶”ì²œ
                self.log("\n  ğŸ’¡ 2ì°¨ ë¡œì§ ê²½ê³„ê°’ ì¶”ì²œ:")
                
                # ë°©ë²• 1: ì •ìƒ ë°ì´í„°ì˜ í•˜ìœ„ í¼ì„¼íƒ€ì¼
                normal_lower_bound = np.percentile(normal_scores, 1)  # ì •ìƒì˜ í•˜ìœ„ 1%
                self.log(f"    - ì •ìƒ ë°ì´í„° í•˜ìœ„ 1%: {normal_lower_bound:.2f}")
                
                # ë°©ë²• 2: ì „ì²´ ë°ì´í„°ì˜ íŠ¹ì • í¼ì„¼íƒ€ì¼
                overall_p3 = np.percentile(scores, 3)
                self.log(f"    - ì „ì²´ ë°ì´í„° í•˜ìœ„ 3%: {overall_p3:.2f}")
                
                # ë°©ë²• 3: í‰ê·  - n*í‘œì¤€í¸ì°¨
                mean_minus_2std = np.mean(scores) - 2 * np.std(scores)
                mean_minus_3std = np.mean(scores) - 3 * np.std(scores)
                self.log(f"    - í‰ê·  - 2Ïƒ: {mean_minus_2std:.2f}")
                self.log(f"    - í‰ê·  - 3Ïƒ: {mean_minus_3std:.2f}")
                
                # ë°©ë²• 4: ì´ìƒ ë°ì´í„°ì˜ ìƒìœ„ ê²½ê³„
                if len(anomaly_scores) > 0:
                    anomaly_upper = np.percentile(anomaly_scores, 90)  # ì´ìƒì˜ ìƒìœ„ 10%
                    self.log(f"    - ì´ìƒ ë°ì´í„° ìƒìœ„ 10%: {anomaly_upper:.2f}")
                
                # ëª¨ë¸ ì •ë³´
                model_info = {
                    'machine_id': machine_id,
                    'sensor': sensor,
                    'train_samples': len(X_train),
                    'training_periods': self.training_periods,
                    'features': self.sensor_config[sensor]['features'],
                    'best_params': self.study.best_params,
                    'decision_boundary': float(decision_boundary),
                    'boundary_method': 'IQR',
                    'boundary_stats': {
                        'q1': float(q1),
                        'q3': float(q3),
                        'iqr': float(iqr)
                    },
                    'anomaly_ratio': float(anomaly_ratio),
                    'score_statistics': {
                        'mean': float(np.mean(scores)),
                        'std': float(np.std(scores)),
                        'min': float(np.min(scores)),
                        'max': float(np.max(scores))
                    },
                    'normal_score_statistics': {
                        'count': int(len(normal_scores)),
                        'mean': float(np.mean(normal_scores)),
                        'std': float(np.std(normal_scores)),
                        'min': float(np.min(normal_scores)),
                        'max': float(np.max(normal_scores)),
                        'percentiles': {
                            'p1': float(np.percentile(normal_scores, 1)),
                            'p5': float(np.percentile(normal_scores, 5)),
                            'p10': float(np.percentile(normal_scores, 10))
                        }
                    },
                    'anomaly_score_statistics': {
                        'count': int(len(anomaly_scores)),
                        'mean': float(np.mean(anomaly_scores)) if len(anomaly_scores) > 0 else None,
                        'std': float(np.std(anomaly_scores)) if len(anomaly_scores) > 0 else None,
                        'min': float(np.min(anomaly_scores)) if len(anomaly_scores) > 0 else None,
                        'max': float(np.max(anomaly_scores)) if len(anomaly_scores) > 0 else None
                    },
                    'score_percentiles': percentile_values,
                    'score_distribution': score_distribution,
                    'secondary_thresholds': {
                        'normal_p1': float(normal_lower_bound),
                        'overall_p3': float(overall_p3),
                        'mean_minus_2std': float(mean_minus_2std),
                        'mean_minus_3std': float(mean_minus_3std),
                        'anomaly_p90': float(anomaly_upper) if len(anomaly_scores) > 0 else None
                    },
                    'evaluation_skipped': False,
                    'trained_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
            
            # ëª¨ë¸ ì €ì¥
            self.log("\nëª¨ë¸ ì €ì¥ ì¤‘...")
            timestamp = datetime.now().strftime('%y%m%d_%H%M%S')
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            model_dir = f"./models/{machine_id}/{sensor}/current_model"
            scale_dir = f"./models/{machine_id}/{sensor}/current_scale"
            os.makedirs(model_dir, exist_ok=True)
            os.makedirs(scale_dir, exist_ok=True)
            
            # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
            for d in [model_dir, scale_dir]:
                for f in os.listdir(d):
                    os.remove(os.path.join(d, f))
            
            # ìƒˆ íŒŒì¼ ì €ì¥
            model_path = os.path.join(model_dir, f"{timestamp}_model.pkl")
            scaler_path = os.path.join(scale_dir, f"{timestamp}_scaler.pkl")
            info_path = os.path.join(model_dir, f"{timestamp}_model_info.json")
            
            joblib.dump(model, model_path)
            scaler.save(scaler_path)
            
            # ğŸ” ë””ë²„ê¹…: ì €ì¥ëœ íŒŒì¼ ê²€ì¦
            self.log(f"\nğŸ” [ë””ë²„ê¹…] ì €ì¥ëœ íŒŒì¼ ê²€ì¦:")
            
            # ëª¨ë¸ ì¬ë¡œë“œ í…ŒìŠ¤íŠ¸
            test_model = joblib.load(model_path)
            self.log(f"  - ëª¨ë¸ ì¬ë¡œë“œ ì„±ê³µ: {type(test_model).__name__}")
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ì¬ë¡œë“œ í…ŒìŠ¤íŠ¸
            test_scaler = CustomRobustScaler()
            test_scaler.load(scaler_path)
            self.log(f"  - ìŠ¤ì¼€ì¼ëŸ¬ ì¬ë¡œë“œ ì„±ê³µ: {len(test_scaler.params)}ê°œ íŠ¹ì§•")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ê²€ì¦
            test_data = X_train[:10]  # ì²˜ìŒ 10ê°œ ìƒ˜í”Œ
            test_scaled = test_scaler.transform(test_data)
            test_scores = test_model.decision_function(test_scaled)
            self.log(f"  - í…ŒìŠ¤íŠ¸ ë³€í™˜: ì›ë³¸ [{test_data.min():.2f}, {test_data.max():.2f}] â†’ "
                    f"ìŠ¤ì¼€ì¼ [{test_scaled.min():.2f}, {test_scaled.max():.2f}]")
            self.log(f"  - í…ŒìŠ¤íŠ¸ ìŠ¤ì½”ì–´: [{test_scores.min():.2f}, {test_scores.max():.2f}]")
            self.log(f"  - í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡: {test_model.predict(test_scaled)}")
            
            with open(info_path, 'w') as f:
                json.dump(model_info, f, indent=2)
            
            self.log(f"\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ:")
            self.log(f"  - ëª¨ë¸: {model_path}")
            self.log(f"  - ìŠ¤ì¼€ì¼ëŸ¬: {scaler_path}")
            self.log(f"  - ì •ë³´: {info_path}")
            
            # í˜„ì¬ ëª¨ë¸ ì •ë³´ ì €ì¥
            self.current_model_info = model_info
            
            # ì „ì²´ ì†Œìš” ì‹œê°„
            total_time = (datetime.now() - total_start_time).total_seconds()
            self.log(f"\nì „ì²´ í•™ìŠµ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ ({total_time/60:.1f}ë¶„)")
            
            self.progress_var.set("í•™ìŠµ ì™„ë£Œ!")
            messagebox.showinfo("ì™„ë£Œ", f"ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\në¨¸ì‹ : {machine_id}\nì„¼ì„œ: {sensor}")
            
        except Exception as e:
            self.log(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
            self.progress_var.set("í•™ìŠµ ì‹¤íŒ¨")
            messagebox.showerror("ì˜¤ë¥˜", f"í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            self.train_button.config(state='normal')
    
    def start_training(self):
        if not self.training_periods:
            messagebox.showerror("ì˜¤ë¥˜", "í•™ìŠµ ê¸°ê°„ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
            return
        
        if not self.conn:
            messagebox.showerror("ì˜¤ë¥˜", "DB ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        self.train_button.config(state='disabled')
        thread = threading.Thread(target=self.train_model)
        thread.daemon = True
        thread.start()
    
    def test_model(self):
        """ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        try:
            # ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ í™•ì¸
            model_path = self.model_file_var.get()
            scaler_path = self.scaler_file_var.get()
            
            if not model_path or not scaler_path:
                messagebox.showerror("ì˜¤ë¥˜", "ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
                return
            
            if not os.path.exists(model_path) or not os.path.exists(scaler_path):
                messagebox.showerror("ì˜¤ë¥˜", "ì„ íƒí•œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return
            
            # ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            self.log("ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì¤‘...")
            model = joblib.load(model_path)
            
            scaler = CustomRobustScaler()
            scaler.load(scaler_path)
            
            # ëª¨ë¸ ì •ë³´ ë¡œë“œ
            info_path = model_path.replace('_model.pkl', '_model_info.json')
            if os.path.exists(info_path):
                with open(info_path, 'r') as f:
                    model_info = json.load(f)
            else:
                # ê¸°ë³¸ ì •ë³´ ì‚¬ìš©
                model_info = {
                    'decision_boundary': -5.0,
                    'sensor': self.test_sensor_var.get()
                }
            
            # í…ŒìŠ¤íŠ¸ ì„¤ì •
            machine_id = self.test_machine_var.get()
            sensor = self.test_sensor_var.get()
            
            # ë‚ ì§œ ë²”ìœ„ í™•ì¸
            start_date = self.test_start_date.get_date()
            end_date = self.test_end_date.get_date()
            
            # ìµœëŒ€ 30ì¼ ì œí•œ
            if (end_date - start_date).days > 30:
                messagebox.showerror("ì˜¤ë¥˜", "í…ŒìŠ¤íŠ¸ ê¸°ê°„ì€ ìµœëŒ€ 30ì¼ê¹Œì§€ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                return
            
            if start_date >= end_date:
                messagebox.showerror("ì˜¤ë¥˜", "ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤.")
                return
            
            # ì„¼ì„œë³„ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ì„ íƒ
            table_name = f"normal_{sensor}_data"
            
            # ê²°ê³¼ í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
            self.result_text.delete(1.0, tk.END)
            self.result_text.insert(tk.END, f"ëª¨ë¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼\n")
            self.result_text.insert(tk.END, f"{'='*60}\n")
            self.result_text.insert(tk.END, f"ë¨¸ì‹ : {machine_id}, ì„¼ì„œ: {sensor}\n")
            self.result_text.insert(tk.END, f"í…Œì´ë¸”: {table_name}\n")
            self.result_text.insert(tk.END, f"ê¸°ê°„: {start_date} ~ {end_date}\n")
            self.result_text.insert(tk.END, f"ìƒ˜í”Œë§: {self.sampling_info_var.get()}\n")
            self.result_text.insert(tk.END, f"ê²°ì • ê²½ê³„: {model_info.get('decision_boundary', 'N/A')}\n\n")
            
            # ì „ì²´ ê¸°ê°„ ë°ì´í„° ìˆ˜ì§‘
            self.log(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ ì¤‘: {start_date} ~ {end_date}")
            
            # ë‚ ì§œë³„ë¡œ ë°ì´í„° ìˆ˜ì§‘
            all_timestamps = []
            all_scores = []
            
            current_date = start_date
            while current_date <= end_date:
                date_str = current_date.strftime("%Y-%m-%d")
                self.result_text.insert(tk.END, f"\n[{date_str}] ì²˜ë¦¬ ì¤‘...")
                self.root.update_idletasks()
                
                # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ì¶œ (ë¼ì¦ˆë² ë¦¬íŒŒì´ì™€ ë™ì¼í•œ ë°©ì‹)
                test_data = self.get_training_data(
                    machine_id, sensor,
                    f"{date_str} 00:00:00",
                    f"{date_str} 23:59:59"
                )
                
                if test_data is None or len(test_data) == 0:
                    self.result_text.insert(tk.END, " ë°ì´í„° ì—†ìŒ\n")
                    current_date += timedelta(days=1)
                    continue
                
                # ì˜ˆì¸¡
                # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìŠ¤ì¼€ì¼ë§
                X_test_scaled = scaler.transform(test_data)
                
                self.log(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {X_test_scaled.shape}")
                
                # ğŸ” ë””ë²„ê¹…: í…ŒìŠ¤íŠ¸ ë°ì´í„° ìŠ¤ì¼€ì¼ í™•ì¸
                self.log(f"\nğŸ” [ë””ë²„ê¹…] í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„:")
                self.log(f"  - ì›ë³¸ ë°ì´í„° ë²”ìœ„: [{test_data.min():.2f}, {test_data.max():.2f}]")
                self.log(f"  - ìŠ¤ì¼€ì¼ í›„ ë²”ìœ„: [{X_test_scaled.min():.2f}, {X_test_scaled.max():.2f}]")
                self.log(f"  - ìŠ¤ì¼€ì¼ í›„ í‰ê· : {X_test_scaled.mean():.4f}")
                self.log(f"  - ìŠ¤ì¼€ì¼ í›„ í‘œì¤€í¸ì°¨: {X_test_scaled.std():.4f}")
                
                # ê° íŠ¹ì§•ë³„ ë¶„í¬
                for i, feature_name in enumerate(self.sensor_config[sensor]['features']):
                    self.log(f"  [{feature_name}]")
                    self.log(f"    ì›ë³¸: mean={test_data[:, i].mean():.2f}, std={test_data[:, i].std():.2f}")
                    self.log(f"    ìŠ¤ì¼€ì¼: mean={X_test_scaled[:, i].mean():.2f}, std={X_test_scaled[:, i].std():.2f}")
                
                # ëª¨ë¸ ì •ë³´
                self.log(f"\nğŸ” [ë””ë²„ê¹…] ëª¨ë¸ ì •ë³´:")
                self.log(f"  - ëª¨ë¸ íƒ€ì…: {type(model).__name__}")
                self.log(f"  - nu: {model.nu}")
                self.log(f"  - gamma: {model.gamma}")
                self.log(f"  - Support vectors: {model.support_vectors_.shape[0]}ê°œ")
                
                # ì˜ˆì¸¡
                self.log("ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
                predictions = model.predict(X_test_scaled)
                scores = model.decision_function(X_test_scaled)
                
                # 5ì´ˆ ìœˆë„ìš°ë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
                window_sec = self.sensor_config[sensor]['window_sec']
                timestamps = [datetime.strptime(f"{date_str} 00:00:00", "%Y-%m-%d %H:%M:%S") + 
                            timedelta(seconds=i*window_sec) for i in range(len(scores))]
                
                all_timestamps.extend(timestamps)
                all_scores.extend(scores)
                
                # ê²°ê³¼ ë¶„ì„
                anomaly_count = np.sum(predictions == -1)
                anomaly_ratio = anomaly_count / len(predictions) * 100
                
                self.result_text.insert(tk.END, f" ì™„ë£Œ (ì´ìƒ: {anomaly_count}/{len(predictions)}, {anomaly_ratio:.1f}%)\n")
                
                current_date += timedelta(days=1)
            
            # í”Œë¡¯ ê·¸ë¦¬ê¸°
            self.plot_test_results(all_timestamps, all_scores, sensor, model_info)
            
            # ì „ì²´ ê²°ê³¼ ìš”ì•½
            if all_scores:
                all_scores = np.array(all_scores)
                total_anomalies = np.sum(all_scores < model_info.get('decision_boundary', 0))
                total_ratio = total_anomalies / len(all_scores) * 100
                
                self.result_text.insert(tk.END, f"\n{'='*60}\n")
                self.result_text.insert(tk.END, f"ì „ì²´ ê²°ê³¼ ìš”ì•½\n")
                self.result_text.insert(tk.END, f"ì´ ìœˆë„ìš° ìˆ˜: {len(all_scores):,}\n")
                self.result_text.insert(tk.END, f"ì´ìƒ íƒì§€: {total_anomalies:,}ê°œ ({total_ratio:.2f}%)\n")
                self.result_text.insert(tk.END, f"ì ìˆ˜ ë²”ìœ„: [{np.min(all_scores):.2f}, {np.max(all_scores):.2f}]\n")
                self.result_text.insert(tk.END, f"í‰ê·  ì ìˆ˜: {np.mean(all_scores):.2f} Â± {np.std(all_scores):.2f}\n")
            
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.log(f"í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
    
    def plot_test_results(self, timestamps, scores, sensor, model_info):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì‹œê³„ì—´ í”Œë¡¯ìœ¼ë¡œ í‘œì‹œ"""
        try:
            # ê¸°ì¡´ í”Œë¡¯ í´ë¦¬ì–´
            self.ax1.clear()
            self.ax2.clear()
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            scores = np.array(scores)
            decision_boundary = model_info.get('decision_boundary', 0)
            
            # 1. Score í”Œë¡¯
            self.ax1.plot(timestamps, scores, 'b-', linewidth=0.5, alpha=0.7, label='Score')
            self.ax1.axhline(y=decision_boundary, color='r', linestyle='--', linewidth=2, 
                            label=f'Decision Boundary ({decision_boundary:.2f})')
            self.ax1.axhline(y=0, color='k', linestyle='-', linewidth=1, alpha=0.3)
            
            # ì´ìƒ êµ¬ê°„ í‘œì‹œ
            anomaly_mask = scores < decision_boundary
            if np.any(anomaly_mask):
                self.ax1.scatter(np.array(timestamps)[anomaly_mask],
                               scores[anomaly_mask],
                               color='red', s=10, alpha=0.5, label='Anomaly')
            
            self.ax1.set_ylabel(f'{sensor.upper()} Score', fontsize=12)
            self.ax1.set_title(f'{self.test_machine_var.get()} - {sensor.upper()} Anomaly Detection Results', fontsize=14)
            self.ax1.legend(loc='upper right')
            self.ax1.grid(True, alpha=0.3)
            
            # 2. ì´ìƒ ë¹ˆë„ íˆìŠ¤í† ê·¸ë¨ (ì‹œê°„ë³„)
            # 1ì‹œê°„ ë‹¨ìœ„ë¡œ ì´ìƒ ê°œìˆ˜ ì§‘ê³„
            hour_bins = pd.date_range(start=min(timestamps), end=max(timestamps), freq='H')
            hour_counts = []
            
            for i in range(len(hour_bins)-1):
                mask = (np.array(timestamps) >= hour_bins[i]) & (np.array(timestamps) < hour_bins[i+1])
                hour_anomalies = np.sum(scores[mask] < decision_boundary) if np.any(mask) else 0
                hour_counts.append(hour_anomalies)
            
            self.ax2.bar(hour_bins[:-1], hour_counts, width=1/24, alpha=0.7, color='red')
            self.ax2.set_ylabel('Anomalies per Hour', fontsize=12)
            self.ax2.set_xlabel('Time', fontsize=12)
            self.ax2.grid(True, alpha=0.3)
            
            # Xì¶• í¬ë§·íŒ…
            self.ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %Hh'))
            self.ax2.xaxis.set_major_locator(mdates.HourLocator(interval=6))
            plt.setp(self.ax2.xaxis.get_majorticklabels(), rotation=45)
            
            self.fig.tight_layout()
            self.canvas.draw()
            
        except Exception as e:
            self.log(f"í”Œë¡¯ ìƒì„± ì˜¤ë¥˜: {e}")
    
    def analyze_hourly_anomalies(self, machine_id, sensor, test_date, predictions, scores):
        """ì‹œê°„ëŒ€ë³„ ì´ìƒ íƒì§€ ë¶„ì„"""
        try:
            # í•´ë‹¹ ë‚ ì§œì˜ ì›ë³¸ ë°ì´í„° ë‹¤ì‹œ ë¡œë“œ (ì‹œê°„ ì •ë³´ í¬í•¨)
            if sensor == 'acc':
                query = """
                SELECT time, x, y, z
                FROM normal_acc_data
                WHERE machine_id = %s
                AND DATE(time) = %s
                ORDER BY time
                """
            else:
                query = """
                SELECT time, mic_value
                FROM normal_mic_data
                WHERE machine_id = %s
                AND DATE(time) = %s
                ORDER BY time
                """
            
            df = pd.read_sql(query, self.conn, params=(machine_id, test_date))
            df['time'] = pd.to_datetime(df['time'])
            
            # 5ì´ˆ ìœˆë„ìš°ë¡œ ê·¸ë£¹í™” (í•™ìŠµê³¼ ë™ì¼)
            window_sec = self.sensor_config[sensor]['window_sec']
            df['window'] = df['time'].dt.floor(f'{window_sec}S')
            df['hour'] = df['time'].dt.hour
            
            # ê° ìœˆë„ìš°ì˜ ì‹œê°„ëŒ€ í• ë‹¹
            window_hours = df.groupby('window')['hour'].first().values
            
            # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ëŠ” ìœˆë„ìš°ë§Œ í•„í„°ë§
            window_samples = window_sec * 10  # DBëŠ” 10Hz
            valid_windows = df.groupby('window').size() >= window_samples * 0.8
            valid_indices = valid_windows[valid_windows].index
            
            # ìœ íš¨í•œ ìœˆë„ìš°ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
            window_idx = 0
            hourly_predictions = {h: [] for h in range(24)}
            hourly_scores = {h: [] for h in range(24)}
            
            for window in valid_indices:
                if window_idx < len(predictions):
                    hour = df[df['window'] == window]['hour'].iloc[0]
                    hourly_predictions[hour].append(predictions[window_idx])
                    hourly_scores[hour].append(scores[window_idx])
                    window_idx += 1
            
            # ì‹œê°„ëŒ€ë³„ í†µê³„ ê³„ì‚°
            hourly_stats = {}
            for hour in range(24):
                if hourly_predictions[hour]:
                    anomaly_count = sum(p == -1 for p in hourly_predictions[hour])
                    total_count = len(hourly_predictions[hour])
                    
                    hourly_stats[hour] = {
                        'anomaly_count': anomaly_count,
                        'total_count': total_count,
                        'anomaly_ratio': anomaly_count / total_count * 100,
                        'mean_score': np.mean(hourly_scores[hour]),
                        'min_score': np.min(hourly_scores[hour]),
                        'max_score': np.max(hourly_scores[hour])
                    }
            
            return hourly_stats
            
        except Exception as e:
            self.log(f"ì‹œê°„ëŒ€ë³„ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return None
    
    def start_testing(self):
        start_date = self.test_start_date.get_date()
        end_date = self.test_end_date.get_date()
        
        if start_date >= end_date:
            messagebox.showerror("ì˜¤ë¥˜", "ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤.")
            return
        
        thread = threading.Thread(target=self.test_model)
        thread.daemon = True
        thread.start()

def main():
    root = tk.Tk()
    app = OCSVMTrainerGUI(root)
    root.mainloop()

if __name__ == "__main__":
    main()