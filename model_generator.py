#!/usr/bin/env python
# train_ocsvm_gui.py

import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext
import psycopg2
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import joblib
import json
import os
import threading
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import KFold
import optuna
from optuna import create_study
from tkcalendar import DateEntry
from tkcalendar import Calendar
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
import matplotlib.dates as mdates

# ë¼ì¦ˆë² ë¦¬íŒŒì´ì˜ CustomRobustScaler êµ¬í˜„
class CustomRobustScaler:
    def __init__(self):
        self.params = {}
    
    def fit(self, X):
        self.params = {}
        for i in range(X.shape[1]):
            col = X[:, i]
            q1, median, q3 = np.percentile(col, [25, 50, 75])
            iqr = q3 - q1
            self.params[i] = {'median': median, 'iqr': iqr}
    
    def transform(self, X):
        X_scaled = np.zeros_like(X, dtype=np.float64)
        for i in range(X.shape[1]):
            median = self.params[i]['median']
            iqr = self.params[i]['iqr']
            X_scaled[:, i] = (X[:, i] - median) / iqr
        return X_scaled
    
    def fit_transform(self, X):
        self.fit(X)
        return self.transform(X)
    
    def save(self, filepath):
        joblib.dump({"params": self.params}, filepath)
    
    def load(self, filepath):
        loaded = joblib.load(filepath)
        self.params = loaded["params"]
        
class OCSVMTrainerGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("OCSVM ëª¨ë¸ í•™ìŠµ ë„êµ¬")
        self.root.geometry("1400x900")
        
        # ë¡œê·¸ í…ìŠ¤íŠ¸ ìœ„ì ¯ì„ ë¨¼ì € ì´ˆê¸°í™”
        self.log_text = None
        
        # DB ì—°ê²°
        self.conn = None
        
        # ì„¤ì •ê°’
        self.sensor_config = {
            "mic": {
                "sampling_rate": 8000,
                "window_sec": 5,
                "features": ["mav", "rms", "peak", "amp_iqr"],
                # Isolation Forest íŒŒë¼ë¯¸í„°
                "n_estimators_range": [100, 300],
                "contamination_range": [0.001, 0.05],
                "max_samples_range": ['auto', 0.5, 1.0],
                "max_features_range": [0.8, 1.0]
            },
            "acc": {
                "sampling_rate": 1666,
                "window_sec": 5,
                "features": ["x_peak", "x_crest_factor", "y_peak", "y_crest_factor", "z_peak", "z_crest_factor"],
                "n_estimators_range": [100, 300],
                "contamination_range": [0.001, 0.05],
                "max_samples_range": ['auto', 0.5, 1.0],
                "max_features_range": [0.8, 1.0]
            }
        }
        
        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì €ì¥
        self.training_periods = []
        self.test_periods = []
        
        # GUI ìƒì„±
        self.create_widgets()
        
        # DB ì—°ê²° (GUI ìƒì„± í›„)
        self.connect_db()
        
        # ë‚ ì§œ ë²”ìœ„ ë¡œë“œ
        self.load_date_range()
    
    def log(self, message):
        """ë¡œê·¸ ë©”ì‹œì§€ ì¶œë ¥"""
        if self.log_text:
            timestamp = datetime.now().strftime("%H:%M:%S")
            self.log_text.insert(tk.END, f"[{timestamp}] {message}\n")
            self.log_text.see(tk.END)
            self.root.update()
        else:
            print(f"[LOG] {message}")  # ë¡œê·¸ ìœ„ì ¯ì´ ì—†ì„ ë•Œ ì½˜ì†” ì¶œë ¥
        
        # ì½˜ì†”ì—ë„ í•­ìƒ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        print(f"[{timestamp}] {message}")
    
    def connect_db(self):
        """DB ì—°ê²° - íŠ¸ëœì­ì…˜ ì—ëŸ¬ ë°©ì§€"""
        try:
            self.conn = psycopg2.connect(
                host='localhost',
                port='5432',
                database='pdm_db',
                user='pdm_user',
                password='pdm_password'
            )
            # autocommit ì„¤ì •ìœ¼ë¡œ íŠ¸ëœì­ì…˜ ì—ëŸ¬ ë°©ì§€
            self.conn.autocommit = True
            self.log("âœ… DB ì—°ê²° ì„±ê³µ")
            
            # í…Œì´ë¸” ë° ë°ì´í„° í™•ì¸
            cur = self.conn.cursor()
            
            # í…Œì´ë¸” í™•ì¸
            cur.execute("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name IN ('normal_acc_data', 'normal_mic_data')
            """)
            tables = [row[0] for row in cur.fetchall()]
            self.log(f"í™•ì¸ëœ í…Œì´ë¸”: {tables}")
            
            # ê° í…Œì´ë¸”ì˜ ë°ì´í„° ìˆ˜ í™•ì¸
            for table in tables:
                cur.execute(f"SELECT COUNT(*) FROM {table}")
                count = cur.fetchone()[0]
                self.log(f"{table}: {count:,}ê°œ ë ˆì½”ë“œ")
            
            # ë¨¸ì‹ ë³„ ë°ì´í„° í™•ì¸
            for table in tables:
                cur.execute(f"""
                    SELECT machine_id, COUNT(*) as cnt, 
                           MIN(time) as min_time, MAX(time) as max_time
                    FROM {table}
                    GROUP BY machine_id
                """)
                for row in cur.fetchall():
                    self.log(f"{table} - {row[0]}: {row[1]:,}ê°œ, {row[2]} ~ {row[3]}")
            
        except Exception as e:
            self.log(f"DB ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            messagebox.showerror("DB ì—°ê²° ì‹¤íŒ¨", str(e))
    
    def create_widgets(self):
        # ë©”ì¸ ë…¸íŠ¸ë¶
        notebook = ttk.Notebook(self.root)
        notebook.pack(fill='both', expand=True, padx=5, pady=5)
        
        # í•™ìŠµ íƒ­
        train_tab = ttk.Frame(notebook)
        notebook.add(train_tab, text="ëª¨ë¸ í•™ìŠµ")
        self.create_train_tab(train_tab)
        
        # í…ŒìŠ¤íŠ¸ íƒ­
        test_tab = ttk.Frame(notebook)
        notebook.add(test_tab, text="ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        self.create_test_tab(test_tab)
        
        # ë¡œê·¸ íƒ­
        log_tab = ttk.Frame(notebook)
        notebook.add(log_tab, text="ë¡œê·¸")
        self.create_log_tab(log_tab)
    
    def create_train_tab(self, parent):
        # ì„¤ì • í”„ë ˆì„
        config_frame = ttk.LabelFrame(parent, text="í•™ìŠµ ì„¤ì •", padding="10")
        config_frame.pack(fill='x', padx=5, pady=5)
        
        # ë¨¸ì‹ /ì„¼ì„œ ì„ íƒ
        row = 0
        ttk.Label(config_frame, text="ë¨¸ì‹ :").grid(row=row, column=0, sticky=tk.W, padx=5)
        self.machine_var = tk.StringVar(value="CURINGOVEN_M1")
        machine_combo = ttk.Combobox(config_frame, textvariable=self.machine_var, 
                                    values=["CURINGOVEN_M1", "HOTCHAMBER_M2"], width=20)
        machine_combo.grid(row=row, column=1, padx=5)
        
        ttk.Label(config_frame, text="ì„¼ì„œ:").grid(row=row, column=2, sticky=tk.W, padx=5)
        self.sensor_var = tk.StringVar(value="acc")
        sensor_combo = ttk.Combobox(config_frame, textvariable=self.sensor_var,
                                   values=["acc", "mic"], width=10)
        sensor_combo.grid(row=row, column=3, padx=5)
        
        # ìµœì í™” ì„¤ì •
        row += 1
        ttk.Label(config_frame, text="Optuna Trials:").grid(row=row, column=0, sticky=tk.W, padx=5)
        self.trials_var = tk.IntVar(value=200)  # ë” ë§ì€ trialsë¡œ ìµœì  gamma íƒìƒ‰
        trials_spinbox = ttk.Spinbox(config_frame, from_=10, to=500, increment=10,
                                    textvariable=self.trials_var, width=10)
        trials_spinbox.grid(row=row, column=1, padx=5)
        
        # ì„±ëŠ¥ í‰ê°€ ìŠ¤í‚µ ì˜µì…˜
        row += 1
        self.skip_eval_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(config_frame, text="ì„±ëŠ¥ í‰ê°€ ìŠ¤í‚µ (ë¹ ë¥¸ í•™ìŠµ)", 
                       variable=self.skip_eval_var).grid(row=row, column=0, columnspan=2, sticky=tk.W, padx=5)
        
        # í•™ìŠµ ê¸°ê°„ í”„ë ˆì„
        period_frame = ttk.LabelFrame(parent, text="í•™ìŠµ ê¸°ê°„ ì„¤ì •", padding="10")
        period_frame.pack(fill='both', expand=True, padx=5, pady=5)
        
        # ê¸°ê°„ ì…ë ¥
        input_frame = ttk.Frame(period_frame)
        input_frame.pack(fill='x')
        
        ttk.Label(input_frame, text="ì‹œì‘ì¼:").pack(side='left', padx=5)
        self.train_start_date = DateEntry(input_frame, width=12, background='darkblue',
                                         foreground='white', borderwidth=2,
                                         date_pattern='yyyy-mm-dd')
        self.train_start_date.pack(side='left', padx=5)
        
        ttk.Label(input_frame, text="ì¢…ë£Œì¼:").pack(side='left', padx=5)
        self.train_end_date = DateEntry(input_frame, width=12, background='darkblue',
                                       foreground='white', borderwidth=2,
                                       date_pattern='yyyy-mm-dd')
        self.train_end_date.pack(side='left', padx=5)
        
        ttk.Button(input_frame, text="ê¸°ê°„ ì¶”ê°€", 
                  command=self.add_training_period).pack(side='left', padx=20)
        
        # ì‹œê°„ í•„í„° í”„ë ˆì„ ì¶”ê°€
        time_filter_frame = ttk.Frame(period_frame)
        time_filter_frame.pack(fill='x', pady=10)
        
        # ì‹œê°„ í•„í„° ì²´í¬ë°•ìŠ¤
        self.use_time_filter = tk.BooleanVar(value=False)
        self.time_filter_check = ttk.Checkbutton(time_filter_frame, text="ì‹œê°„ í•„í„° ì‚¬ìš©", 
                                                 variable=self.use_time_filter,
                                                 command=self.toggle_time_filter)
        self.time_filter_check.pack(side='left', padx=5)
        
        # ì‹œê°„ ì„ íƒ ìœ„ì ¯ë“¤
        self.time_widgets_frame = ttk.Frame(time_filter_frame)
        self.time_widgets_frame.pack(side='left', padx=20)
        
        ttk.Label(self.time_widgets_frame, text="ì‹œì‘ ì‹œê°„:").pack(side='left', padx=5)
        self.start_hour = ttk.Spinbox(self.time_widgets_frame, from_=0, to=23, width=3, format="%02.0f")
        self.start_hour.set("00")
        self.start_hour.pack(side='left')
        ttk.Label(self.time_widgets_frame, text=":").pack(side='left')
        self.start_minute = ttk.Spinbox(self.time_widgets_frame, from_=0, to=59, width=3, format="%02.0f")
        self.start_minute.set("00")
        self.start_minute.pack(side='left')
        
        ttk.Label(self.time_widgets_frame, text="  ì¢…ë£Œ ì‹œê°„:").pack(side='left', padx=(20,5))
        self.end_hour = ttk.Spinbox(self.time_widgets_frame, from_=0, to=23, width=3, format="%02.0f")
        self.end_hour.set("23")
        self.end_hour.pack(side='left')
        ttk.Label(self.time_widgets_frame, text=":").pack(side='left')
        self.end_minute = ttk.Spinbox(self.time_widgets_frame, from_=0, to=59, width=3, format="%02.0f")
        self.end_minute.set("59")
        self.end_minute.pack(side='left')
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ì‹œê°„ í•„í„° ë¹„í™œì„±í™”
        self.toggle_time_filter()
        
        # ê¸°ê°„ ëª©ë¡
        list_frame = ttk.Frame(period_frame)
        list_frame.pack(fill='both', expand=True, pady=10)
        
        # íŠ¸ë¦¬ë·°
        columns = ('ì‹œì‘ì¼', 'ì¢…ë£Œì¼', 'ì‹œê°„ëŒ€', 'ì¼ìˆ˜')
        self.train_tree = ttk.Treeview(list_frame, columns=columns, height=8)
        self.train_tree.heading('#0', text='No')
        self.train_tree.heading('ì‹œì‘ì¼', text='ì‹œì‘ì¼')
        self.train_tree.heading('ì¢…ë£Œì¼', text='ì¢…ë£Œì¼')
        self.train_tree.heading('ì‹œê°„ëŒ€', text='ì‹œê°„ëŒ€')
        self.train_tree.heading('ì¼ìˆ˜', text='ì¼ìˆ˜')
        
        self.train_tree.column('#0', width=50)
        self.train_tree.column('ì‹œì‘ì¼', width=150)
        self.train_tree.column('ì¢…ë£Œì¼', width=150)
        self.train_tree.column('ì‹œê°„ëŒ€', width=100)
        self.train_tree.column('ì¼ìˆ˜', width=80)
        
        self.train_tree.pack(side='left', fill='both', expand=True)
        
        # ìŠ¤í¬ë¡¤ë°”
        scrollbar = ttk.Scrollbar(list_frame, orient='vertical', command=self.train_tree.yview)
        scrollbar.pack(side='right', fill='y')
        self.train_tree.configure(yscrollcommand=scrollbar.set)
        
        # ë²„íŠ¼ í”„ë ˆì„
        button_frame = ttk.Frame(period_frame)
        button_frame.pack(fill='x')
        
        ttk.Button(button_frame, text="ì„ íƒ ì‚­ì œ", 
                  command=self.remove_training_period).pack(side='left', padx=5)
        ttk.Button(button_frame, text="ëª¨ë‘ ì‚­ì œ", 
                  command=self.clear_training_periods).pack(side='left', padx=5)
        
        # í•™ìŠµ ë²„íŠ¼
        train_button_frame = ttk.Frame(parent)
        train_button_frame.pack(fill='x', padx=5, pady=10)
        
        self.train_button = ttk.Button(train_button_frame, text="ëª¨ë¸ í•™ìŠµ ì‹œì‘", 
                                      command=self.start_training)
        self.train_button.pack(side='left', padx=5)
        
        self.progress_var = tk.StringVar(value="ëŒ€ê¸° ì¤‘...")
        ttk.Label(train_button_frame, textvariable=self.progress_var).pack(side='left', padx=20)
    
    def create_test_tab(self, parent):
        # ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒ í”„ë ˆì„
        model_frame = ttk.LabelFrame(parent, text="ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì„ íƒ", padding="10")
        model_frame.pack(fill='x', padx=5, pady=5)
        
        # ì„¼ì„œ íƒ€ì… ì„ íƒ
        ttk.Label(model_frame, text="ì„¼ì„œ íƒ€ì…:").grid(row=0, column=0, sticky=tk.W, padx=5)
        self.test_sensor_var = tk.StringVar(value="acc")
        sensor_radio_frame = ttk.Frame(model_frame)
        sensor_radio_frame.grid(row=0, column=1, columnspan=2, sticky=tk.W, padx=5)
        
        ttk.Radiobutton(sensor_radio_frame, text="ê°€ì†ë„ ì„¼ì„œ", variable=self.test_sensor_var, 
                        value="acc", command=self.update_test_settings).pack(side='left', padx=5)
        ttk.Radiobutton(sensor_radio_frame, text="ë§ˆì´í¬ ì„¼ì„œ", variable=self.test_sensor_var, 
                        value="mic", command=self.update_test_settings).pack(side='left', padx=5)
        
        # ëª¨ë¸ íŒŒì¼ ì„ íƒ
        ttk.Label(model_frame, text="ëª¨ë¸ íŒŒì¼:").grid(row=1, column=0, sticky=tk.W, padx=5)
        self.model_file_var = tk.StringVar()
        ttk.Entry(model_frame, textvariable=self.model_file_var, width=50).grid(row=1, column=1, padx=5)
        ttk.Button(model_frame, text="ì°¾ì•„ë³´ê¸°", 
                  command=self.browse_model_file).grid(row=1, column=2, padx=5)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ ì„ íƒ
        ttk.Label(model_frame, text="ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼:").grid(row=2, column=0, sticky=tk.W, padx=5)
        self.scaler_file_var = tk.StringVar()
        ttk.Entry(model_frame, textvariable=self.scaler_file_var, width=50).grid(row=2, column=1, padx=5)
        ttk.Button(model_frame, text="ì°¾ì•„ë³´ê¸°", 
                  command=self.browse_scaler_file).grid(row=2, column=2, padx=5)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • í”„ë ˆì„
        db_frame = ttk.LabelFrame(parent, text="ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •", padding="10")
        db_frame.pack(fill='x', padx=5, pady=5)
        
        # ë¨¸ì‹  ì„ íƒ
        ttk.Label(db_frame, text="ë¨¸ì‹ :").grid(row=0, column=0, sticky=tk.W, padx=5)
        self.test_machine_var = tk.StringVar(value="CURINGOVEN_M1")
        ttk.Combobox(db_frame, textvariable=self.test_machine_var, 
                    values=["CURINGOVEN_M1", "HOTCHAMBER_M2"], 
                    width=20).grid(row=0, column=1, padx=5)
        
        # ìƒ˜í”Œë§ ì„¤ì • í‘œì‹œ
        ttk.Label(db_frame, text="ìƒ˜í”Œë§ ì„¤ì •:").grid(row=1, column=0, sticky=tk.W, padx=5)
        self.sampling_info_var = tk.StringVar(value="ê°€ì†ë„: 1666Hz, 5ì´ˆ ìœˆë„ìš°")
        ttk.Label(db_frame, textvariable=self.sampling_info_var).grid(row=1, column=1, sticky=tk.W, padx=5)
        
        # íŠ¹ì§• ì •ë³´ í‘œì‹œ
        ttk.Label(db_frame, text="ì¶”ì¶œ íŠ¹ì§•:").grid(row=2, column=0, sticky=tk.W, padx=5)
        self.features_info_var = tk.StringVar(value="x_peak, x_crest_factor, y_peak, y_crest_factor, z_peak, z_crest_factor")
        features_label = ttk.Label(db_frame, textvariable=self.features_info_var, wraplength=400)
        features_label.grid(row=2, column=1, sticky=tk.W, padx=5)
        
        # í…ŒìŠ¤íŠ¸ ê¸°ê°„ í”„ë ˆì„
        test_period_frame = ttk.LabelFrame(parent, text="í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì„¤ì •", padding="10")
        test_period_frame.pack(fill='both', expand=True, padx=5, pady=5)
        
        # ê¸°ê°„ ì…ë ¥
        test_input_frame = ttk.Frame(test_period_frame)
        test_input_frame.pack(fill='x')
        
        ttk.Label(test_input_frame, text="ì‹œì‘ì¼:").pack(side='left', padx=5)
        self.test_start_date = DateEntry(
            test_input_frame, 
            width=12, 
            background='darkblue',
            foreground='white', 
            borderwidth=2,
            date_pattern='yyyy-mm-dd',
            showweeknumbers=False,
            cursor="hand2"
        )
        self.test_start_date.pack(side='left', padx=5)
        
        ttk.Label(test_input_frame, text="ì¢…ë£Œì¼:").pack(side='left', padx=5)
        self.test_end_date = DateEntry(
            test_input_frame, 
            width=12, 
            background='darkblue',
            foreground='white', 
            borderwidth=2,
            date_pattern='yyyy-mm-dd',
            showweeknumbers=False,
            cursor="hand2"
        )
        self.test_end_date.pack(side='left', padx=5)
        
        ttk.Label(test_input_frame, text="(ìµœëŒ€ 5ì¼)").pack(side='left', padx=10)
        
        # í…ŒìŠ¤íŠ¸ ë²„íŠ¼
        ttk.Button(parent, text="í…ŒìŠ¤íŠ¸ ì‹œì‘", 
                  command=self.start_testing).pack(pady=10)
        
        # í”Œë¡¯ ì˜ì—­
        plot_frame = ttk.LabelFrame(parent, text="ì‹œê³„ì—´ í”Œë¡¯", padding="10")
        plot_frame.pack(fill='both', expand=True, padx=5, pady=5)
        
        # matplotlib Figure
        self.fig, (self.ax1, self.ax2) = plt.subplots(2, 1, figsize=(12, 8))
        self.canvas = FigureCanvasTkAgg(self.fig, master=plot_frame)
        self.canvas.get_tk_widget().pack(fill='both', expand=True)
        
        test_button_frame = ttk.Frame(test_period_frame)
        test_button_frame.pack(fill='x')
        
        # ê²°ê³¼ í‘œì‹œ
        result_frame = ttk.LabelFrame(parent, text="í…ŒìŠ¤íŠ¸ ê²°ê³¼", padding="10")
        result_frame.pack(fill='x', padx=5, pady=5)
        
        self.result_text = scrolledtext.ScrolledText(result_frame, height=10)
        self.result_text.pack(fill='both', expand=True)
    
    def create_log_tab(self, parent):
        # ë¡œê·¸ í…ìŠ¤íŠ¸ ìœ„ì ¯ ìƒì„±
        self.log_text = scrolledtext.ScrolledText(parent, height=30)
        self.log_text.pack(fill='both', expand=True, padx=5, pady=5)
    
    def update_test_settings(self):
        """ì„¼ì„œ íƒ€ì…ì— ë”°ë¼ ì„¤ì • ì •ë³´ ì—…ë°ì´íŠ¸"""
        sensor = self.test_sensor_var.get()
        if sensor == "acc":
            self.sampling_info_var.set("ê°€ì†ë„: 1666Hz, 5ì´ˆ ìœˆë„ìš°")
            self.features_info_var.set("x_peak, x_crest_factor, y_peak, y_crest_factor, z_peak, z_crest_factor")
        else:
            self.sampling_info_var.set("ë§ˆì´í¬: 8000Hz, 5ì´ˆ ìœˆë„ìš°")
            self.features_info_var.set("mav, rms, peak, amp_iqr")
    
    def browse_model_file(self):
        """ëª¨ë¸ íŒŒì¼ ì°¾ì•„ë³´ê¸°"""
        from tkinter import filedialog
        filename = filedialog.askopenfilename(
            title="ëª¨ë¸ íŒŒì¼ ì„ íƒ",
            initialdir="./models",
            filetypes=[("Pickle files", "*.pkl"), ("All files", "*.*")]
        )
        if filename:
            self.model_file_var.set(filename)
            
            # ëª¨ë¸ ì •ë³´ íŒŒì¼ë„ ê°™ì´ ì°¾ê¸°
            info_path = filename.replace('_model.pkl', '_model_info.json')
            if os.path.exists(info_path):
                with open(info_path, 'r') as f:
                    model_info = json.load(f)
                    # ì„¼ì„œ íƒ€ì… ìë™ ì„¤ì •
                    self.test_sensor_var.set(model_info.get('sensor', 'acc'))
                    self.update_test_settings()
    
    def browse_scaler_file(self):
        """ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ ì°¾ì•„ë³´ê¸°"""
        from tkinter import filedialog
        filename = filedialog.askopenfilename(
            title="ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ ì„ íƒ",
            initialdir="./models",
            filetypes=[("Pickle files", "*.pkl"), ("All files", "*.*")]
        )
        if filename:
            self.scaler_file_var.set(filename)
    
    def load_date_range(self):
        """DBì—ì„œ ë°ì´í„° ë‚ ì§œ ë²”ìœ„ í™•ì¸"""
        try:
            cur = self.conn.cursor()
            cur.execute("""
                SELECT MIN(DATE(time)), MAX(DATE(time))
                FROM normal_acc_data
            """)
            min_date, max_date = cur.fetchone()
            
            if min_date and max_date:
                self.train_start_date.set_date(min_date)
                self.train_end_date.set_date(max_date)
                self.test_start_date.set_date(max_date - timedelta(days=7))
                self.test_end_date.set_date(max_date)
                
                self.log(f"ë°ì´í„° ë²”ìœ„: {min_date} ~ {max_date}")
        except Exception as e:
            self.log(f"ë‚ ì§œ ë²”ìœ„ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def check_data_exists(self):
        """ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        try:
            cur = self.conn.cursor()
            
            # ê° í…Œì´ë¸”ì˜ ë°ì´í„° ìˆ˜ í™•ì¸
            for table in ['acc_data', 'mic_data']:
                cur.execute(f"SELECT COUNT(*) FROM {table}")
                count = cur.fetchone()[0]
                self.log(f"{table}: {count}ê°œ ë ˆì½”ë“œ")
                
            # ë¨¸ì‹ ë³„ ë°ì´í„° ìˆ˜ í™•ì¸
            for table in ['acc_data', 'mic_data']:
                cur.execute(f"""
                    SELECT machine_id, COUNT(*), MIN(time), MAX(time)
                    FROM {table}
                    GROUP BY machine_id
                """)
                for row in cur.fetchall():
                    self.log(f"{table} - {row[0]}: {row[1]}ê°œ, {row[2]} ~ {row[3]}")
                    
        except Exception as e:
            self.log(f"ë°ì´í„° í™•ì¸ ì‹¤íŒ¨: {e}")
    
    def stratified_time_sampling(self, X_scaled, period_info, target_size):
        """ì‹œê°„ëŒ€ë³„ ê· ë“± ìƒ˜í”Œë§"""
        sample_indices = []
        
        # ê° ê¸°ê°„ì„ ì‹œê°„ëŒ€ë³„ë¡œ ë‚˜ëˆ„ê¸°
        for info in period_info:
            period_data_count = info['count']
            period_sample_size = int(target_size * (period_data_count / len(X_scaled)))
            
            if period_sample_size > 0:
                # í•˜ë£¨ë¥¼ 4ê°œ ì‹œê°„ëŒ€ë¡œ ë‚˜ëˆ„ê¸° (6ì‹œê°„ ë‹¨ìœ„)
                # ë˜ëŠ” í”¼í¬/ì˜¤í”„í”¼í¬ ì‹œê°„ëŒ€ë¡œ ë‚˜ëˆ„ê¸°
                period_start = info['start_idx']
                period_end = info['end_idx']
                
                # ê· ë“± ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
                indices = np.linspace(period_start, period_end-1, 
                                    period_sample_size, dtype=int)
                sample_indices.extend(indices)
        
        return np.array(sample_indices)
    
    def toggle_time_filter(self):
        """ì‹œê°„ í•„í„° ì²´í¬ë°•ìŠ¤ ìƒíƒœì— ë”°ë¼ ì‹œê°„ ì„ íƒ ìœ„ì ¯ í™œì„±í™”/ë¹„í™œì„±í™”"""
        if self.use_time_filter.get():
            for widget in self.time_widgets_frame.winfo_children():
                widget.configure(state='normal')
        else:
            for widget in self.time_widgets_frame.winfo_children():
                if isinstance(widget, (ttk.Spinbox, ttk.Entry)):
                    widget.configure(state='disabled')
    
    def add_training_period(self):
        try:
            start_date = self.train_start_date.get_date()
            end_date = self.train_end_date.get_date()
            start = start_date.strftime("%Y-%m-%d")
            end = end_date.strftime("%Y-%m-%d")
            
            if start_date > end_date:  # >= ì—ì„œ > ë¡œ ë³€ê²½í•˜ì—¬ ê°™ì€ ë‚  í—ˆìš©
                messagebox.showerror("ì˜¤ë¥˜", "ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤.")
                return
            
            days = (end_date - start_date).days + 1
            
            # ì‹œê°„ í•„í„° ì •ë³´ ì¶”ê°€
            if self.use_time_filter.get():
                start_time = f"{self.start_hour.get()}:{self.start_minute.get()}"
                end_time = f"{self.end_hour.get()}:{self.end_minute.get()}"
                time_range = f"{start_time}~{end_time}"
            else:
                time_range = "ì „ì²´"
                start_time = "00:00"
                end_time = "23:59"
            
            # íŠ¸ë¦¬ì— ì¶”ê°€
            item_id = self.train_tree.insert('', 'end', 
                                           text=str(len(self.training_periods) + 1),
                                           values=(start, end, time_range, days))
            
            # ê¸°ê°„ ì •ë³´ì— ì‹œê°„ í•„í„° í¬í•¨
            period_info = {
                'start_date': start,
                'end_date': end,
                'start_time': start_time,
                'end_time': end_time,
                'use_time_filter': self.use_time_filter.get()
            }
            self.training_periods.append(period_info)
            self.log(f"í•™ìŠµ ê¸°ê°„ ì¶”ê°€: {start} ~ {end} ({time_range}) - {days}ì¼")
            
        except ValueError:
            messagebox.showerror("ì˜¤ë¥˜", "ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. (YYYY-MM-DD)")
    
    def remove_training_period(self):
        selected = self.train_tree.selection()
        if selected:
            for item in selected:
                idx = self.train_tree.index(item)
                del self.training_periods[idx]
                self.train_tree.delete(item)
            
            # ë²ˆí˜¸ ì¬ì •ë ¬
            for i, item in enumerate(self.train_tree.get_children()):
                self.train_tree.item(item, text=str(i + 1))
    
    def clear_training_periods(self):
        self.training_periods.clear()
        for item in self.train_tree.get_children():
            self.train_tree.delete(item)

    
    def extract_features_acc(self, x_data, y_data, z_data):
        """ACC íŠ¹ì§• ì¶”ì¶œ - ë‹¤ìš´ìƒ˜í”Œë§ëœ ë°ì´í„°ìš©"""
        try:
            # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ
            if len(x_data) < 10:
                raise ValueError(f"ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŒ: {len(x_data)}ê°œ")
            
            features = []
            
            # Xì¶•
            x_rms = np.sqrt(np.mean(x_data**2))
            x_peak = np.max(np.abs(x_data))
            x_crest = x_peak / x_rms if x_rms > 1e-10 else 0
            
            # Yì¶•
            y_rms = np.sqrt(np.mean(y_data**2))
            y_peak = np.max(np.abs(y_data))
            y_crest = y_peak / y_rms if y_rms > 1e-10 else 0
            
            # Zì¶•
            z_rms = np.sqrt(np.mean(z_data**2))
            z_peak = np.max(np.abs(z_data))
            z_crest = z_peak / z_rms if z_rms > 1e-10 else 0
            
            return np.array([x_peak, x_crest, y_peak, y_crest, z_peak, z_crest])
            
        except Exception as e:
            self.log(f"ACC íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            raise
    
    def extract_features_mic(self, mic_data):
        """MIC íŠ¹ì§• ì¶”ì¶œ - ë‹¤ìš´ìƒ˜í”Œë§ëœ ë°ì´í„°ìš©"""
        try:
            # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ
            if len(mic_data) < 10:
                raise ValueError(f"ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŒ: {len(mic_data)}ê°œ")
            
            mav = np.mean(np.abs(mic_data))
            rms = np.sqrt(np.mean(mic_data**2))
            peak = np.max(np.abs(mic_data))
            
            # IQR ê³„ì‚° ì‹œ ë°ì´í„°ê°€ ì ìœ¼ë¯€ë¡œ ì¡°ì‹¬
            q1, q3 = np.percentile(np.abs(mic_data), [25, 75])
            amp_iqr = q3 - q1
            
            return np.array([mav, rms, peak, amp_iqr])
            
        except Exception as e:
            self.log(f"MIC íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            raise
    
    def get_training_data(self, machine_id, sensor, start_date, end_date, start_time="00:00", end_time="23:59", return_timestamps=False):
        """DBì—ì„œ í•™ìŠµ ë°ì´í„° ì¶”ì¶œ - ë‹¤ìš´ìƒ˜í”Œë§ëœ ë°ì´í„° ì²˜ë¦¬"""
        window_sec = self.sensor_config[sensor]['window_sec']
        
        # DBì—ëŠ” 1ì´ˆì— 10ê°œì”©ë§Œ ì €ì¥ë˜ì–´ ìˆìŒ
        db_sampling_rate = 10  # 1ì´ˆì— 10ê°œ
        window_samples = window_sec * db_sampling_rate  # 5ì´ˆ * 10 = 50ê°œ
        
        # ì „ì²´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
        if sensor == 'acc':
            query = """
            SELECT time, x, y, z
            FROM normal_acc_data
            WHERE machine_id = %s
            AND time >= %s::date + %s::time 
            AND time <= %s::date + %s::time
            ORDER BY time
            """
        else:  # mic
            query = """
            SELECT time, mic_value
            FROM normal_mic_data
            WHERE machine_id = %s
            AND time >= %s::date + %s::time 
            AND time <= %s::date + %s::time
            ORDER BY time
            """
        
        self.log(f"ë°ì´í„° ì¶”ì¶œ ì¤‘: {machine_id}, {sensor}, {start_date} {start_time} ~ {end_date} {end_time}")
        
        try:
            start_time_obj = datetime.now()
            df = pd.read_sql(query, self.conn, 
                            params=(machine_id, start_date, start_time, end_date, end_time))
            
            if df.empty:
                self.log(f"ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
                return None
            
            self.log(f"ì „ì²´ ë°ì´í„°: {len(df)}ê°œ ìƒ˜í”Œ")
            self.log(f"ë°ì´í„° ë¡œë“œ ì‹œê°„: {(datetime.now() - start_time_obj).total_seconds():.1f}ì´ˆ")
            
            # Pythonì—ì„œ 5ì´ˆ ìœˆë„ìš°ë¡œ ë¶„í• 
            features_list = []
            timestamps_list = []  # ìœˆë„ìš° íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
            
            # ì‹œê°„ì„ datetimeìœ¼ë¡œ ë³€í™˜
            df['time'] = pd.to_datetime(df['time'])
            
            # 5ì´ˆ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
            df['window'] = df['time'].dt.floor(f'{window_sec}s')  # 'S' -> 's'ë¡œ ë³€ê²½
            
            # ìœˆë„ìš°ë³„ë¡œ ì²˜ë¦¬
            window_count = 0
            for window, group in df.groupby('window'):
                # ìµœì†Œ 40ê°œ ì´ìƒ (80%) ë°ì´í„°ê°€ ìˆëŠ” ìœˆë„ìš°ë§Œ ì‚¬ìš©
                if len(group) >= window_samples * 0.8:  
                    try:
                        if sensor == 'acc':
                            features = self.extract_features_acc(
                                group['x'].values,
                                group['y'].values,
                                group['z'].values
                            )
                        else:
                            features = self.extract_features_mic(group['mic_value'].values)
                        
                        features_list.append(features)
                        window_count += 1
                        
                        # íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥ (ìœˆë„ìš°ì˜ ì‹œì‘ ì‹œê°„)
                        if return_timestamps:
                            timestamps_list.append(window)
                        
                        # ì§„í–‰ ìƒí™© ë¡œê·¸ (1000ê°œë§ˆë‹¤)
                        if window_count % 1000 == 0:
                            elapsed = (datetime.now() - start_time_obj).total_seconds()
                            rate = window_count / elapsed if elapsed > 0 else 0
                            self.log(f"  ì²˜ë¦¬ì¤‘: {window_count:,}ê°œ ìœˆë„ìš° ({rate:.0f} windows/sec)")
                        
                        # GUI ì—…ë°ì´íŠ¸ (100ê°œë§ˆë‹¤)
                        if window_count % 100 == 0:
                            self.progress_var.set(f"ë°ì´í„° ì¶”ì¶œ ì¤‘: {start_date} ~ {end_date} ({window_count}ê°œ)")
                            self.root.update_idletasks()
                            
                    except Exception as e:
                        if window_count % 1000 == 0:
                            self.log(f"  ìœˆë„ìš° ì²˜ë¦¬ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        continue
            
            total_time = (datetime.now() - start_time_obj).total_seconds()
            self.log(f"ì¶”ì¶œ ì™„ë£Œ: {len(features_list)}ê°œ ìœˆë„ìš° (ì´ {total_time:.1f}ì´ˆ)")
            
            if return_timestamps:
                return np.array(features_list) if features_list else None, timestamps_list
            else:
                return np.array(features_list) if features_list else None
            
        except Exception as e:
            self.log(f"ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def train_model(self):
        """ëª¨ë¸ í•™ìŠµ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
        try:
            total_start_time = datetime.now()
            machine_id = self.machine_var.get()
            sensor = self.sensor_var.get()
            n_trials = self.trials_var.get()
            
            self.log(f"\n{'='*60}")
            self.log(f"{machine_id} / {sensor} ëª¨ë¸ í•™ìŠµ ì‹œì‘")
            self.log(f"{'='*60}")
            
            # í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
            all_features = []
            period_info = []  # ê° ê¸°ê°„ë³„ ì •ë³´ ì €ì¥
            
            self.log(f"í•™ìŠµ ê¸°ê°„: {len(self.training_periods)}ê°œ")
            
            for idx, period in enumerate(self.training_periods):
                # ê¸°ê°„ ì •ë³´ ì¶”ì¶œ (ì´ì „ ë²„ì „ í˜¸í™˜ì„± ìœ ì§€)
                if isinstance(period, dict):
                    start = period['start_date']
                    end = period['end_date']
                    start_time = period.get('start_time', '00:00')
                    end_time = period.get('end_time', '23:59')
                    time_str = f" ({start_time}~{end_time})" if period.get('use_time_filter', False) else ""
                else:
                    # ì´ì „ ë²„ì „ í˜¸í™˜ (íŠœí”Œ í˜•ì‹)
                    start, end = period
                    start_time, end_time = '00:00', '23:59'
                    time_str = ""
                
                self.log(f"\n[{idx+1}/{len(self.training_periods)}] ê¸°ê°„: {start} ~ {end}{time_str}")
                self.progress_var.set(f"ë°ì´í„° ì¶”ì¶œ ì¤‘: {start} ~ {end}{time_str}")
                features = self.get_training_data(machine_id, sensor, start, end, start_time, end_time)
                if features is not None and len(features) > 0:
                    all_features.append(features)
                    period_info.append({
                        'period': f"{start} ~ {end}{time_str}",
                        'start_idx': len(np.vstack(all_features[:-1])) if len(all_features) > 1 else 0,
                        'end_idx': len(np.vstack(all_features)),
                        'count': len(features)
                    })
                    self.log(f"âœ… ì¶”ì¶œ ì„±ê³µ: {len(features)}ê°œ ìœˆë„ìš°")
            
            if not all_features:
                self.log("âŒ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                self.progress_var.set("í•™ìŠµ ë°ì´í„° ì—†ìŒ")
                return
            
            X_train = np.vstack(all_features)
            self.log(f"\nì „ì²´ í•™ìŠµ ë°ì´í„°: {X_train.shape}")
            
            # ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ (fitë§Œ ìˆ˜í–‰)
            self.log("\nìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì‹œì‘...")
            self.progress_var.set("ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì¤‘...")
            scaler_start = datetime.now()
            scaler = CustomRobustScaler()
            scaler.fit(X_train)  # ì „ì²´ ë°ì´í„°ë¡œ ë²”ìœ„ë§Œ í•™ìŠµ
            scaler_time = (datetime.now() - scaler_start).total_seconds()
            self.log(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ ì™„ë£Œ ({scaler_time:.1f}ì´ˆ)")
            
            # ì „ì²´ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (í•œ ë²ˆì—)
            self.log("\nì „ì²´ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì‹œì‘...")
            self.progress_var.set("ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì¤‘...")
            scaling_start = datetime.now()
            X_scaled = scaler.transform(X_train)  # ì „ì²´ë¥¼ í•œ ë²ˆì— ë³€í™˜
            scaling_time = (datetime.now() - scaling_start).total_seconds()
            self.log(f"âœ… ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ ({scaling_time:.1f}ì´ˆ)")
            
            # ğŸ” ë””ë²„ê¹…: ì›ë³¸ ë°ì´í„°ì™€ ìŠ¤ì¼€ì¼ëœ ë°ì´í„° ë¹„êµ
            self.log("\nğŸ” [ë””ë²„ê¹…] ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ê²€ì¦:")
            for i, feature_name in enumerate(self.sensor_config[sensor]['features']):
                self.log(f"\n  [{feature_name}]")
                self.log(f"    ì›ë³¸ - min: {X_train[:, i].min():.2f}, max: {X_train[:, i].max():.2f}, "
                        f"mean: {X_train[:, i].mean():.2f}, std: {X_train[:, i].std():.2f}")
                self.log(f"    ìŠ¤ì¼€ì¼ - min: {X_scaled[:, i].min():.2f}, max: {X_scaled[:, i].max():.2f}, "
                        f"mean: {X_scaled[:, i].mean():.2f}, std: {X_scaled[:, i].std():.2f}")
                
                # ìŠ¤ì¼€ì¼ëŸ¬ íŒŒë¼ë¯¸í„° í™•ì¸
                self.log(f"    ìŠ¤ì¼€ì¼ëŸ¬ - median: {scaler.params[i]['median']:.2f}, "
                        f"IQR: {scaler.params[i]['iqr']:.2f}")
            
            # ğŸ” ì „ì²´ ìŠ¤ì¼€ì¼ëœ ë°ì´í„° í†µê³„
            self.log(f"\nğŸ” [ë””ë²„ê¹…] ì „ì²´ ìŠ¤ì¼€ì¼ëœ ë°ì´í„°:")
            self.log(f"  - ë²”ìœ„: [{X_scaled.min():.4f}, {X_scaled.max():.4f}]")
            self.log(f"  - í‰ê· : {X_scaled.mean():.4f}")
            self.log(f"  - í‘œì¤€í¸ì°¨: {X_scaled.std():.4f}")
            self.log(f"  - ì¤‘ì•™ê°’: {np.median(X_scaled):.4f}")
            self.log(f"  - 95% ë²”ìœ„: [{np.percentile(X_scaled, 2.5):.4f}, "
                    f"{np.percentile(X_scaled, 97.5):.4f}]")
            

            # ì—­ë³€í™˜ í•¨ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)
            def inverse_transform_scores(transformed_scores, transform_info):
                """ë³€í™˜ëœ ì ìˆ˜ë¥¼ ì›ë³¸ìœ¼ë¡œ ì—­ë³€í™˜ (exponential transformì˜ ì—­ë³€í™˜ì€ ë³µì¡í•˜ë¯€ë¡œ ê·¼ì‚¬ì¹˜ ì‚¬ìš©)"""
                # ìƒˆë¡œìš´ exponential transformì˜ ì •í™•í•œ ì—­ë³€í™˜ì€ ë³µì¡í•˜ë¯€ë¡œ
                # ê·¼ì‚¬ì ì¸ ì—­ë³€í™˜ì„ ì œê³µ (ì£¼ë¡œ ë””ë²„ê¹…/í…ŒìŠ¤íŠ¸ ëª©ì )
                median_score = transform_info.get('median_score', -0.5)
                return transformed_scores * 0.1 + median_score  # ê°„ë‹¨í•œ ê·¼ì‚¬
            
            # Isolation Forest ìµœì í™”
            self.log(f"\ní•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ (Optuna {n_trials} trials)")
            optuna_start = datetime.now()
            self.progress_var.set(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘... (0/{n_trials})")
            
            opt_config = self.sensor_config[sensor]
            n_estimators_range = opt_config['n_estimators_range']
            contamination_range = opt_config['contamination_range']
            max_samples_range = opt_config['max_samples_range']
            max_features_range = opt_config['max_features_range']
            
            # ì‹œê°„ì  ë¶„í¬ë¥¼ ê³ ë ¤í•œ ê³„ì¸µì  ìƒ˜í”Œë§
            target_sample_size = min(20000, int(len(X_scaled) * 0.1))  # ìµœëŒ€ 20,000ê°œ
            
            if target_sample_size < len(X_scaled):
                self.log(f"\nğŸ“Š ê³„ì¸µì  ìƒ˜í”Œë§ ì‹œì‘: {len(X_scaled)}ê°œ â†’ {target_sample_size}ê°œ")
                
                sample_indices = []
                
                # ê° ê¸°ê°„ë³„ë¡œ ë¹„ë¡€ì ìœ¼ë¡œ ìƒ˜í”Œë§
                for info in period_info:
                    period_ratio = info['count'] / len(X_scaled)
                    period_sample_size = int(target_sample_size * period_ratio)
                    
                    if period_sample_size > 0:
                        # í•´ë‹¹ ê¸°ê°„ ë‚´ì—ì„œ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
                        period_indices = np.arange(info['start_idx'], info['end_idx'])
                        
                        if period_sample_size < len(period_indices):
                            # ì‹œê°„ ê°„ê²©ì„ ë‘ê³  ê· ë“±í•˜ê²Œ ì„ íƒ
                            step = len(period_indices) // period_sample_size
                            selected = period_indices[::step][:period_sample_size]
                        else:
                            selected = period_indices
                        
                        sample_indices.extend(selected)
                        self.log(f"  - {info['period']}: {len(selected)}ê°œ ìƒ˜í”Œ")
                
                sample_indices = np.array(sample_indices)
                
                # ìƒ˜í”Œë§ëœ ë°ì´í„° ìŠ¤ì¼€ì¼ë§
                X_sample = scaler.transform(X_train[sample_indices])
                self.log(f"âœ… ì´ {len(X_sample)}ê°œ ìƒ˜í”Œ ì¶”ì¶œ ì™„ë£Œ")
            else:
                X_sample = X_scaled
                self.log(f"ğŸ“Š ë°ì´í„°ê°€ ì¶©ë¶„íˆ ì‘ì•„ ì „ì²´ ì‚¬ìš©: {len(X_scaled)}ê°œ")
            
            trial_count = 0
            # study ë³€ìˆ˜ë¥¼ í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ë§Œë“¤ì–´ objective í•¨ìˆ˜ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ
            self.study = create_study(direction='maximize')
            
            # K-fold ì„¤ì • (ë¼ì¦ˆë² ë¦¬íŒŒì´ëŠ” 3ì„ ì‚¬ìš©)
            n_splits = 3
            
            def objective(trial):
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                current_trial = len(self.study.trials)
                self.progress_var.set(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘... ({current_trial}/{n_trials})")
                self.root.update_idletasks()
                
                # Isolation Forest í•˜ì´í¼íŒŒë¼ë¯¸í„°
                n_estimators = trial.suggest_int('n_estimators', n_estimators_range[0], n_estimators_range[1])
                contamination = trial.suggest_float('contamination', contamination_range[0], contamination_range[1], log=True)
                
                # max_samples ì²˜ë¦¬
                if max_samples_range[0] == 'auto':
                    max_samples = trial.suggest_categorical('max_samples', ['auto'] + list(max_samples_range[1:]))
                else:
                    max_samples = trial.suggest_float('max_samples', max_samples_range[0], max_samples_range[-1])
                
                max_features = trial.suggest_float('max_features', max_features_range[0], max_features_range[1])
                
                # K-Fold ì‚¬ìš© (ë¼ì¦ˆë² ë¦¬íŒŒì´ì™€ ë™ì¼)
                if n_splits <= 1:
                    model = IsolationForest(n_estimators=n_estimators, contamination=contamination,
                                          max_samples=max_samples, max_features=max_features,
                                          random_state=42, n_jobs=-1)
                    model.fit(X_sample)
                    preds = model.predict(X_sample)
                    return np.mean(preds == -1)  # ì´ìƒì¹˜ ë¹„ìœ¨
                
                # K-Foldê°€ ìˆëŠ” ê²½ìš°
                from sklearn.model_selection import KFold
                kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
                scores = []
                for train_idx, test_idx in kf.split(X_sample):
                    X_train, X_test = X_sample[train_idx], X_sample[test_idx]
                    model = IsolationForest(n_estimators=n_estimators, contamination=contamination,
                                          max_samples=max_samples, max_features=max_features,
                                          random_state=42, n_jobs=-1)
                    model.fit(X_train)
                    preds = model.predict(X_test)
                    scores.append(np.mean(preds == -1))
                
                return np.mean(scores)
            
            # Optuna ì½œë°± í•¨ìˆ˜
            def optuna_callback(study, trial):
                nonlocal trial_count
                trial_count += 1
                if trial_count % 10 == 0 or trial_count <= 5:
                    max_samples_str = trial.params.get('max_samples', 'N/A')
                    if isinstance(max_samples_str, float):
                        max_samples_str = f"{max_samples_str:.2f}"
                    
                    self.log(f"  Trial {trial_count}: n_estimators={trial.params['n_estimators']}, "
                            f"contamination={trial.params['contamination']:.4f}, "
                            f"max_samples={max_samples_str}, score={trial.value:.4f}")
            
            # ìµœì í™” ì‹¤í–‰ (direction='minimize'ë¡œ ë³€ê²½)
            self.study = create_study(direction='minimize')  # ì´ìƒì¹˜ ë¹„ìœ¨ ìµœì†Œí™”
            self.study.optimize(objective, n_trials=n_trials, callbacks=[optuna_callback])
            
            optuna_time = (datetime.now() - optuna_start).total_seconds()
            self.log(f"\nâœ… ìµœì í™” ì™„ë£Œ ({optuna_time:.1f}ì´ˆ)")
            
            best_params_str = f"n_estimators={self.study.best_params['n_estimators']}, "
            best_params_str += f"contamination={self.study.best_params['contamination']:.4f}, "
            best_params_str += f"max_samples={self.study.best_params.get('max_samples', 'auto')}, "
            best_params_str += f"max_features={self.study.best_params['max_features']:.2f}"
            
            self.log(f"ìµœì  íŒŒë¼ë¯¸í„°: {best_params_str}")
            self.log(f"ìµœì  ì ìˆ˜: {self.study.best_value:.4f}")
            
            # ìµœì  ëª¨ë¸ë¡œ ì „ì²´ ë°ì´í„° í•™ìŠµ
            self.progress_var.set("ìµœì¢… ëª¨ë¸ í•™ìŠµ ì¤‘...")
            self.log("\nğŸ” ìµœì¢… ëª¨ë¸ í•™ìŠµ ë°ì´í„° í™•ì¸...")
            best_n_estimators = self.study.best_params['n_estimators']
            best_contamination = self.study.best_params['contamination']
            best_max_samples = self.study.best_params.get('max_samples', 'auto')
            best_max_features = self.study.best_params['max_features']
            
            # í•™ìŠµ ì§ì „ ë°ì´í„° í™•ì¸
            self.log(f"\nğŸ” [ì¤‘ìš”] ìµœì¢… í•™ìŠµ ë°ì´í„° ê²€ì¦:")
            self.log(f"  - X_scaled shape: {X_scaled.shape}")
            self.log(f"  - X_scaled dtype: {X_scaled.dtype}")
            self.log(f"  - X_scaled ì „ì²´ ë²”ìœ„: [{X_scaled.min():.4f}, {X_scaled.max():.4f}]")
            self.log(f"  - X_scaled ì „ì²´ í‰ê· : {X_scaled.mean():.4f}")
            self.log(f"  - X_scaled ì „ì²´ í‘œì¤€í¸ì°¨: {X_scaled.std():.4f}")
            
            # ì²« 5ê°œ ìƒ˜í”Œ ìƒì„¸ í™•ì¸
            self.log(f"\nğŸ” ì²« 5ê°œ ìƒ˜í”Œ ìƒì„¸ í™•ì¸:")
            for i in range(min(5, len(X_scaled))):
                self.log(f"  ìƒ˜í”Œ {i}: {X_scaled[i]}")
            
            # ê° íŠ¹ì§•ë³„ ë²”ìœ„ í™•ì¸
            self.log(f"\nğŸ” ê° íŠ¹ì§•ë³„ ìŠ¤ì¼€ì¼ëœ ë²”ìœ„:")
            for i, feature_name in enumerate(self.sensor_config[sensor]['features']):
                self.log(f"  [{feature_name}] ë²”ìœ„: [{X_scaled[:, i].min():.4f}, {X_scaled[:, i].max():.4f}], "
                        f"í‰ê· : {X_scaled[:, i].mean():.4f}, í‘œì¤€í¸ì°¨: {X_scaled[:, i].std():.4f}")
            
            model = IsolationForest(n_estimators=best_n_estimators, contamination=best_contamination,
                                  max_samples=best_max_samples, max_features=best_max_features,
                                  random_state=42, n_jobs=-1)
            
            self.log(f"\nìµœì¢… ëª¨ë¸ í•™ìŠµ ì‹œì‘ ({best_params_str})...")
            
            # ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬: ê¸°ê°„ë³„ ë¹„ë¡€ ìƒ˜í”Œë§
            max_train_samples = 10000
            if len(X_scaled) > max_train_samples:
                self.log(f"\nğŸ“Š ê¸°ê°„ë³„ ë¹„ë¡€ ìƒ˜í”Œë§: {len(X_scaled):,}ê°œ â†’ {max_train_samples:,}ê°œ")
                
                sampled_indices = []
                for info in period_info:
                    period_start = info['start_idx']
                    period_end = info['end_idx']
                    period_count = period_end - period_start
                    
                    # ê° ê¸°ê°„ì—ì„œ ë¹„ë¡€ì ìœ¼ë¡œ ìƒ˜í”Œ
                    period_sample_size = int(max_train_samples * (period_count / len(X_scaled)))
                    if period_sample_size > 0:
                        # ê· ë“± ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
                        indices = np.linspace(period_start, period_end-1, period_sample_size, dtype=int)
                        sampled_indices.extend(indices)
                        self.log(f"  - {info['period']}: {period_sample_size}ê°œ ìƒ˜í”Œ")
                
                sampled_indices = np.array(sampled_indices)
                X_train_final = X_scaled[sampled_indices]
            else:
                X_train_final = X_scaled
                
            model.fit(X_train_final)
            
            self.log("âœ… ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
            
            # ğŸ” ë””ë²„ê¹…: Isolation Forest ëª¨ë¸ ì •ë³´
            self.log(f"\nğŸ” [ë””ë²„ê¹…] Isolation Forest ëª¨ë¸ ì •ë³´:")
            self.log(f"  - íŠ¸ë¦¬ ê°œìˆ˜: {len(model.estimators_)}ê°œ")
            self.log(f"  - ìƒ˜í”Œ í¬ê¸°: {model.max_samples_}")
            if hasattr(model, 'offset_'):
                self.log(f"  - Offset: {model.offset_}")
            
            # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (ì„ íƒì )
            skip_evaluation = self.skip_eval_var.get()  # GUI ì²´í¬ë°•ìŠ¤ ê°’ ì‚¬ìš©
            
            # ğŸ” í•­ìƒ ì‘ì€ ìƒ˜í”Œë¡œ score ë¶„í¬ í™•ì¸
            sample_size = min(1000, len(X_scaled))
            sample_indices = np.random.choice(len(X_scaled), sample_size, replace=False)
            debug_scores = model.score_samples(X_scaled[sample_indices])
            
            # ì ìˆ˜ ë³€í™˜
            debug_transformed, transform_info = self.transform_scores(debug_scores)
            
            self.log(f"\nğŸ” [ë””ë²„ê¹…] ìƒ˜í”Œ {sample_size}ê°œì˜ score ë¶„í¬ (ì›ë³¸):")
            self.log(f"  - ë²”ìœ„: [{debug_scores.min():.2f}, {debug_scores.max():.2f}]")
            self.log(f"  - ë³€í™˜ í›„: [{debug_transformed.min():.2f}, {debug_transformed.max():.2f}]")
            
            if skip_evaluation:
                self.log("\nâš¡ ì„±ëŠ¥ í‰ê°€ ë‹¨ê³„ ìŠ¤í‚µ (ë¹ ë¥¸ í•™ìŠµ ëª¨ë“œ)")
                self.log(f"ì „ì²´ ë°ì´í„°({len(X_scaled):,}ê°œ)ë¡œ score ê³„ì‚° ì¤‘...")
                
                # ê°„ë‹¨í•œ ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€ëµì ì¸ ì„±ëŠ¥ë§Œ í™•ì¸
                sample_size = min(10000, len(X_scaled))
                sample_indices = np.random.choice(len(X_scaled), sample_size, replace=False)
                sample_scores = model.score_samples(X_scaled[sample_indices])
                
                # ì ìˆ˜ ë³€í™˜
                sample_scores_transformed, transform_info = self.transform_scores(sample_scores)
                
                # ê²°ì • ê²½ê³„ ì„¤ì •: ì •ìƒ ë°ì´í„°ì˜ í•˜ìœ„ 5%ë¥¼ ê²½ê³„ë¡œ
                decision_boundary = np.percentile(sample_scores_transformed, 5)
                
                # ğŸ” ë””ë²„ê¹…: boundary ê³„ì‚° ê³¼ì •
                self.log(f"\nğŸ” [ë””ë²„ê¹…] Decision Boundary ê³„ì‚°:")
                self.log(f"  - ì›ë³¸ Score ë¶„í¬: [{sample_scores.min():.4f}, {sample_scores.max():.4f}]")
                self.log(f"  - ë³€í™˜ Score ë¶„í¬: [{sample_scores_transformed.min():.2f}, {sample_scores_transformed.max():.2f}]")
                self.log(f"  - ê²°ì • ê²½ê³„: {decision_boundary:.3f}")
                
                # ì •ìƒ ë°ì´í„° ë¶„í¬ í™•ì¸
                normal_scores = sample_scores_transformed[sample_scores_transformed > decision_boundary]
                self.log(f"\nğŸ“Š ì •ìƒ ë°ì´í„° ë¶„í¬:")
                self.log(f"  - ë²”ìœ„: [{normal_scores.min():.3f}, {normal_scores.max():.3f}]")
                self.log(f"  - í‰ê· : {normal_scores.mean():.3f}")
                self.log(f"  - ì¤‘ì•™ê°’: {np.median(normal_scores):.3f}")
                
                model_info = {
                    'machine_id': machine_id,
                    'sensor': sensor,
                    'model_type': 'IsolationForest',
                    'train_samples': len(X_train),
                    'training_periods': self.training_periods,
                    'features': self.sensor_config[sensor]['features'],
                    'best_params': self.study.best_params,
                    'decision_boundary': float(decision_boundary),
                    'boundary_method': 'percentile_5',
                    'score_transform': transform_info,
                    'boundary_stats': {
                        'percentile_5': float(decision_boundary),
                        'method': 'percentile_based'
                    },
                    'evaluation_skipped': True,
                    'trained_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
            else:
                # ì „ì²´ ì„±ëŠ¥ í‰ê°€ (ë°°ì¹˜ ì²˜ë¦¬)
                self.log("\nëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
                eval_start = datetime.now()
                
                batch_size = 10000
                predictions = []
                scores = []
                
                self.log(f"ì „ì²´ {len(X_scaled):,}ê°œ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰ (ë°°ì¹˜ í¬ê¸°: {batch_size:,})")
                
                for i in range(0, len(X_scaled), batch_size):
                    batch_end = min(i + batch_size, len(X_scaled))
                    batch = X_scaled[i:batch_end]
                    
                    # ë°°ì¹˜ ì˜ˆì¸¡
                    batch_predictions = model.predict(batch)
                    batch_scores = model.score_samples(batch)
                    
                    predictions.extend(batch_predictions)
                    scores.extend(batch_scores)
                    
                    # ì§„í–‰ ìƒí™© ë¡œê·¸ (10ê°œ ë°°ì¹˜ë§ˆë‹¤)
                    if (i // batch_size + 1) % 10 == 0 or batch_end == len(X_scaled):
                        progress = batch_end / len(X_scaled) * 100
                        elapsed = (datetime.now() - eval_start).total_seconds()
                        rate = batch_end / elapsed if elapsed > 0 else 0
                        eta = (len(X_scaled) - batch_end) / rate if rate > 0 else 0
                        
                        self.log(f"  ì˜ˆì¸¡ ì§„í–‰: {batch_end:,}/{len(X_scaled):,} ({progress:.1f}%) "
                                f"- {rate:.0f} samples/sec, ETA: {eta:.0f}ì´ˆ")
                        self.progress_var.set(f"ì„±ëŠ¥ í‰ê°€ ì¤‘... {progress:.1f}%")
                        self.root.update_idletasks()
                
                # numpy ë°°ì—´ë¡œ ë³€í™˜
                predictions = np.array(predictions)
                scores = np.array(scores)
                
                eval_time = (datetime.now() - eval_start).total_seconds()
                self.log(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ ({eval_time:.1f}ì´ˆ)")
                
                # ì ìˆ˜ ë³€í™˜
                scores_transformed, transform_info = self.transform_scores(scores)
                
                # ê²°ì • ê²½ê³„ ê³„ì‚° (5% ë°±ë¶„ìœ„ìˆ˜ ë°©ì‹)
                self.log("\nê²°ì • ê²½ê³„ ê³„ì‚° ì¤‘...")
                
                # ê²°ì • ê²½ê³„ ì„¤ì •: ì •ìƒ ë°ì´í„°ì˜ í•˜ìœ„ 5%ë¥¼ ê²½ê³„ë¡œ
                decision_boundary = np.percentile(scores_transformed, 5)
                
                # ì •ìƒ ë°ì´í„° ë¶„í¬ í™•ì¸
                normal_scores_transformed = scores_transformed[scores_transformed > decision_boundary]
                self.log(f"\nğŸ“Š ì •ìƒ ë°ì´í„° ë¶„í¬:")
                self.log(f"  - ë²”ìœ„: [{normal_scores_transformed.min():.3f}, {normal_scores_transformed.max():.3f}]")
                self.log(f"  - í‰ê· : {normal_scores_transformed.mean():.3f}")
                self.log(f"  - ì¤‘ì•™ê°’: {np.median(normal_scores_transformed):.3f}")
                self.log(f"  - ê²°ì • ê²½ê³„: {decision_boundary:.3f}")
                
                # ğŸ” ë””ë²„ê¹…: ì „ì²´ í‰ê°€ì—ì„œë„ ê²½ê³„ê°’ í™•ì¸
                self.log(f"\nğŸ” [ë””ë²„ê¹…] ì „ì²´ í‰ê°€ Decision Boundary:")
                self.log(f"  - ì „ì²´ Score ë¶„í¬ (ì›ë³¸):")
                self.log(f"    â€¢ ë²”ìœ„: [{np.min(scores):.2f}, {np.max(scores):.2f}]")
                self.log(f"    â€¢ í‰ê· : {np.mean(scores):.2f}")
                self.log(f"    â€¢ í‘œì¤€í¸ì°¨: {np.std(scores):.2f}")
                self.log(f"  - ì „ì²´ Score ë¶„í¬ (ë³€í™˜):")
                self.log(f"    â€¢ ë²”ìœ„: [{np.min(scores_transformed):.2f}, {np.max(scores_transformed):.2f}]")
                self.log(f"    â€¢ í‰ê· : {np.mean(scores_transformed):.2f}")
                self.log(f"    â€¢ í‘œì¤€í¸ì°¨: {np.std(scores_transformed):.2f}")
                self.log(f"  - Percentiles:")
                for p in [1, 5, 10, 25, 50, 75, 90, 95, 99]:
                    self.log(f"    â€¢ P{p}: {np.percentile(scores_transformed, p):.2f}")
                
                
                # ì´ìƒì¹˜ ë¹„ìœ¨ ê³„ì‚°
                predictions = (scores_transformed > decision_boundary).astype(int) * 2 - 1
                anomaly_ratio = np.sum(predictions == -1) / len(predictions) * 100
                
                self.log(f"âœ… í•™ìŠµ ì™„ë£Œ!")
                self.log(f"  - ì´ìƒì¹˜ ë¹„ìœ¨: {anomaly_ratio:.2f}%")
                self.log(f"  - ê²°ì • ê²½ê³„: {decision_boundary:.4f}")
                self.log(f"  - ë³€í™˜ ì ìˆ˜ ë²”ìœ„: [{np.min(scores_transformed):.4f}, {np.max(scores_transformed):.4f}]")
                self.log(f"  - ë³€í™˜ ì ìˆ˜ í‰ê· Â±í‘œì¤€í¸ì°¨: {np.mean(scores_transformed):.4f} Â± {np.std(scores_transformed):.4f}")
                
                # ê¸°ê°„ë³„ ì„±ëŠ¥ ë¶„ì„
                self.log("\nğŸ“Š ê¸°ê°„ë³„ ì„±ëŠ¥:")
                for info in period_info:
                    start_idx = info['start_idx']
                    end_idx = info['end_idx']
                    period_scores_transformed = scores_transformed[start_idx:end_idx]
                    period_predictions = predictions[start_idx:end_idx]
                    period_anomaly_ratio = np.sum(period_predictions == -1) / len(period_predictions) * 100
                    
                    self.log(f"  - {info['period']}: ì´ìƒ {period_anomaly_ratio:.1f}%, "
                            f"ì ìˆ˜ {np.mean(period_scores_transformed):.2f}Â±{np.std(period_scores_transformed):.2f}")
                
                # 2ì°¨ ë¡œì§ì„ ìœ„í•œ ìƒì„¸ í†µê³„ ë¶„ì„
                self.log("\nğŸ“Š 2ì°¨ ë¡œì§ ê²½ê³„ê°’ ì„¤ì •ì„ ìœ„í•œ ë¶„ì„:")
                
                # ì •ìƒ/ì´ìƒ ë°ì´í„° ë¶„ë¦¬
                normal_scores_transformed = scores_transformed[predictions == 1]
                anomaly_scores_transformed = scores_transformed[predictions == -1]
                
                self.log(f"  ì •ìƒ ë°ì´í„° ì ìˆ˜ ë¶„í¬:")
                self.log(f"    - ê°œìˆ˜: {len(normal_scores_transformed):,}ê°œ ({len(normal_scores_transformed)/len(scores_transformed)*100:.1f}%)")
                self.log(f"    - í‰ê· Â±í‘œì¤€í¸ì°¨: {np.mean(normal_scores_transformed):.2f} Â± {np.std(normal_scores_transformed):.2f}")
                self.log(f"    - ìµœì†Œ/ìµœëŒ€: {np.min(normal_scores_transformed):.2f} / {np.max(normal_scores_transformed):.2f}")
                
                self.log(f"  ì´ìƒ ë°ì´í„° ì ìˆ˜ ë¶„í¬:")
                self.log(f"    - ê°œìˆ˜: {len(anomaly_scores_transformed):,}ê°œ ({len(anomaly_scores_transformed)/len(scores_transformed)*100:.1f}%)")
                self.log(f"    - í‰ê· Â±í‘œì¤€í¸ì°¨: {np.mean(anomaly_scores_transformed):.2f} Â± {np.std(anomaly_scores_transformed):.2f}")
                self.log(f"    - ìµœì†Œ/ìµœëŒ€: {np.min(anomaly_scores_transformed):.2f} / {np.max(anomaly_scores_transformed):.2f}")
                
                # í¼ì„¼íƒ€ì¼ ê¸°ë°˜ ê²½ê³„ê°’ í›„ë³´
                percentiles = [0.1, 0.5, 1, 2, 3, 5, 10, 15, 20]
                percentile_values = {}
                
                self.log("\n  ì „ì²´ ì ìˆ˜ í¼ì„¼íƒ€ì¼:")
                for p in percentiles:
                    val = np.percentile(scores_transformed, p)
                    percentile_values[f"p{p}"] = float(val)
                    self.log(f"    - {p:5.1f}%: {val:8.2f}")
                
                # ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬
                self.log("\n  ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬:")
                score_ranges = [
                    (-np.inf, -17, "ê·¹ì‹¬í•œ ì´ìƒ"),
                    (-17, -15, "ì‹¬ê°í•œ ì´ìƒ"),
                    (-15, -10, "ì¤‘ê°„ ì´ìƒ"),
                    (-10, -5, "ê²½ë¯¸í•œ ì´ìƒ"),
                    (-5, -2, "ì˜ì‹¬ êµ¬ê°„"),
                    (-2, 0, "ê²½ê³„ êµ¬ê°„"),
                    (0, 1, "ì •ìƒ ë²”ìœ„"),
                    (1, np.inf, "ë§¤ìš° ì •ìƒ")
                ]
                
                score_distribution = {}
                self.log("    [ì „ì²´ ë°ì´í„°]")
                for min_score, max_score, label in score_ranges:
                    count = np.sum((scores_transformed >= min_score) & (scores_transformed < max_score))
                    ratio = count / len(scores_transformed) * 100
                    self.log(f"    - {label:12s} [{min_score:6.0f} ~ {max_score:6.0f}]: "
                            f"{count:6,}ê°œ ({ratio:5.1f}%)")
                
                # ì •ìƒ ë°ì´í„°ì˜ ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬
                self.log("\n    [ì •ìƒìœ¼ë¡œ ë¶„ë¥˜ëœ ë°ì´í„°]")
                normal_distribution = {}
                for min_score, max_score, label in score_ranges:
                    count = np.sum((normal_scores_transformed >= min_score) & (normal_scores_transformed < max_score))
                    ratio = count / len(normal_scores_transformed) * 100 if len(normal_scores_transformed) > 0 else 0
                    normal_distribution[label] = {
                        'count': int(count),
                        'ratio': float(ratio),
                        'range': [float(min_score) if min_score != -np.inf else None,
                                 float(max_score) if max_score != np.inf else None]
                    }
                    if count > 0:
                        self.log(f"    - {label:12s} [{min_score:6.0f} ~ {max_score:6.0f}]: "
                                f"{count:6,}ê°œ ({ratio:5.1f}%)")
                
                # ì´ìƒ ë°ì´í„°ì˜ ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬
                self.log("\n    [ì´ìƒìœ¼ë¡œ ë¶„ë¥˜ëœ ë°ì´í„°]")
                anomaly_distribution = {}
                for min_score, max_score, label in score_ranges:
                    count = np.sum((anomaly_scores_transformed >= min_score) & (anomaly_scores_transformed < max_score))
                    ratio = count / len(anomaly_scores_transformed) * 100 if len(anomaly_scores_transformed) > 0 else 0
                    anomaly_distribution[label] = {
                        'count': int(count),
                        'ratio': float(ratio),
                        'range': [float(min_score) if min_score != -np.inf else None,
                                 float(max_score) if max_score != np.inf else None]
                    }
                    if count > 0:
                        self.log(f"    - {label:12s} [{min_score:6.0f} ~ {max_score:6.0f}]: "
                                f"{count:6,}ê°œ ({ratio:5.1f}%)")
                
                # ì „ì²´ í†µí•© ë¶„í¬
                for min_score, max_score, label in score_ranges:
                    total_count = np.sum((scores_transformed >= min_score) & (scores_transformed < max_score))
                    normal_count = np.sum((normal_scores_transformed >= min_score) & (normal_scores_transformed < max_score))
                    anomaly_count = np.sum((anomaly_scores_transformed >= min_score) & (anomaly_scores_transformed < max_score))
                    
                    score_distribution[label] = {
                        'total': {
                            'count': int(total_count),
                            'ratio': float(total_count / len(scores_transformed) * 100)
                        },
                        'normal': {
                            'count': int(normal_count),
                            'ratio': float(normal_count / len(normal_scores_transformed) * 100) if len(normal_scores_transformed) > 0 else 0,
                            'of_total': float(normal_count / total_count * 100) if total_count > 0 else 0
                        },
                        'anomaly': {
                            'count': int(anomaly_count),
                            'ratio': float(anomaly_count / len(anomaly_scores_transformed) * 100) if len(anomaly_scores_transformed) > 0 else 0,
                            'of_total': float(anomaly_count / total_count * 100) if total_count > 0 else 0
                        },
                        'range': [float(min_score) if min_score != -np.inf else None,
                                 float(max_score) if max_score != np.inf else None]
                    }
                
                # êµì°¨ ë¶„ì„
                self.log("\n  ğŸ“Š ì •ìƒ/ì´ìƒ êµì°¨ ë¶„ì„:")
                
                # ì •ìƒìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆì§€ë§Œ ì ìˆ˜ê°€ ë‚®ì€ ë°ì´í„°
                normal_but_low_score = np.sum(normal_scores_transformed < -2)
                if normal_but_low_score > 0:
                    self.log(f"    - ì •ìƒ ë¶„ë¥˜ì§€ë§Œ ì ìˆ˜ < -2: {normal_but_low_score:,}ê°œ "
                            f"({normal_but_low_score/len(normal_scores_transformed)*100:.1f}%)")
                    
                    # ìƒì„¸ ë¶„í¬
                    for threshold in [-5, -10, -15]:
                        count = np.sum(normal_scores_transformed < threshold)
                        if count > 0:
                            self.log(f"      â€¢ ì ìˆ˜ < {threshold}: {count:,}ê°œ "
                                    f"({count/len(normal_scores_transformed)*100:.2f}%)")
                
                # ì´ìƒìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆì§€ë§Œ ì ìˆ˜ê°€ ë†’ì€ ë°ì´í„°
                if len(anomaly_scores_transformed) > 0:
                    anomaly_but_high_score = np.sum(anomaly_scores_transformed > 0)
                    if anomaly_but_high_score > 0:
                        self.log(f"    - ì´ìƒ ë¶„ë¥˜ì§€ë§Œ ì ìˆ˜ > 0: {anomaly_but_high_score:,}ê°œ "
                                f"({anomaly_but_high_score/len(anomaly_scores_transformed)*100:.1f}%)")
                
                # ê²½ê³„ ê·¼ì²˜ ë°ì´í„° ë¶„ì„
                boundary_range = 2  # ê²°ì • ê²½ê³„ Â±2
                near_boundary = np.sum(np.abs(scores_transformed - decision_boundary) < boundary_range)
                self.log(f"    - ê²°ì • ê²½ê³„({decision_boundary:.2f}) Â±{boundary_range} ë²”ìœ„: "
                        f"{near_boundary:,}ê°œ ({near_boundary/len(scores_transformed)*100:.1f}%)")
                
                # 2ì°¨ ë¡œì§ ê²½ê³„ê°’ ì¶”ì²œ
                self.log("\n  ğŸ’¡ 2ì°¨ ë¡œì§ ê²½ê³„ê°’ ì¶”ì²œ:")
                
                # ë°©ë²• 1: ì •ìƒ ë°ì´í„°ì˜ í•˜ìœ„ í¼ì„¼íƒ€ì¼
                normal_lower_bound = np.percentile(normal_scores_transformed, 1)  # ì •ìƒì˜ í•˜ìœ„ 1%
                self.log(f"    - ì •ìƒ ë°ì´í„° í•˜ìœ„ 1%: {normal_lower_bound:.2f}")
                
                # ë°©ë²• 2: ì „ì²´ ë°ì´í„°ì˜ íŠ¹ì • í¼ì„¼íƒ€ì¼
                overall_p3 = np.percentile(scores_transformed, 3)
                self.log(f"    - ì „ì²´ ë°ì´í„° í•˜ìœ„ 3%: {overall_p3:.2f}")
                
                # ë°©ë²• 3: í‰ê·  - n*í‘œì¤€í¸ì°¨
                mean_minus_2std = np.mean(scores_transformed) - 2 * np.std(scores_transformed)
                mean_minus_3std = np.mean(scores_transformed) - 3 * np.std(scores_transformed)
                self.log(f"    - í‰ê·  - 2Ïƒ: {mean_minus_2std:.2f}")
                self.log(f"    - í‰ê·  - 3Ïƒ: {mean_minus_3std:.2f}")
                
                # ë°©ë²• 4: ì´ìƒ ë°ì´í„°ì˜ ìƒìœ„ ê²½ê³„
                if len(anomaly_scores_transformed) > 0:
                    anomaly_upper = np.percentile(anomaly_scores_transformed, 90)  # ì´ìƒì˜ ìƒìœ„ 10%
                    self.log(f"    - ì´ìƒ ë°ì´í„° ìƒìœ„ 10%: {anomaly_upper:.2f}")
                
                # ëª¨ë¸ ì •ë³´
                model_info = {
                    'machine_id': machine_id,
                    'sensor': sensor,
                    'model_type': 'IsolationForest',
                    'train_samples': len(X_train),
                    'training_periods': self.training_periods,
                    'features': self.sensor_config[sensor]['features'],
                    'best_params': self.study.best_params,
                    'decision_boundary': float(decision_boundary),
                    'boundary_method': 'percentile_5',
                    'score_transform': transform_info,
                    'boundary_stats': {
                        'percentile_5': float(decision_boundary),
                        'method': 'percentile_based'
                    },
                    'anomaly_ratio': float(anomaly_ratio),
                    'score_statistics': {
                        'mean': float(np.mean(scores_transformed)),
                        'std': float(np.std(scores_transformed)),
                        'min': float(np.min(scores_transformed)),
                        'max': float(np.max(scores_transformed))
                    },
                    'normal_score_statistics': {
                        'count': int(len(normal_scores_transformed)),
                        'mean': float(np.mean(normal_scores_transformed)),
                        'std': float(np.std(normal_scores_transformed)),
                        'min': float(np.min(normal_scores_transformed)),
                        'max': float(np.max(normal_scores_transformed)),
                        'percentiles': {
                            'p1': float(np.percentile(normal_scores_transformed, 1)),
                            'p5': float(np.percentile(normal_scores_transformed, 5)),
                            'p10': float(np.percentile(normal_scores_transformed, 10))
                        }
                    },
                    'anomaly_score_statistics': {
                        'count': int(len(anomaly_scores_transformed)),
                        'mean': float(np.mean(anomaly_scores_transformed)) if len(anomaly_scores_transformed) > 0 else None,
                        'std': float(np.std(anomaly_scores_transformed)) if len(anomaly_scores_transformed) > 0 else None,
                        'min': float(np.min(anomaly_scores_transformed)) if len(anomaly_scores_transformed) > 0 else None,
                        'max': float(np.max(anomaly_scores_transformed)) if len(anomaly_scores_transformed) > 0 else None
                    },
                    'score_percentiles': percentile_values,
                    'score_distribution': score_distribution,
                    'secondary_thresholds': {
                        'normal_p1': float(normal_lower_bound),
                        'overall_p3': float(overall_p3),
                        'mean_minus_2std': float(mean_minus_2std),
                        'mean_minus_3std': float(mean_minus_3std),
                        'anomaly_p90': float(anomaly_upper) if len(anomaly_scores_transformed) > 0 else None
                    },
                    'evaluation_skipped': False,
                    'trained_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
            
            # ëª¨ë¸ ì €ì¥
            self.log("\nëª¨ë¸ ì €ì¥ ì¤‘...")
            timestamp = datetime.now().strftime('%y%m%d_%H%M%S')
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            model_dir = f"./models/{machine_id}/{sensor}/current_model"
            scale_dir = f"./models/{machine_id}/{sensor}/current_scale"
            os.makedirs(model_dir, exist_ok=True)
            os.makedirs(scale_dir, exist_ok=True)
            
            # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
            for d in [model_dir, scale_dir]:
                for f in os.listdir(d):
                    os.remove(os.path.join(d, f))
            
            # ìƒˆ íŒŒì¼ ì €ì¥
            model_path = os.path.join(model_dir, f"{timestamp}_model.pkl")
            scaler_path = os.path.join(scale_dir, f"{timestamp}_scaler.pkl")
            info_path = os.path.join(model_dir, f"{timestamp}_model_info.json")
            
            joblib.dump(model, model_path)
            scaler.save(scaler_path)
            
            # ğŸ” ë””ë²„ê¹…: ì €ì¥ëœ íŒŒì¼ ê²€ì¦
            self.log(f"\nğŸ” [ë””ë²„ê¹…] ì €ì¥ëœ íŒŒì¼ ê²€ì¦:")
            
            # ëª¨ë¸ ì¬ë¡œë“œ í…ŒìŠ¤íŠ¸
            test_model = joblib.load(model_path)
            self.log(f"  - ëª¨ë¸ ì¬ë¡œë“œ ì„±ê³µ: {type(test_model).__name__}")
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ì¬ë¡œë“œ í…ŒìŠ¤íŠ¸
            test_scaler = CustomRobustScaler()
            test_scaler.load(scaler_path)
            self.log(f"  - ìŠ¤ì¼€ì¼ëŸ¬ ì¬ë¡œë“œ ì„±ê³µ: {len(test_scaler.params)}ê°œ íŠ¹ì§•")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ê²€ì¦
            test_data = X_train[:10]  # ì²˜ìŒ 10ê°œ ìƒ˜í”Œ
            test_scaled = test_scaler.transform(test_data)
            test_scores = test_model.score_samples(test_scaled)
            
            # ì ìˆ˜ ë³€í™˜ í…ŒìŠ¤íŠ¸
            test_transformed, _ = self.transform_scores(test_scores)
            
            self.log(f"  - í…ŒìŠ¤íŠ¸ ë³€í™˜: ì›ë³¸ [{test_data.min():.2f}, {test_data.max():.2f}] â†’ "
                    f"ìŠ¤ì¼€ì¼ [{test_scaled.min():.2f}, {test_scaled.max():.2f}]")
            self.log(f"  - í…ŒìŠ¤íŠ¸ ìŠ¤ì½”ì–´: ì›ë³¸ [{test_scores.min():.4f}, {test_scores.max():.4f}] â†’ "
                    f"ë³€í™˜ [{test_transformed.min():.2f}, {test_transformed.max():.2f}]")
            self.log(f"  - í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡: {test_model.predict(test_scaled)}")
            
            with open(info_path, 'w') as f:
                json.dump(model_info, f, indent=2)
            
            self.log(f"\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ:")
            self.log(f"  - ëª¨ë¸: {model_path}")
            self.log(f"  - ìŠ¤ì¼€ì¼ëŸ¬: {scaler_path}")
            self.log(f"  - ì •ë³´: {info_path}")
            
            # í˜„ì¬ ëª¨ë¸ ì •ë³´ ì €ì¥
            self.current_model_info = model_info
            
            # ì „ì²´ ì†Œìš” ì‹œê°„
            total_time = (datetime.now() - total_start_time).total_seconds()
            self.log(f"\nì „ì²´ í•™ìŠµ ì†Œìš” ì‹œê°„: {total_time:.1f}ì´ˆ ({total_time/60:.1f}ë¶„)")
            
            self.progress_var.set("í•™ìŠµ ì™„ë£Œ!")
            messagebox.showinfo("ì™„ë£Œ", f"ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\në¨¸ì‹ : {machine_id}\nì„¼ì„œ: {sensor}")
            
        except Exception as e:
            self.log(f"âŒ í•™ìŠµ ì‹¤íŒ¨: {e}")
            self.progress_var.set("í•™ìŠµ ì‹¤íŒ¨")
            messagebox.showerror("ì˜¤ë¥˜", f"í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            self.train_button.config(state='normal')
    
    def start_training(self):
        if not self.training_periods:
            messagebox.showerror("ì˜¤ë¥˜", "í•™ìŠµ ê¸°ê°„ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
            return
        
        if not self.conn:
            messagebox.showerror("ì˜¤ë¥˜", "DB ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        self.train_button.config(state='disabled')
        thread = threading.Thread(target=self.train_model)
        thread.daemon = True
        thread.start()
    
    def test_model(self):
        """ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        try:
            # ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ í™•ì¸
            model_path = self.model_file_var.get()
            scaler_path = self.scaler_file_var.get()
            
            if not model_path or not scaler_path:
                messagebox.showerror("ì˜¤ë¥˜", "ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
                return
            
            if not os.path.exists(model_path) or not os.path.exists(scaler_path):
                messagebox.showerror("ì˜¤ë¥˜", "ì„ íƒí•œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return
            
            # ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            self.log("ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì¤‘...")
            model = joblib.load(model_path)
            
            scaler = CustomRobustScaler()
            scaler.load(scaler_path)
            
            # ëª¨ë¸ ì •ë³´ ë¡œë“œ
            info_path = model_path.replace('_model.pkl', '_model_info.json')
            if os.path.exists(info_path):
                with open(info_path, 'r') as f:
                    model_info = json.load(f)
            else:
                # ê¸°ë³¸ ì •ë³´ ì‚¬ìš©
                model_info = {
                    'decision_boundary': -5.0,
                    'sensor': self.test_sensor_var.get(),
                    'score_transform': None
                }
            
            # í…ŒìŠ¤íŠ¸ ì„¤ì •
            machine_id = self.test_machine_var.get()
            sensor = self.test_sensor_var.get()
            
            # ë‚ ì§œ ë²”ìœ„ í™•ì¸
            start_date = self.test_start_date.get_date()
            end_date = self.test_end_date.get_date()
            
            # ìµœëŒ€ 5ì¼ ì œí•œ
            if (end_date - start_date).days > 5:
                messagebox.showerror("ì˜¤ë¥˜", "í…ŒìŠ¤íŠ¸ ê¸°ê°„ì€ ìµœëŒ€ 5ì¼ê¹Œì§€ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                return
            
            if start_date > end_date:
                messagebox.showerror("ì˜¤ë¥˜", "ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ëŠ¦ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # ì„¼ì„œë³„ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ì„ íƒ
            table_name = f"normal_{sensor}_data"
            
            # ê²°ê³¼ í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
            self.result_text.delete(1.0, tk.END)
            self.result_text.insert(tk.END, f"Model Test Results\n")
            self.result_text.insert(tk.END, f"{'='*60}\n")
            self.result_text.insert(tk.END, f"Machine: {machine_id}, Sensor: {sensor}\n")
            self.result_text.insert(tk.END, f"Table: {table_name}\n")
            self.result_text.insert(tk.END, f"Period: {start_date} ~ {end_date}\n")
            self.result_text.insert(tk.END, f"Sampling: {self.sampling_info_var.get()}\n")
            self.result_text.insert(tk.END, f"Decision Boundary: {model_info.get('decision_boundary', 'N/A')}\n\n")
            
            # ì „ì²´ ê¸°ê°„ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸°
            self.log(f"Extracting test data: {start_date} ~ {end_date}")
            
            # ì „ì²´ ê¸°ê°„ ë°ì´í„° ì¶”ì¶œ (í•œ ë²ˆì—)
            start_str = start_date.strftime("%Y-%m-%d")
            end_str = end_date.strftime("%Y-%m-%d")
            
            # get_training_data í•¨ìˆ˜ ì‚¬ìš© (ì „ì²´ ê¸°ê°„)
            test_features, test_timestamps = self.get_training_data(machine_id, sensor, start_str, end_str, "00:00", "23:59", return_timestamps=True)
            
            if test_features is None or len(test_features) == 0:
                self.result_text.insert(tk.END, "No data found for the selected period.\n")
                self.log("No test data found")
                return
            
            self.result_text.insert(tk.END, f"Processing {len(test_features)} windows...\n")
            self.root.update_idletasks()
            
            # ìŠ¤ì¼€ì¼ë§
            X_test_scaled = scaler.transform(test_features)
            self.log(f"âœ… Test data normalized: {X_test_scaled.shape}")
            
            # ì˜ˆì¸¡
            self.log("Running model predictions...")
            predictions = model.predict(X_test_scaled)
            scores = model.score_samples(X_test_scaled)
            
            # ì ìˆ˜ ë³€í™˜
            if model_info.get('score_transform'):
                scores_transformed, _ = self.transform_scores(scores)
            else:
                min_score = np.min(scores)
                max_score = np.max(scores)
                scores_transformed = -10 + (scores - min_score) * 20 / (max_score - min_score)
            
            self.log(f"âœ… Prediction complete:")
            self.log(f"  - Original score range: [{scores.min():.4f}, {scores.max():.4f}]")
            self.log(f"  - Transformed score range: [{scores_transformed.min():.2f}, {scores_transformed.max():.2f}]")
            
            # ì‹¤ì œ íƒ€ì„ìŠ¤íƒ¬í”„ ì‚¬ìš©
            all_timestamps = pd.to_datetime(test_timestamps)
            all_scores = scores_transformed
            
            # í”Œë¡¯ ê·¸ë¦¬ê¸°
            self.plot_test_results(all_timestamps, all_scores, sensor, model_info)
            
            # ì „ì²´ ê²°ê³¼ ìš”ì•½
            if len(all_scores) > 0:
                all_scores = np.array(all_scores)
                decision_boundary_value = float(model_info.get('decision_boundary', 0))
                total_anomalies = np.sum(all_scores < decision_boundary_value)
                total_ratio = total_anomalies / len(all_scores) * 100
                
                self.result_text.insert(tk.END, f"\n{'='*60}\n")
                self.result_text.insert(tk.END, f"Overall Summary\n")
                self.result_text.insert(tk.END, f"Total windows: {len(all_scores):,}\n")
                self.result_text.insert(tk.END, f"Anomalies detected: {total_anomalies:,} ({total_ratio:.2f}%)\n")
                self.result_text.insert(tk.END, f"Score range: [{np.min(all_scores):.2f}, {np.max(all_scores):.2f}]\n")
                self.result_text.insert(tk.END, f"Average score: {np.mean(all_scores):.2f} Â± {np.std(all_scores):.2f}\n")
                
                # ë‚ ì§œë³„ í†µê³„ ì¶”ê°€
                df_results = pd.DataFrame({'timestamp': all_timestamps, 'score': all_scores})
                df_results['date'] = df_results['timestamp'].dt.date
                daily_stats = df_results.groupby('date')['score'].agg(['count', 'mean', 'std'])
                
                self.result_text.insert(tk.END, f"\nDaily Statistics:\n")
                for date, stats in daily_stats.iterrows():
                    # ë‚ ì§œë³„ ë°ì´í„° í•„í„°ë§
                    date_data = df_results[df_results['date'] == date]
                    date_scores = date_data['score'].values
                    anomaly_count = np.sum(date_scores < decision_boundary_value)
                    self.result_text.insert(tk.END, f"  {date}: {stats['count']} windows, {anomaly_count} anomalies, avg score: {stats['mean']:.2f}\n")
            
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.log(f"í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
    
    def plot_test_results(self, timestamps, scores, sensor, model_info):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì‹œê³„ì—´ í”Œë¡¯ìœ¼ë¡œ í‘œì‹œ"""
        try:
            # ê¸°ì¡´ í”Œë¡¯ í´ë¦¬ì–´
            self.ax1.clear()
            self.ax2.clear()
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            scores = np.array(scores)
            timestamps = pd.to_datetime(timestamps)
            decision_boundary = float(model_info.get('decision_boundary', 0))  # floatìœ¼ë¡œ ë³€í™˜
            
            # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
            if len(scores) == 0:
                self.ax1.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=self.ax1.transAxes)
                self.ax2.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=self.ax2.transAxes)
                return
            
            # 1. Score í”Œë¡¯ (ê°œì„ ëœ Xì¶•)
            self.ax1.plot(timestamps, scores, 'b-', linewidth=0.5, alpha=0.7, label='Score')
            self.ax1.axhline(y=decision_boundary, color='r', linestyle='--', linewidth=2, 
                            label=f'Decision Boundary ({decision_boundary:.2f})')
            self.ax1.axhline(y=0, color='k', linestyle='-', linewidth=1, alpha=0.3)
            
            # ì´ìƒ êµ¬ê°„ í‘œì‹œ
            anomaly_mask = scores < decision_boundary
            if np.any(anomaly_mask):
                self.ax1.scatter(timestamps[anomaly_mask],
                               scores[anomaly_mask],
                               color='red', s=10, alpha=0.5, label='Anomaly')
            
            self.ax1.set_ylabel('Score', fontsize=12)
            self.ax1.set_title(f'{self.test_machine_var.get()} - {sensor.upper()} Anomaly Detection Results', fontsize=14)
            self.ax1.legend(loc='upper right')
            self.ax1.grid(True, alpha=0.3)
            
            # Xì¶• í¬ë§· ê°œì„ 
            # ë‚ ì§œ ë²”ìœ„ì— ë”°ë¼ ì ì ˆí•œ í¬ë§· ì„ íƒ
            date_range = (timestamps[-1] - timestamps[0]).days
            
            if date_range == 0:  # ê°™ì€ ë‚ 
                # ì‹œê°„ ë²”ìœ„ í™•ì¸
                time_range_hours = (timestamps[-1] - timestamps[0]).total_seconds() / 3600
                
                if time_range_hours <= 6:  # 6ì‹œê°„ ì´í•˜
                    self.ax1.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))
                    self.ax1.xaxis.set_major_locator(mdates.MinuteLocator(interval=30))
                    self.ax1.xaxis.set_minor_locator(mdates.MinuteLocator(interval=10))
                elif time_range_hours <= 12:  # 12ì‹œê°„ ì´í•˜
                    self.ax1.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))
                    self.ax1.xaxis.set_major_locator(mdates.HourLocator(interval=1))
                    self.ax1.xaxis.set_minor_locator(mdates.MinuteLocator(interval=30))
                else:  # í•˜ë£¨ ì „ì²´
                    self.ax1.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))
                    self.ax1.xaxis.set_major_locator(mdates.HourLocator(interval=2))
                    self.ax1.xaxis.set_minor_locator(mdates.HourLocator(interval=1))
            elif date_range <= 3:  # 3ì¼ ì´í•˜
                self.ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))
                self.ax1.xaxis.set_major_locator(mdates.HourLocator(interval=6))
            elif date_range <= 7:  # 1ì£¼ì¼ ì´í•˜
                self.ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))
                self.ax1.xaxis.set_major_locator(mdates.HourLocator(interval=12))
            
            plt.setp(self.ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')
            
            # 2. ë‚ ì§œë³„ ì´ìƒì¹˜ ë¹„ìœ¨ ë§‰ëŒ€ ê·¸ë˜í”„
            # DataFrame ìƒì„±
            df = pd.DataFrame({
                'timestamp': timestamps,
                'score': scores,
                'is_anomaly': anomaly_mask
            })
            df['date'] = df['timestamp'].dt.date
            df['hour'] = df['timestamp'].dt.hour
            
            # ë‚ ì§œë³„ ì´ìƒì¹˜ ë¹„ìœ¨ ê³„ì‚°
            daily_stats = df.groupby('date').agg({
                'is_anomaly': ['sum', 'count']
            })
            daily_stats.columns = ['anomaly_count', 'total_count']
            daily_stats['anomaly_ratio'] = daily_stats['anomaly_count'] / daily_stats['total_count'] * 100
            
            # ë§‰ëŒ€ ê·¸ë˜í”„
            dates = daily_stats.index
            ratios = daily_stats['anomaly_ratio'].values
            
            # datesë¥¼ matplotlibì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if len(dates) > 0:
                # pandas dateë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
                dates_for_plot = [pd.to_datetime(d) for d in dates]
                bars = self.ax2.bar(dates_for_plot, ratios, alpha=0.7, color='red')
            else:
                bars = []
            
            # ë°ì´í„°ê°€ ì—†ëŠ” ë‚ ì§œ ì²˜ë¦¬
            if len(bars) == 0:
                self.ax2.text(0.5, 0.5, 'No anomalies detected', ha='center', va='center', transform=self.ax2.transAxes)
                self.fig.tight_layout()
                self.canvas.draw()
                return
            
            # ê°’ ë ˆì´ë¸” ì¶”ê°€ (ë§‰ëŒ€ ìœ„ì—)
            for bar, ratio in zip(bars, ratios):
                if ratio > 0:
                    height = bar.get_height()
                    self.ax2.text(bar.get_x() + bar.get_width()/2., height,
                                f'{ratio:.1f}%',
                                ha='center', va='bottom', fontsize=8)
            
            self.ax2.set_ylabel('Daily Anomaly Ratio (%)', fontsize=12)
            self.ax2.set_xlabel('Date', fontsize=12)
            self.ax2.grid(True, alpha=0.3, axis='y')
            
            # Xì¶• í¬ë§·
            if len(dates) <= 7:
                self.ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
                self.ax2.xaxis.set_major_locator(mdates.DayLocator())
            else:
                self.ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
                self.ax2.xaxis.set_major_locator(mdates.DayLocator(interval=max(1, len(dates)//10)))
            
            plt.setp(self.ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')
            
            # í‰ê· ì„  ì¶”ê°€
            mean_ratio = daily_stats['anomaly_ratio'].mean()
            self.ax2.axhline(y=mean_ratio, color='orange', linestyle=':', linewidth=2, 
                            label=f'Average: {mean_ratio:.1f}%')
            self.ax2.legend()
            
            self.fig.tight_layout()
            self.canvas.draw()
            
            # ì¶”ê°€ ì •ë³´ë¥¼ ê²°ê³¼ í…ìŠ¤íŠ¸ì— ì¶œë ¥
            self.result_text.insert(tk.END, f"\n\nğŸ“Š Hourly Anomaly Analysis:\n")
            
            # ì‹œê°„ëŒ€ë³„ í†µê³„
            hourly_stats = df.groupby('hour')['is_anomaly'].agg(['sum', 'count'])
            hourly_stats['ratio'] = hourly_stats['sum'] / hourly_stats['count'] * 100
            
            self.result_text.insert(tk.END, f"\nAverage anomaly ratio by hour:\n")
            for hour in range(24):
                if hour in hourly_stats.index:
                    ratio = hourly_stats.loc[hour, 'ratio']
                    count = hourly_stats.loc[hour, 'count']
                    if ratio > 0:
                        self.result_text.insert(tk.END, f"  {hour:02d}:00: {ratio:5.1f}% (n={count})\n")
            
            # ì‹¤ì œ ë°ì´í„° ê°„ê²© ì •ë³´ ì¶”ê°€
            if len(timestamps) > 1:
                time_diffs = np.diff(timestamps.values).astype('timedelta64[s]').astype(float)
                avg_interval = np.mean(time_diffs)
                std_interval = np.std(time_diffs)
                
                self.result_text.insert(tk.END, f"\nğŸ“Š Data Collection Interval:\n")
                self.result_text.insert(tk.END, f"  Average: {avg_interval:.1f} seconds\n")
                self.result_text.insert(tk.END, f"  Std Dev: {std_interval:.1f} seconds\n")
                self.result_text.insert(tk.END, f"  Range: [{np.min(time_diffs):.1f}, {np.max(time_diffs):.1f}] seconds\n")
            
        except Exception as e:
            self.log(f"í”Œë¡¯ ìƒì„± ì˜¤ë¥˜: {e}")
    
    def analyze_hourly_anomalies(self, machine_id, sensor, test_date, predictions, scores):
        """ì‹œê°„ëŒ€ë³„ ì´ìƒ íƒì§€ ë¶„ì„"""
        try:
            # í•´ë‹¹ ë‚ ì§œì˜ ì›ë³¸ ë°ì´í„° ë‹¤ì‹œ ë¡œë“œ (ì‹œê°„ ì •ë³´ í¬í•¨)
            if sensor == 'acc':
                query = """
                SELECT time, x, y, z
                FROM normal_acc_data
                WHERE machine_id = %s
                AND DATE(time) = %s
                ORDER BY time
                """
            else:
                query = """
                SELECT time, mic_value
                FROM normal_mic_data
                WHERE machine_id = %s
                AND DATE(time) = %s
                ORDER BY time
                """
            
            df = pd.read_sql(query, self.conn, params=(machine_id, test_date))
            df['time'] = pd.to_datetime(df['time'])
            
            # 5ì´ˆ ìœˆë„ìš°ë¡œ ê·¸ë£¹í™” (í•™ìŠµê³¼ ë™ì¼)
            window_sec = self.sensor_config[sensor]['window_sec']
            df['window'] = df['time'].dt.floor(f'{window_sec}S')
            df['hour'] = df['time'].dt.hour
            
            # ê° ìœˆë„ìš°ì˜ ì‹œê°„ëŒ€ í• ë‹¹
            window_hours = df.groupby('window')['hour'].first().values
            
            # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ëŠ” ìœˆë„ìš°ë§Œ í•„í„°ë§
            window_samples = window_sec * 10  # DBëŠ” 10Hz
            valid_windows = df.groupby('window').size() >= window_samples * 0.8
            valid_indices = valid_windows[valid_windows].index
            
            # ìœ íš¨í•œ ìœˆë„ìš°ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
            window_idx = 0
            hourly_predictions = {h: [] for h in range(24)}
            hourly_scores = {h: [] for h in range(24)}
            
            for window in valid_indices:
                if window_idx < len(predictions):
                    hour = df[df['window'] == window]['hour'].iloc[0]
                    hourly_predictions[hour].append(predictions[window_idx])
                    hourly_scores[hour].append(scores[window_idx])
                    window_idx += 1
            
            # ì‹œê°„ëŒ€ë³„ í†µê³„ ê³„ì‚°
            hourly_stats = {}
            for hour in range(24):
                if len(hourly_predictions[hour]) > 0:
                    anomaly_count = sum(p == -1 for p in hourly_predictions[hour])
                    total_count = len(hourly_predictions[hour])
                    
                    hourly_stats[hour] = {
                        'anomaly_count': anomaly_count,
                        'total_count': total_count,
                        'anomaly_ratio': anomaly_count / total_count * 100,
                        'mean_score': np.mean(hourly_scores[hour]),
                        'min_score': np.min(hourly_scores[hour]),
                        'max_score': np.max(hourly_scores[hour])
                    }
            
            return hourly_stats
            
        except Exception as e:
            self.log(f"ì‹œê°„ëŒ€ë³„ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return None
    
    def transform_scores(self, scores):
        """Isolation Forest ì ìˆ˜ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ì´ìƒ ì ìˆ˜ë¡œ ë³€í™˜"""
        # Isolation Forest ì ìˆ˜ íŠ¹ì„±:
        # - 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì •ìƒ
        # - ìŒìˆ˜ì¼ìˆ˜ë¡ ì´ìƒ
        # - ì¼ë°˜ì ìœ¼ë¡œ -0.1 ~ -0.7 ë²”ìœ„
        
        # ì¤‘ì•™ê°’ì„ ê¸°ì¤€ì ìœ¼ë¡œ ì‚¬ìš©
        median_score = np.median(scores)
        
        # ë¹„ì„ í˜• ë³€í™˜ í•¨ìˆ˜ (ì§€ìˆ˜ í•¨ìˆ˜ ê¸°ë°˜)
        def exponential_transform(x):
            # ì¤‘ì•™ê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”
            normalized = (x - median_score) / abs(median_score) if median_score != 0 else x
            
            # ì§€ìˆ˜ ë³€í™˜ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë¶„í¬ ìƒì„±
            if normalized >= 0:
                # ì •ìƒ ë²”ìœ„: 0 ê·¼ì²˜ ìœ ì§€
                return normalized * 2  # ìµœëŒ€ +2 ì •ë„
            else:
                # ì´ìƒ ë²”ìœ„: ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€
                # normalizedê°€ -1ì¼ ë•Œ ì•½ -7
                # normalizedê°€ -2ì¼ ë•Œ ì•½ -15
                # normalizedê°€ -3ì¼ ë•Œ ì•½ -25
                return 5 * (np.exp(abs(normalized)) - 1) * np.sign(normalized)
        
        # ë²¡í„°í™”ëœ ë³€í™˜ ì ìš©
        transformed = np.vectorize(exponential_transform)(scores)
        
        # í´ë¦¬í•‘ ì—†ìŒ - ìì—°ìŠ¤ëŸ¬ìš´ ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        
        # í†µê³„ ì •ë³´
        stats = {
            'median_score': float(median_score),
            'original_range': [float(np.min(scores)), float(np.max(scores))],
            'transformed_range': [float(np.min(transformed)), float(np.max(transformed))]
        }
        
        return transformed, stats
    
    def start_testing(self):
        start_date = self.test_start_date.get_date()
        end_date = self.test_end_date.get_date()
        
        if start_date > end_date:
            messagebox.showerror("ì˜¤ë¥˜", "ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ëŠ¦ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        thread = threading.Thread(target=self.test_model)
        thread.daemon = True
        thread.start()

def main():
    root = tk.Tk()
    app = OCSVMTrainerGUI(root)
    root.mainloop()

if __name__ == "__main__":
    main()